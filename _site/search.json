[
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html",
    "title": "Place Opportunity",
    "section": "",
    "text": "In this lab, we’ll extend our knowledge of opportunity maps, and more generally in standardizing and creating indexes from data. This strategy is in some ways an extension of the strategy we used to measure neighborhood change. Our measurement of neighborhood change, however, focused on a single dimension of change - income. In this case, we’ll produce a multidimensional measure of change.\nAs we discussed in class, opportunity maps and other indexes are commonly used to be able to illustrate the distribution of resources across space and over time. These types of indexes are designed to be multidimensional, and therefore require a strong theoretical framework connecting concepts related to opportunity to indicators or measures designed to proxy or represent these concepts."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#introduction",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#introduction",
    "title": "Place Opportunity",
    "section": "",
    "text": "In this lab, we’ll extend our knowledge of opportunity maps, and more generally in standardizing and creating indexes from data. This strategy is in some ways an extension of the strategy we used to measure neighborhood change. Our measurement of neighborhood change, however, focused on a single dimension of change - income. In this case, we’ll produce a multidimensional measure of change.\nAs we discussed in class, opportunity maps and other indexes are commonly used to be able to illustrate the distribution of resources across space and over time. These types of indexes are designed to be multidimensional, and therefore require a strong theoretical framework connecting concepts related to opportunity to indicators or measures designed to proxy or represent these concepts."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#goals",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#goals",
    "title": "Place Opportunity",
    "section": "Goals",
    "text": "Goals\nThis lab introduces you to the following:\n\nCommon methods for standardizing and constructing indexes from demographic data\nBasic concepts associated with relational joins between datasets\nReinforces and provides a means of practicing data visualization and presentation"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#core-concepts",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#core-concepts",
    "title": "Place Opportunity",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\nacross()\nreduce()\nrowwise()\nscale()\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#github-lab-repository",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#github-lab-repository",
    "title": "Place Opportunity",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#download-and-prepare-data-for-opportunity-mapping",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#download-and-prepare-data-for-opportunity-mapping",
    "title": "Place Opportunity",
    "section": "Download and Prepare Data for Opportunity Mapping",
    "text": "Download and Prepare Data for Opportunity Mapping\nThe National Housing Conference brief which you read for Tuesday outlines several suggested data sources for opportunity mapping. For the purpose of learning some of the methods associated with opportunity mapping, we’ll work with three primary data sources:\n\nU.S. Census Bureau - American Community Survey\nHUD Location Affordability Index (version 3.0)\nEPA EJSCREEN\n\nYou should independently explore the documentation for these datasets and their indicators - there is a treasure trove of documentation to look at. When possible, download data documentation to your docuemtnation project folder.\nWe’ll divide these data into a few conceptual categories commonly seen in opportunity maps:\n\nEducation\nHousing Characteristics\nSocial Capital\nPublic Health and Safety\nEmployment and Workforce\nTransportation and Mobility\n\n\n\n\nCategory\nIndicator\nData Source\n\n\n\n\nEducation\nPopulation with a High School Diploma or greater\nACS\n\n\nEducation\nPopulation with a Bachelor’s Degree or greater\nACS\n\n\nHousing Characteristics\nMedian Home Value\nACS\n\n\nHousing Characteristics\nMedian Gross Rent\nHUD LAI\n\n\nHousing Characteristics\nPercentage Single Family Housing Units\nHUD LAI\n\n\nHousing Characteristics\nGross Rent as Percentage of Income\nACS\n\n\nHousing Characteristics\nHousing Cost Burden\nACS\n\n\nHousing Characteristics\nResidential Vacancy Rate\nACS\n\n\nSocial Capital\nPopulation Age 25 - 44\nACS\n\n\nSocial Capital\nMedian Household Income\nACS\n\n\nSocial Capital\nPercent of Households in Poverty\nACS\n\n\nSocial Capital\nPercentage of Owner-Occupied Housing Units\nACS\n\n\nEmployment and Worforce\nJob Density\nHUD LAI\n\n\nEmployment and Worforce\nRetail Density\nHUD LAI\n\n\nTransportation and Mobility\nMedian Commute Time\nHUD LAI\n\n\nTransportation and Mobility\nPublic Transit Use (Journey to work)\nHUD LAI\n\n\nPublic Health and Safety\nNATA Cancer Risk Index\nEPA EJSCREEN (2022)\n\n\nPublic Health and Safety\nNATA Respiratory Hazard Index\nEPA EJSCREEN (2022)\n\n\nPublic Health and Safety\nTraffic Proximity and Volume\nEPA EJSCREEN (2022)\n\n\nPublic Health and Safety\nParticulate Matter\nEPA EJSCREEN(2022)\n\n\n\nI have provided download links for the EJSCREEN and LAI data. We will use tidycensus to load ACS data.\n\nACS Data\nLet’s start by loading some ACS data:\n\n\nCode\nDL_Year&lt;-2020\nsurvey &lt;- \"acs5\"\nstate&lt;-c(\"NY\")\nsource(\"scripts/1_Get_ACS.R\")\n\n\nThis should look familiar - we have a script kept in the scripts folder with general code to download selected variables from the American Community Survey. Rather than placing this code in our notebook, we can use the source() command to run the script in its entirety. We are defining the variables (api_key, DL_Year, survey, state) in our notebook, so that these can be referenced when the script is running.\nWe have downloaded a lot of ACS data here at the census tract level for all tracts in Illinois:\n\n\n\nTable\nDescription\n\n\n\n\nB01001\nAge and Population\n\n\nB02001\nRace\n\n\nB03001\nEthnicity\n\n\nB05002\nForeign Born\n\n\nB11001\nFemale Headed Household\n\n\nB17001\nPoverty Rate\n\n\nB19013\nMedian Household Income\n\n\nB25002\nResidential Vacancy\n\n\nB25003\nHousing Tenure\n\n\nB25077\nMedian Home Value\n\n\nB25106\nHousing Cost Burden\n\n\n\nI went ahead and turned this raw data into some selected indicators:\n\n\n\n\n\n\n\n\nTable\nLabel\nDescription\n\n\n\n\nB01001\nunder18\nProportion of Population under 18\n\n\nB01001\nover65\nProportion of Population over 65\n\n\nB01001\nP_Female\nProportion of Population female\n\n\nB01001\nPop\nTotal Population size\n\n\nB02001\nPWhite\nProportion Population White\n\n\nB02001\nPBlack\nProportion Population Black\n\n\nB02001\nPAIAN\nProportion Population AIAN\n\n\nB02001\nPAsian\nProportion Population Asian\n\n\nB02001\nPNonwhite\nProportion Population Nonwhite\n\n\nB03001\nPLatino\nProportion Population Latino Ethnicity (Of all Races)\n\n\nB05002\nPForeignborn\nProportion Population Foreign Born\n\n\nB19013\nMHHI\nMedian Household Income\n\n\nB11001\nP_FHHH\nProportion Female Headed Households\n\n\nB17001\nPov\nProportion Households Below Poverty\n\n\nB25003\nP_Own\nProportion Owner Occupied Housing Units\n\n\nB25077\nMHV\nMedian Home Value\n\n\nB25106\nCostBurden\nHousing Cost Burden\n\n\nB25002\nRvac\nResidential Vacancy Rate\n\n\n\n\nJoin ACS Data\nNow that we have these individual tables downloaded, we need to combine these together into a single dataset. We’ve done this many times in the past using left_join() but only with respect to two datasets at a time. Let’s introduce an alternative strategy here, using the purrr package’s reduce() function combined with left_join().\nFirst, here’s the way we might accomplish this as we have in the past:\n\n\nCode\nacs_data&lt;-left_join(B01001, B02001, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B03001, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B05002, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B11001, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B17001, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B19013, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B25002, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B25003, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B25077, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B25106, by=\"GEOID\")\n\n\nThis works just fine, but there’s a lot of repition of code here with so many data frames.\nNow let’s look at how we can implement the same thing using reduce():\n\n\nCode\nacs_data &lt;- list(B01001, B02001, B03001, B05002, B11001, B17001, B19013, B25002, B25003, B25077, B25106) |&gt; \n  reduce(left_join, by = \"GEOID\")\n\n\nIn this case, we create a list containing each of the data frames we want to join together (note that we don’t put these in quotes since they’re objects and not names). We then use the reduce() command and tell reduce that we want to perform a left join and also supply the common variable name.\nBut what is reduce() really doing here? If you look at it’s documentation (?reduce), you’ll see that:\nreduce() is an operation that combines elements of a vector into a single value. The combination is driven by .f, a binary function, and takes two values and returns a single value: Reducing f over 1:3 computes the values f(f(1,2), 3).\nSo if we are doing a left join, this is essentially doing the following:\n\n\nCode\nacs_data &lt;- left_join(df1, df2)\nacs_data &lt;- left_join(acs_data, df3)\n\n\nAnd on and on until reduce has iterated through all elements of the list. This helps us cut down on repetitious code.\nInspecting our handiwork, our acs_data table (for which we downloaded tract-level data for New York state has 5,411 observations and 19 variables:\n\n\nCode\nstr(acs_data)\n\n\ntibble [5,411 × 19] (S3: tbl_df/tbl/data.frame)\n $ GEOID       : chr [1:5411] \"36011040600\" \"36011040700\" \"36011040800\" \"36011040900\" ...\n $ under18     : num [1:5411] 0.183 0.187 0.192 0.218 0.2 ...\n $ over65      : num [1:5411] 0.234 0.216 0.182 0.18 0.165 ...\n $ Pop         : num [1:5411] 3400 3633 4668 3683 3088 ...\n $ P_Female    : num [1:5411] 0.504 0.518 0.419 0.476 0.456 ...\n $ PWhite      : num [1:5411] 0.945 0.978 0.841 0.982 0.94 ...\n $ PBlack      : num [1:5411] 0.037647 0 0.113325 0.000272 0 ...\n $ PAIAN       : num [1:5411] 0 0 0 0.000815 0.000324 ...\n $ PAsian      : num [1:5411] 0.00235 0 0.00771 0.00434 0.0068 ...\n $ PNonwhite   : num [1:5411] 0.0547 0.022 0.1594 0.0185 0.0599 ...\n $ PLatino     : num [1:5411] 0.01647 0.00798 0.02571 0.02607 0.04955 ...\n $ PForeignborn: num [1:5411] 0.0176 0.0242 0.0176 0.0141 0.0385 ...\n $ P_FHHH      : num [1:5411] 0.0931 0.0486 0.089 0.0989 0.0638 ...\n $ Pov         : num [1:5411] 0.0507 0.049 0.0991 0.158 0.087 ...\n $ MHHI        : num [1:5411] 84330 93493 64811 66711 73182 ...\n $ Rvac        : num [1:5411] 0.0898 0.1612 0.3182 0.176 0.246 ...\n $ P_Own       : num [1:5411] 0.917 0.935 0.828 0.872 0.836 ...\n $ MHV         : num [1:5411] 186500 174900 147100 142700 147600 ...\n $ CostBurden  : num [1:5411] 0.224 0.206 0.283 0.246 0.172 ...\n\n\nWhile we’re at it, let’s do a little cleaning up. We have a lot of data frames which are now consolidated into acs_data. We can delete other objects from our environment using rm()\n\n\nCode\nrm(B01001, B02001, B03001, B05002, B11001, B17001, B19013, B25002, B25003, B25077, B25106)\n\n\n\n\n\nLocation Affordability Index Data\nLet’s download the HUD Location Affordability Index data from here. We can then load it directly.\n\n\nCode\nLAI &lt;- read_csv(\"data_raw/Location_Affordability_Index_v.3.csv\")\n\n\nRows: 73763 Columns: 444\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (7): GEOID, STATE, COUNTY, TRACT, CNTY_FIPS, STUSAB, area_type\ndbl (437): OBJECTID, households, owner_occupied_hu, renter_occupied_hu, pct_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s go ahead and select the following variables (our variables of interest) and overwrite the initial file:\n\nGEOID\npct_transit_j2w\nmedian_gross_rent\npct_hu_1_detatched\njob_density_simple\nretail_density_simple\nmedian_commute\n\n\n\nCode\nLAI&lt;-LAI |&gt;  select(GEOID, \n                    STUSAB,\n                    pct_transit_j2w,\n                    median_gross_rent,\n                    pct_hu_1_detached,\n                    job_density_simple,\n                    retail_density_simple,\n                    median_commute\n                    )\n\n\n\n\nEJSCREEN Data\nWe can download the EJSCREEN data here. Unlike the LAI data which we could read directly from the .csv file, the EJSCREEN data will need to be downloaded and unzipped before it can be used.\nWe’ll use the read_csv() function from the readr package to load raw EJSCREEN data, located in the “data” folder. Load these into an object called “ejscreen”:\nInspect the data. Let’s select a subset of the variables in the data which we’ll work more with. Based upon our framework, we’ll select the following variables and overwrite the original ejscreen object with our selection:\n\nID\nACSTOTPOP\nCANCER\nRESP\nPTRAF\nPM25\n\n\n\nCode\nejscreen&lt;-ejscreen |&gt; \n  select(ID,\n        ACSTOTPOP,\n        CANCER,\n        RESP,\n        PTRAF,\n        PM25)"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#joining-the-three-datasets",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#joining-the-three-datasets",
    "title": "Place Opportunity",
    "section": "Joining the Three Datasets",
    "text": "Joining the Three Datasets\nOk - we’re getting somewhere, I promise! We now have three datasets - acs_data, which contains tract-level data for New York state, LAI which contains tract-level data for the entire US, and ejscreen which contains block group level data for the US. We’re so close to joining these together and indexing the data!\n\nCreating Tract Data from Block Groups\nOne thing, though - we need to convert our ejscreen block groups data into tracts. Block groups are geographic subdivisions of tracts. Fortunately for us, Census FIPS codes are hierarchical - the combined state-county-tract-blockgroup variable called “ID” in ejscreen contains the tract FIPS code: - State (2 characters) - County (3 characters) - Tract (6 characters) - Block Group (1 character)\nOk, this is all well and good - how do we use this knowledge to combine things together? We could simply average the block group values together to approximate a tract-level average. At the same time, block groups are likely to have different populations. Given this potential heterogeneity, let’s use the population weighted average of those block group characteristics to constitute our tract characteristics.\nTo aggregate our block group data, we need to extract the tract characters from the ID column, and then we can group_by() and summarise our data based upon the tract FIPS code. Base R fortunately has a function called substr() (substring) which can handle the extraction for us:\n\n\nCode\nejscreen&lt;-ejscreen |&gt; \n  mutate(GEOID = substr(ID, 0, 11))\n\n\nWhat happened here? We’re creating a new column called GEOID, and then invoking substr. We tell that function that we want a substring from the ID field, and then we say 0, 11, where 0 is the first position if we were counting from the left of the first character in the ID variable, and 11 is the last character that we want (remember, the combined FIPS code is 12 characters long, and we want all but the last character).\nAnd now we can use group_by() and summarise() to group together the data into census tract level aggregates. We can use weighted.mean() to calculate… weight for it… the weighted average (professor humor)! GEOID contains our combined state-county-tract FIPS code, ACSTOTPOP contains out population for the purpose of weighting, and we want to calculate averages for CANCER, RESP, PTRAF, and PM25. Figure out how to get that done.\nSneak preview - you’re going to hit an error - think about what rows might need to be removed to calculate a weighted average:\n\n\nCode\nejscreen&lt;-ejscreen |&gt;  \n  filter(ACSTOTPOP != 0, CANCER != \"None\") |&gt; \n  group_by(GEOID) |&gt;  \n  summarise(\n  CANCER = weighted.mean(as.numeric(CANCER), ACSTOTPOP),\n  RESP = weighted.mean(as.numeric(RESP), ACSTOTPOP),\n  PTRAF = weighted.mean(PTRAF, ACSTOTPOP),\n  PM25 = weighted.mean(as.numeric(PM25), ACSTOTPOP))\n\n\nOk - now we have around 73,000 tract-level observations for the EPA data - not too far off from what we have for the LAI data. Both of these datasets include observations for the entire US, while our ACS data only contains observations for New York State. Using your newfound knowledge of how to join data together based upon a common column, create a new object named dataset which contains combined ACS, ejscreen, and LAI data for Illinois census tracts:\n\n\nCode\ndataset&lt;- list(acs_data,  LAI, ejscreen) |&gt; reduce(left_join, by=\"GEOID\")\n\ndataset |&gt; \n  head() |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      under18\n      over65\n      Pop\n      P_Female\n      PWhite\n      PBlack\n      PAIAN\n      PAsian\n      PNonwhite\n      PLatino\n      PForeignborn\n      P_FHHH\n      Pov\n      MHHI\n      Rvac\n      P_Own\n      MHV\n      CostBurden\n      STUSAB\n      pct_transit_j2w\n      median_gross_rent\n      pct_hu_1_detached\n      job_density_simple\n      retail_density_simple\n      median_commute\n      CANCER\n      RESP\n      PTRAF\n      PM25\n    \n  \n  \n    36011040600\n0.1829412\n0.2341176\n3400\n0.5044118\n0.9452941\n0.0376470588\n0.0000000000\n0.002352941\n0.05470588\n0.016470588\n0.01764706\n0.09307876\n0.05068672\n84330\n0.08979001\n0.9172633\n186500\n0.2235481\nNY\n0.7344093\n869\n92.82640\n0.071023683\n0.006993935\n16.00\n19.70603\n0.2238431\n76.199249\n6.211746\n    36011040700\n0.1871731\n0.2163501\n3633\n0.5183044\n0.9779796\n0.0000000000\n0.0000000000\n0.000000000\n0.02202037\n0.007982384\n0.02422241\n0.04855024\n0.04899532\n93493\n0.16119910\n0.9352664\n174900\n0.2063385\nNY\n0.0000000\n787\n94.71613\n0.084031686\n0.000897934\n13.95\n19.58912\n0.2248577\n8.835063\n6.179412\n    36011040800\n0.1919452\n0.1820908\n4668\n0.4190231\n0.8406170\n0.1133247644\n0.0000000000\n0.007712082\n0.15938303\n0.025706941\n0.01756641\n0.08900524\n0.09908915\n64811\n0.31816154\n0.8278796\n147100\n0.2827225\nNY\n0.1046997\n621\n75.63869\n0.021437169\n0.003085479\n20.63\n17.56237\n0.1972023\nNA\n6.092557\n    36011040900\n0.2180288\n0.1802878\n3683\n0.4759707\n0.9815368\n0.0002715178\n0.0008145534\n0.004344285\n0.01846321\n0.026065707\n0.01411892\n0.09890110\n0.15799510\n66711\n0.17600453\n0.8722527\n142700\n0.2458791\nNY\n0.1057314\n733\n70.25913\n0.004264647\n0.000176875\n18.12\n17.22243\n0.1934044\nNA\n6.108516\n    36011041001\n0.2001295\n0.1648316\n3088\n0.4562824\n0.9400907\n0.0000000000\n0.0003238342\n0.006800518\n0.05990933\n0.049546632\n0.03853627\n0.06375839\n0.08699902\n73182\n0.24604681\n0.8355705\n147600\n0.1719799\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n    36011041002\n0.1115152\n0.1921212\n1650\n0.4678788\n0.8909091\n0.0151515152\n0.0042424242\n0.013333333\n0.10909091\n0.065454545\n0.02424242\n0.04838710\n0.09794239\n79167\n0.35416667\n0.8440860\n169900\n0.1487455\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n  \n  \n  \n\n\n\n\nNow that we have our base dataset together, we can filter to our area of interest, in this case, the boroughs that make up New York City:\n\n\n\nFIPS Code\nCounty Name\nBorough Name\n\n\n\n\n36047\nKings County\nBrooklyn\n\n\n36005\nBronx County\nBronx\n\n\n36081\nQueens County\nQueens\n\n\n36085\nRichmond County\nStaten Island\n\n\n36061\nNew York County\nManhattan\n\n\n\n\n\nCode\ndataset &lt;- dataset |&gt; \n  filter(substr(GEOID, 0, 5) %in% c(\"36047\", \"36005\", \"36081\", \"36085\", \"36061\"))\n\n\nThe above code uses substr() to pull out the first 5 characters from the GEOID field (the combined state and county FIPS codes) and then matches them against the FIPS codes for the five boroughs of New York City."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#create-standardized-scores",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#create-standardized-scores",
    "title": "Place Opportunity",
    "section": "Create Standardized Scores",
    "text": "Create Standardized Scores\nRemembering back to your high school or college statistics class, a Z-score (or standardized score) can be be calculated by subtracting from a given observation the mean of all observations and then dividing by the standard deviation of all observations.\nRecall:\n\\(z = \\frac{x-\\mu}{\\sigma}\\) where: \\(x\\) is the individual observation we want to standardize \\(\\mu\\) is the population mean \\(\\sigma\\) is the population standard deviation\nFind the mean and standard deviation for a variable in our dataset and manually calculate a z-score (just for fun).\n\n\nCode\ndataset |&gt; \n  summarise(\n  under18_mean = mean(under18, na.rm=TRUE), \n  under18_sd = sd(under18, na.rm=TRUE),\n  z_under18 = (under18 - under18_mean)/under18_sd\n  )\n\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n# A tibble: 2,327 × 3\n   under18_mean under18_sd z_under18\n          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1        0.205     0.0773   0.960  \n 2        0.205     0.0773   1.98   \n 3        0.205     0.0773   0.572  \n 4        0.205     0.0773  -0.254  \n 5        0.205     0.0773   0.980  \n 6        0.205     0.0773   0.706  \n 7        0.205     0.0773  -0.253  \n 8        0.205     0.0773   0.865  \n 9        0.205     0.0773  -0.00949\n10        0.205     0.0773   0.539  \n# ℹ 2,317 more rows\n\n\nFor the first observation, the value for Under 18 is 27.9 percent. The average value for tracts in Illinois is 27.9 percent, and the standard deviation is 7.73. The standardized score of .960 indicates that the observation is .96 of a standard deviation above the mean. There’s absolutely nothing wrong with doing this manually, however, we can use the scale() function to do the same thing:\n\n\nCode\ndataset |&gt;  \n  mutate(under18 = scale(under18))\n\n\n# A tibble: 2,327 × 30\n   GEOID       under18[,1] over65   Pop P_Female PWhite PBlack   PAIAN PAsian\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 36047009202     0.960   0.0747  3453    0.514 0.296  0.0568 0.00782 0.300 \n 2 36047009401     1.98    0.0763  2293    0.515 0.0754 0.0754 0       0.690 \n 3 36047009402     0.572   0.0885  2746    0.548 0.172  0.0138 0       0.695 \n 4 36047009600    -0.254   0.132   5858    0.570 0.362  0.0446 0.00751 0.340 \n 5 36047009800     0.980   0.0864  6021    0.498 0.153  0.0485 0       0.383 \n 6 36047010000     0.706   0.128   5978    0.541 0.264  0.0238 0.0110  0.423 \n 7 36047010100    -0.253   0.0842  3944    0.508 0.510  0.0418 0.0327  0.0903\n 8 36047010200     0.865   0.126   4844    0.515 0.124  0.0142 0       0.705 \n 9 36047010401    -0.00949 0.125   2152    0.487 0.106  0.0279 0       0.838 \n10 36047010402     0.539   0.148   2256    0.523 0.0638 0      0       0.879 \n# ℹ 2,317 more rows\n# ℹ 21 more variables: PNonwhite &lt;dbl&gt;, PLatino &lt;dbl&gt;, PForeignborn &lt;dbl&gt;,\n#   P_FHHH &lt;dbl&gt;, Pov &lt;dbl&gt;, MHHI &lt;dbl&gt;, Rvac &lt;dbl&gt;, P_Own &lt;dbl&gt;, MHV &lt;dbl&gt;,\n#   CostBurden &lt;dbl&gt;, STUSAB &lt;chr&gt;, pct_transit_j2w &lt;dbl&gt;,\n#   median_gross_rent &lt;dbl&gt;, pct_hu_1_detached &lt;dbl&gt;, job_density_simple &lt;dbl&gt;,\n#   retail_density_simple &lt;dbl&gt;, median_commute &lt;dbl&gt;, CANCER &lt;dbl&gt;,\n#   RESP &lt;dbl&gt;, PTRAF &lt;dbl&gt;, PM25 &lt;dbl&gt;\n\n\nThis brings up an important question - what should the reference geography be for our opportunity measures? We narrowed down our dataset to New York City, however, we could have just as easily used the state or the nation. It’s important to keep in mind that once we construct Z-scores for our indicators, they are with reference to the distribiotn of values we choose to include. We are using the city of New York City as our point of reference. Using the state as a point of reference may be relevant and useful, especially if we want to make some comparisons across the state. At the same time, the characteristics of New York City and other metropolitan areas are likely to be very different than the balance of the state, and so comparisons for the purpose of standardizing our data may be a bit distorted. Now that we have our reference region selected, we can move forward with standardizing each indicator value.\nBefore we do this, it can be useful to provide a more intuitive description of each variable (since most people do not think about indicators in standardized terms). You may recall us previously using commands like summary() for basic descriptives.\nWe’re going to use a variant of the summarise command which you’ve used in the past. summarise_at() allows us to select variables to summarise by their name, and will then apply the same function across all of those variables. Let’s find the mean for each of the indicator variables in our compiled dataset:\n\n\nCode\ndataset |&gt; \n  summarise_at(vars(under18:median_commute), mean, na.rm=TRUE)\n\n\n# A tibble: 1 × 25\n  under18 over65   Pop P_Female PWhite PBlack   PAIAN PAsian PNonwhite PLatino\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1   0.205  0.151 3601.    0.519  0.410  0.250 0.00443  0.148     0.590   0.271\n# ℹ 15 more variables: PForeignborn &lt;dbl&gt;, P_FHHH &lt;dbl&gt;, Pov &lt;dbl&gt;, MHHI &lt;dbl&gt;,\n#   Rvac &lt;dbl&gt;, P_Own &lt;dbl&gt;, MHV &lt;dbl&gt;, CostBurden &lt;dbl&gt;, STUSAB &lt;dbl&gt;,\n#   pct_transit_j2w &lt;dbl&gt;, median_gross_rent &lt;dbl&gt;, pct_hu_1_detached &lt;dbl&gt;,\n#   job_density_simple &lt;dbl&gt;, retail_density_simple &lt;dbl&gt;, median_commute &lt;dbl&gt;\n\n\nWe use vars() to specify the variables to summarize by their name, using the colon to specify through (e.g. under18 through median_commute). We specify that we want our summary statistic to be the mean, and we specify that we want to remove NAs so we get a usable statistic out. This gives us some usable statistics for the entire region. We might also want to derive summaries for each of the counties in the region - how could we do this using your knowledge of group_by(), substr(), and summarise_at()?\n\n\nCode\ndataset |&gt; \n  group_by(substr(GEOID, 0, 5)) |&gt; \n  summarise_at(vars(under18:median_commute), mean, na.rm=TRUE)\n\n\nWarning: There were 5 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `STUSAB = (function (x, ...) ...`.\nℹ In group 1: `substr(GEOID, 0, 5) = \"36005\"`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\n\n# A tibble: 5 × 26\n  `substr(GEOID, 0, 5)` under18 over65   Pop P_Female PWhite PBlack   PAIAN\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 36005                   0.241  0.134 3953.    0.528  0.235  0.344 0.00712\n2 36047                   0.224  0.144 3201.    0.524  0.430  0.313 0.00320\n3 36061                   0.134  0.167 5255.    0.520  0.570  0.142 0.00342\n4 36081                   0.195  0.160 3132.    0.509  0.355  0.200 0.00528\n5 36085                   0.215  0.160 3775.    0.513  0.688  0.124 0.00231\n# ℹ 18 more variables: PAsian &lt;dbl&gt;, PNonwhite &lt;dbl&gt;, PLatino &lt;dbl&gt;,\n#   PForeignborn &lt;dbl&gt;, P_FHHH &lt;dbl&gt;, Pov &lt;dbl&gt;, MHHI &lt;dbl&gt;, Rvac &lt;dbl&gt;,\n#   P_Own &lt;dbl&gt;, MHV &lt;dbl&gt;, CostBurden &lt;dbl&gt;, STUSAB &lt;dbl&gt;,\n#   pct_transit_j2w &lt;dbl&gt;, median_gross_rent &lt;dbl&gt;, pct_hu_1_detached &lt;dbl&gt;,\n#   job_density_simple &lt;dbl&gt;, retail_density_simple &lt;dbl&gt;, median_commute &lt;dbl&gt;\n\n\nYou can start to see some of the differences that exist between distributions at the borough (county) level. Do any stick out to you at this point?\nOk, we have created a numeric summary table that may be useful for reporting out. Now we can produce our standardized scores which we can use for index making.\nNow that you have defined your region, described the data for your region, and know how to standardize the values, you can create a scaled version of your data. Earlier, we learned how to use summarise_at() to select variables to summarize using a specific function. There’s also a mutate_at() function which allows you to perform the same alteration upon all of the variables you select. Take a look at the documentation for mutate_at(). You’ll need to supply the variables you want to mutate, as well as the function (read closely the documentation and examples before proceeding).\nNow let’s go ahead and create a separate dataset called dataset_scaled which contains the scaled values (there may a few fields which either do not need scaling or who you can remove):\n\n\nCode\ndataset_scaled&lt;-dataset |&gt;  \n  mutate_at(vars(under18, over65, P_Female, PNonwhite, PForeignborn, P_FHHH, Pov, MHHI, P_Own, MHV, CostBurden, CANCER, RESP, PTRAF, PM25, pct_transit_j2w, median_gross_rent, pct_hu_1_detached, job_density_simple, retail_density_simple, median_commute), list(scale=scale))\n\n\nWe are in the home stretch! We are very close to being able to make our index. We need to determine how we think each variable is related to opportunity. In some cases, higher values are “good” (more favorable), and in some cases, lower values are “good”. We need to make sure that those values are all moving in the same direction so that when we combine them they do not counter-act each other.\nBelow is a table proposing potential directions for each variable.\n\nNegative: For a variable labeled Negative, higher values are likely to indicate lower levels of opportunity (less favorable).\nPositive: Far a variable labeled Positive, higher values are likely to indicate higher levels of opportunity (more favorable).\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelationship to Opportunity\nCategory\n\n\n\n\nunder18\nProportion of Population under 18\nNegative\nDemographic Structure\n\n\nover65\nProportion of Population over 65\nNegative\nDemographic Structure\n\n\nPNonwhite\nProportion Population Nonwhite\nNegative\nDemographic Structure\n\n\nPForeignborn\nProportion Population Foreign Born\nNegative\nDemographic Structure\n\n\nMHHI\nMedian Household Income\nPositive\nEmployment and Economy\n\n\nP_FHHH\nProportion Female Headed Households\nNegative\nDemographic Structure\n\n\nPov\nProportion Households Below Poverty\nNegative\nDemographic Structure\n\n\nP_Own\nProportion Owner Occupied Housing Units\nPositive\nHousing\n\n\nMHV\nMedian Home Value\nPositive\nHousing\n\n\nCostBurden\nHousing Cost Burden\nNegative\nHousing\n\n\nCANCER\nCancer Risk Index\nNegative\nEnvironmental Health\n\n\nRESP\nRespiratory Hazard Index\nNegative\nEnvironmental Health\n\n\nPTRAF\nTraffic Proximity Index\nNegative\nEnvironmental Health\n\n\nPM25\nParticulate Matter Index\nNegative\nEnvironmental Health\n\n\npct_transit_j2w\nCommuting by Public Transportation\nPositive\nTransportation\n\n\nmedian_gross_rent\nMedian Gross Rent\nNegative\nHousing\n\n\npct_hu_1_detatched\nDetatched Housing Units\nPositive\nHousing\n\n\njob_density_simple\nJob Density\nPositive\nEmployment and Economy\n\n\nretail_density_simple\nRetail Density\nPositive\nEmployment and Economy\n\n\nmedian_commute\nMedian Commute Time\nNegative\nTransportation\n\n\n\nWe are going to create an **additive* index, where standardized values are simply added together. We can transform values so that they are moving in the same direction by simply switching the sign on values that need to be reversed (e.g. multiply by -1). Based upon the above table, Here’s what that would look like:\n\n\nCode\ndataset_scaled&lt;-dataset_scaled |&gt;  mutate(\n  under18_scale = under18_scale*-1,\n  over65_scale = over65_scale*-1,\n  PNonwhite_scale = PNonwhite_scale*-1,\n  PForeignborn_scale = PForeignborn_scale*-1,\n  P_FHHH_scale = P_FHHH_scale*-1,\n  Pov_scale = Pov_scale*-1,\n  CostBurden_scale = CostBurden_scale *-1,\n  CANCER_scale = CANCER_scale*-1,\n  RESP_scale = RESP_scale*-1,\n  PTRAF_scale = PTRAF_scale*-1,\n  PM25_scale = PM25_scale*-1,\n  median_gross_rent_scale = median_gross_rent_scale*-1,\n  median_commute_scale = median_commute_scale*-1\n)\n\n\nBy making these changes, once we add up our indicator values, we are essentially saying that larger values are indicative of greater opportunity and smaller values are indicative of less opportunity. Let’s do two things here - we can add up all of our values to get an overall opportunity score, but let’s also create sub-indicators for each category, as this may be useful information for us to observe.\nTo do this, add several new variables to dataset_scaled that sum together each of the index measures by category, and then create a separate indicator that sums up those category columns into an overall opportunity measure:\nUse the following labels:\n\ndem_index = Demographic Structure\nemp_index = Employment and Economy\nhou_index = Housing\nenv_index = Environmental Health\ntra_index = Transportation\ntot_index = Combined Total Index\n\n\n\nCode\ndataset_scaled&lt;-dataset_scaled |&gt; \n  rowwise() |&gt; \n  mutate(\n  dem_index = sum(under18_scale, over65_scale, PNonwhite_scale, PForeignborn_scale, P_FHHH_scale, Pov_scale, na.rm=TRUE),\n  emp_index = sum(MHHI_scale, job_density_simple_scale, retail_density_simple_scale, na.rm=TRUE),\n  hou_index = sum(P_Own_scale, MHV_scale, CostBurden_scale, median_gross_rent_scale, pct_hu_1_detached_scale, na.rm=TRUE),\n  env_index = sum(CANCER_scale, RESP_scale, PTRAF_scale, PM25_scale, na.rm=TRUE),\n  tra_index = sum(pct_transit_j2w_scale, median_commute_scale,na.rm=TRUE),\n  tot_index = dem_index + emp_index + hou_index + env_index + tra_index\n) \n\n\nSomething’s a little different here that makes a big difference. Did you notice rowwise()? Typically, if we were to ask dplyr to mutate by providing a sum, it would do so by column. rowwise() modifies this and asks for something to happen across a data observation (row) instead of by column. Therefore, when we ask in a mutate statement for the sum of values, we are asking R to add up the values in the same row and then insert the product into a new variable (in this case our named indexes). Pretty cool, right?\nOk - now we have subindex values as well as a total index value. We could analyze these and interpret them as is, but there’s one more thing that we might do to make these easier to interpret. Let’s quickly look at some descriptive stats for our index values and think about what may be challenging for us to interpret:\n\n\nCode\ndataset_scaled |&gt; \n  select(dem_index, emp_index, hou_index, env_index, tra_index, tot_index) |&gt; \n  summary()\n\n\n   dem_index          emp_index         hou_index         env_index      \n Min.   :-10.7759   Min.   :-2.4750   Min.   :-7.3245   Min.   :-14.084  \n 1st Qu.: -2.1811   1st Qu.:-0.8973   1st Qu.:-1.5020   1st Qu.: -1.639  \n Median : -0.1554   Median :-0.3840   Median :-0.1535   Median :  0.000  \n Mean   :  0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   :  0.000  \n 3rd Qu.:  2.1316   3rd Qu.: 0.1484   3rd Qu.: 1.1933   3rd Qu.:  1.443  \n Max.   : 11.8808   Max.   :31.0470   Max.   :10.5857   Max.   :  9.302  \n   tra_index         tot_index      \n Min.   :-6.7759   Min.   :-19.994  \n 1st Qu.:-0.8376   1st Qu.: -3.762  \n Median : 0.0000   Median :  0.000  \n Mean   : 0.0000   Mean   :  0.000  \n 3rd Qu.: 1.1383   3rd Qu.:  3.605  \n Max.   : 3.3514   Max.   : 35.604  \n\n\nLooking at this, there are a few challenges:\n\nSince each subindex component consists of between two and six values, they end up having very different scales. This means that we can’t interpret anything about the magnitude by comparing these values to each other.\nWe have both positive and negative index values. Negative values don’t necessarily mean anything other than that the sum of index values is negative.\n\nOne strategy for making these values more interpretable is to rescale them. Do not confuse rescaling with the type of standardizing which we previously performed by converting our indicator values into z scores. Rescaling will take the data from an existing range and convert it to a new range of values.\nRescaling converts our data from its existing range of values to a new range of values, preserving the magnitude of difference that exists between the values:\nIf we want to convert our data to the range [0,1] here’s what we’d do: \\(x_{rescaled} = \\frac{x-min(x)}{max(x)-min(x)}\\)\nWe can also convert our data to an arbitrary range. Let’s try 0,100: \\(x_{rescaled} = a+\\frac{x-min(x)(b-a)}{max(x) -min(x)}\\)\nFortunately, R has us covered here too - the rescale function in the scales package will rescale to whatever range we wish:\n\n\nCode\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\ndataset_scaled &lt;-dataset_scaled |&gt; \n  ungroup() |&gt; \n  mutate(dem_index = rescale(dem_index, to = c(0,100)),\n         emp_index = rescale(emp_index, to = c(0,100)),\n         hou_index = rescale(hou_index, to = c(0,100)),\n         env_index = rescale(env_index, to = c(0,100)),\n         tra_index = rescale(tra_index, to = c(0,100)),\n         tot_index = rescale(tot_index, to = c(0,100))\n  )\n\ndataset_scaled |&gt;  \n  select(dem_index, emp_index, hou_index, env_index, tra_index, tot_index) |&gt; \n  summary()\n\n\n   dem_index        emp_index         hou_index        env_index     \n Min.   :  0.00   Min.   :  0.000   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.: 37.93   1st Qu.:  4.707   1st Qu.: 32.51   1st Qu.: 53.22  \n Median : 46.88   Median :  6.238   Median : 40.04   Median : 60.23  \n Mean   : 47.56   Mean   :  7.383   Mean   : 40.90   Mean   : 60.23  \n 3rd Qu.: 56.97   3rd Qu.:  7.826   3rd Qu.: 47.56   3rd Qu.: 66.40  \n Max.   :100.00   Max.   :100.000   Max.   :100.00   Max.   :100.00  \n   tra_index        tot_index     \n Min.   :  0.00   Min.   :  0.00  \n 1st Qu.: 58.64   1st Qu.: 29.20  \n Median : 66.91   Median : 35.96  \n Mean   : 66.91   Mean   : 35.96  \n 3rd Qu.: 78.15   3rd Qu.: 42.45  \n Max.   :100.00   Max.   :100.00  \n\n\nThe values are all rescaled so that the minimum is now zero and the maximum value is 100. We use ungroup() here so that if we have used group_by() in the past, R ignores those past groupings when we rescale. Looking at the descriptive statistics for these variables, there’s still some skew in many cases. We might want to try visualizing these distributions so that we can better understand their implications."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#demographics-by-opportunity-levels",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#demographics-by-opportunity-levels",
    "title": "Place Opportunity",
    "section": "Demographics by Opportunity Levels",
    "text": "Demographics by Opportunity Levels\nLet’s take a look at our overall opportunity index again:\n\n\nCode\nggplot(data=dataset_scaled, aes(x=tot_index))+geom_histogram(bins=100)+\n  labs(title = \"Opportunity Index: Histogram\", x=\"Opportunity Index\", y=\"Count\")+\n  theme_classic()\n\n\n\n\n\nHow might we start to think about some of the demographic correlates of who lives in places with higher aggregate opportunity and lower aggregate opportunity? You’ll recall from when we initially created our index that we had downloaded some information on race and ethnicity which was not incorporated into our opportunity measures. We might be interested to see how our composite measure of low and high opportunity are related to the racial and ethnic composition of the census tracts they are from.\nWe might turn towards a some bivariate visualizations to help us here. Let’s start with a simple scatterplot:\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PWhite, y =tot_index))+geom_point()\n\n\n\n\n\nHow would you describe the relationship visualized here (note that I shifted the index values to the y scale and placed percent white on the x scale)? To make it easier, let’s add another geometry to the same plot, in this case, by fitting a line to our data:\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PWhite, y =tot_index))+geom_point()+geom_smooth(method = \"lm\")\n\n\n\n\n\nHow would you describe the relationship between the tract-level concentration of the white population and the overall index score?\nWe might want to polish things up a little more - here are some suggested additions to think about.\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PWhite, y =tot_index))+\n  geom_point(alpha = .6, cex = .75)+ # Use alpha to control the transparency of the points and cex to control their size\n  geom_smooth(method = \"lm\")+\n  labs(x=\"White (%)\", y = \"Combined Opportunity Index\")+ # Add labels to the x and y axes\n  scale_x_continuous(labels = scales::percent)+ # Use the scales package to convert X-axis to percentages\n  theme_minimal() #Use themes to eliminate extra shading on plots\n\n\n\n\n\nYour turn - produce similar plots for the Black, Asian, and Latino populations:\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PBlack, y =tot_index))+geom_point()+geom_smooth(method = \"lm\")\n\n\n\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PAsian, y =tot_index))+geom_point()+geom_smooth(method = \"lm\")\n\n\n\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PLatino, y =tot_index))+geom_point()+geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#extending-your-analysis",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#extending-your-analysis",
    "title": "Place Opportunity",
    "section": "Extending your Analysis",
    "text": "Extending your Analysis\nThe most standard next step in opportunity mapping is…\n\nMaking a map. Drawing from your index data, make a map of your overall index score for your place or region, as well as for index sub-components. What spatial patterns emerge? Map out key demographic information - how do these seem to be related to your overall measures of opportunity?\nCreate summary visualizations and tables that display where the highest and lowest levels of opportunity are within the region? Who lives in these places?\nConsider connecting your opportunity indexes to other data (examples might include public health statistics, transportation costs, presence of public or subsidized housing, etc.) How does a multidimensional opportunity index help you tell stories about these places?\nCreate a journalistic narrative about opportunity in your place or for another region. What makes your story powerful, and what story is it telling?"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#lab-evaluation",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#lab-evaluation",
    "title": "Place Opportunity",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nA demonstrated conceptual understanding of the opportunity concept and the way it becomes operationalized with quantitative demographic data.\nCorrect re-deployment of the code based used in this lab for your own place or region.\nA clear narrative analysis of opportunity for your place.\nProper formatting of tables, figures, and visualizations.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#references",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#references",
    "title": "Place Opportunity",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "syllabus/index.html",
    "href": "syllabus/index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Neighborhood Analysis! I am excited to teach and learn with you this semester."
  },
  {
    "objectID": "syllabus/index.html#course-overview-and-objectives",
    "href": "syllabus/index.html#course-overview-and-objectives",
    "title": "Syllabus",
    "section": "Course Overview and Objectives",
    "text": "Course Overview and Objectives\nThis course teaches techniques for analyzing the demographic, economic, physical, and social conditions that exist at the neighborhood and local government scale. While our focus will be on analyzing current conditions, we will also learn how to tell stories about neighborhood change, and will learn advanced strategies for comparing neighborhoods to each other. We will learn how to describe community characteristics with small area census data, work with local administrative data, and will think about how our analysis of quantitative data fit with other forms of data and engagement to fill in gaps in knowledge.\nBy the end of this course, we will:\n\nBecome familiar with common sources of information used to describe neighborhoods and neighborhood characteristics;\nLearn how to use R, RStudio, and Github to create reproducible analysis;\nLearn how to work collaboratively to tell compelling stories for deliberation and decision-making."
  },
  {
    "objectID": "syllabus/index.html#course-format-and-expectations",
    "href": "syllabus/index.html#course-format-and-expectations",
    "title": "Syllabus",
    "section": "Course Format and Expectations",
    "text": "Course Format and Expectations\nOur focus during class sessions will be on learning by doing. Our class will meet twice per week - Tuesdays will typically be devoted to introducing new strategies and information, and Thursdays will typically be devoted to applying these strategies. Outside of class, you’ll use class resources and video tutorials to learn basic concepts and to prepare to apply these concepts in class.\n\nClass Computing Environment\nUnless otherwise noted, please plan on bringing to class a computer that will run R and RStudio (available for PC/Mac/Linux) and for which you have administrative privileges.\n\n\nWhat to Expect from me\n\nEmail: The easiest way to communicate with me outside of class is via email. I try to respond to emails sent during the week within 24 hours. Emails sent over the weekend will receive a response within 48 hours. If you don’t hear from me after that amount of time, it’s okay to nudge me to respond.\nOne-on-one meetings: I am available to speak one on one - you can book an appointment via my Calendly page. Of course, I am happy to chat before or after class if I am available.\n\n\n\n\n\n\nTroubleshooting: Plan on using our class Github Discussion Forum, email, and office hours to get help with troubleshooting problems as they arise in your work. The Resources page provides thoughts and resources for troubleshooting. I also encourage you to work with others in the class to troubleshoot problems - it is highly likely that others in the class have encountered similar problems, and this allows us to build a repository of our problems and responses.\n\n\n\nWhat I Expect From You\n\nBe Present: I expect that you’ll engage fully in our course sessions and in our class community.\nActively Support Each Other: I expect that each of you will take on individual leadership roles within our class, that includes actively supporting our learning community over the course of the semester. This class assumes collaboration and sharing as part of our learning model.\nRead with Care: This course focuses on learning by doing, however, there are important details contained within the documentation on our course website and within reading selections. Details matter in this class - be intentional about reading carefully and completely important course documents (including this syllabus).\nAct with Integrity: I expect that you will act with integrity in all that you do in this class. The class contract grading system places trust in you to not just meet the nominal standards contained within the contract, but to push yourself to produce your best work.\nSeek Balance: I expect that you’ll actively work to find balance between the many demands in your life. This means budgeting adequate time to engage fully in our course but also budgeting time for adequate rest and sleep, exercise, and other actions that support your mental and physical health.\n\n\n\nCourse Prerequisites\nUP 570: Neighborhood Analysis is open to students with graduate-level standing. While students will benefit from having some prior familiarity with R and RStudio or will have taken UP 517: Data Science for Planners, the class this semester will accomodate students with no prior background in coding or statistical analysis.\nOur first few course sessions will focus on ensuring that we are all familiar with basic workflows and methods which we’ll make use of over the course of the semester.\nPlease talk with me if you have any questions regarding whether this course is right for you."
  },
  {
    "objectID": "syllabus/index.html#evaluation-and-course-expectations",
    "href": "syllabus/index.html#evaluation-and-course-expectations",
    "title": "Syllabus",
    "section": "Evaluation and Course Expectations",
    "text": "Evaluation and Course Expectations\nYou will find detailed information on assignments, evaluation, and grading in the Assignments section.\n\nClass Attendance\nYou are expected to attend all of our class sessions in order to meet my standards for adequate performance in this course. Please notify me in advance of any course sessions which you will miss. Your final grades will be reduced by 1% per unexcused absence.\nFor those students who need to miss class due to a religious observance, please complete the Request for Accommodation for Religious Observances form should any instructors require an absence letter in order to manage the absence. In order to best facilitate planning and communication between students and faculty, I request that students make requests for absence letters as early as possible in the semester in which the request applies.\nFor more information on attendance policy as described in the University of Illinois Student Code, please see Sections 1-501 and 1-502.\n\n\nUse of AI Tools\nArtificial intelligence tools like Chat GPT have quickly made waves with their ability to produce text, code, and explanations from natural language prompts. I encourage (and at times will expect) you to integrate such tools into your problem solving strategies and workflow in our class. These tools must, however, be used with care and with understanding around how they synthesize and produce information.\nA few words of guidance:\n\nAssume that any code produced will require additional tweaking or troubleshooting to be implemented effectively in your workflow.\nAssume any facts or figures rendered via AI models are incorrect.\nUse these tools to help you break through coding or analysis challenges, not for writing up your narrative or findings.\nWhile you do not need to provide a citation for Chat GPT or other AI tools in your narrative or references, please indicate in any methods statements that these tools were employed. Please also reflect upon your application of these tools in your assignment submission reflections.\n\nA few related expectations:\n\nLet’s keep a running dialog about how you’re employing these tools, what some of the challenges are, and how you may want to integrate such technology into your workflows.\nYou are ultimately responsible for all content represented in your work. As part of the class pedagogy, you will electronically publish your assignments - this means you are publicly responsible for the content you produce.\nAll writing should be your own and written by you and not generative AI (unless you are quoting or citing someone else).\nDeviations from these expectations will be treated as academic dishonesty and are subject to review and disposition based upon the University’s honor code."
  },
  {
    "objectID": "syllabus/index.html#honor-code",
    "href": "syllabus/index.html#honor-code",
    "title": "Syllabus",
    "section": "Honor Code",
    "text": "Honor Code\nThe Illinois Student Code states: “It is the responsibility of the student to refrain from infractions of academic integrity, from conduct that may lead to suspicion of such infractions, and from conduct that aids others in such infractions.” Note that you are subject to the Honor Code, as well as procedures for addressing violations to the Code, regardless of whether you have read it and understand it. According to the Code, “ignorance is no excuse.”\nTo meet this standard in this course, note the following: in written work, all ideas (as well as data or other information) that are not your own must be cited. Note that ideas that require citation may not have been published or written down anywhere. While you are free—and indeed encouraged—to discuss assignments with your peers, all of your analysis, and writing should be your own. The consequence for violating these expectations may include receiving no credit for the assignment in question, and may include automatic failure of the course.\n\n\n\n\n\n\nA Simple Standard\n\n\n\nPut simply, give credit where credit is due."
  },
  {
    "objectID": "syllabus/index.html#learning-environment-and-support",
    "href": "syllabus/index.html#learning-environment-and-support",
    "title": "Syllabus",
    "section": "Learning Environment and Support",
    "text": "Learning Environment and Support\n\nOur Learning Environment\nThe Department of Urban and Regional Planning is committed to maintaining a learning environment that is rooted in the goals and responsibilities of professional planners. By enrolling in a class offered by the Department of Urban and Regional Planning, students agree to be responsible for maintaining an atmosphere of mutual respect in all activities, including lectures, discussions, labs, projects, and extracurricular opportunities. The University of Illinois Student Code should be considered part of this syllabus. See in particular Student Code Article 1-Student Rights and Responsibilities, Part 1. Student Rights: §1-102.\n\n\nOur Class Environment\nAs part of our classroom and university community, you have an obligation to do the following:\n\nAttend all class sessions if you are feeling well.\nIf you feel ill, do not come to class.\nIf you test positive for covid or have an exposure that requires testing or quarantine, do not come to class.\nPlease be respectful of all members of our learning community and their decisions regarding health and safety precautions.\n\n\n\nAccomodations for Students with Disabilities\nIf you need accommodations for any sort of disability, please make an office hours appointment so we can discuss your needs and ways I can support your learning. To ensure that disability-related concerns are properly addressed, students who require assistance to participate in this class should contact Disability Resources and Educational Services (DRES). DRES provides students with academic accommodations, access, and support services. To contact DRES you may visit 1207 S. Oak St., Champaign, call 333-4603 (V/TDD), or e-mail disability@illinois.edu.\n\n\nSexual Misconduct Reporting Obligation\nThe University of Illinois is committed to combating sexual misconduct. Faculty and staff members are required to report any instances of sexual misconduct to the University’s Title IX and Disability Office. In turn, an individual with the Title IX and Disability Office will provide information about rights and options, including accommodations, support services, the campus disciplinary process, and law enforcement options.\nA list of the designated University employees who, as counselors, confidential advisors, and medical professionals, do not have this reporting responsibility and can maintain confidentiality, can be found here. Other information about resources and reporting is available at wecare.illinois.edu.\n\n\nMental Health\nThe University of Illinois offers a variety of confidential services including individual and group counseling, crisis intervention, psychiatric services, and specialized screenings which are covered through the Student Health Fee. If you or someone you know experiences mental health concerns, please contact or visit any of the University’s resources provided below. Getting help is a smart and courageous thing to do for yourself and for those who care about you.\n\nCounseling Center: (217) 333-3704\nMcKinley Health Center: (217) 333-2700\n988 Suicide and Crisis Lifeline: (800) 273-8255\nRosecrance Crisis Line (217) 359-4141\n\nIf you are in immediate danger, call 911."
  },
  {
    "objectID": "syllabus/index.html#your-wellness",
    "href": "syllabus/index.html#your-wellness",
    "title": "Syllabus",
    "section": "Your Wellness",
    "text": "Your Wellness\nWellness at Illinois: Throughout the semester, you may need assistance coping with emotional, interpersonal, or academic concerns. wellness.illinois.edu is a good resource to identify help for yourself or others who may need assistance. Please do not hesitate to reach out or request assistance.\n\nLearning R and Your Wellness\nProblem solving is a major part of being a coder - you will face challenges related to working with the software this semester. That’s a given, and is an expected part of learning in this class. Part of the goal is to teach you how to understand the intentionality and logic behind the software so that you can anticipate where errors are likely to occur.\n\n\n\n\n\n\nExperiential Learning\n\n\n\nThe only way to do this is to encounter errors - and there will be many!\n\n\nYou are entering an intermediate to advanced stage of learning a new language, its grammar, and its application. While this will be frustrating at times, there is a major payoff in the capacity you will gain in analytic skills and problem-solving. This payoff will come slowly over time - do not expect it to come easily.\n\nYou are not alone in this struggle. In addition to your classmates and others who are going through the same thing, there is a large R user community, and lots of existing documentation and troubleshooting. Any problem you will encounter has likely been encountered and addressed before.\n\n\n\n\n\n\nTip\n\n\n\nWhen I run into an error, after an initial check for simple issues like closing parentheses and spelling errors, I copy and paste error codes directly into a web search to see how others have dealt with similar problems. I encourage you to do the same.\n\n\nYou got this, and there will be a payoff, so long as you use the tools consistently - I promise!\n\n\nWe’re Here For You\nWe’ve been living in particularly abnormal times for the last few years - full stop. It would be irresponsible to expect that teaching and learning would occur “normally” right now. We continue to teach and learn under emergency circumstances.\nAs you face challenges this semester (and beyond) I need you to communicate with me, either during our course sessions or individually. You can schedule an appointment with me at your convenience via my Calendly page. I promise to listen, to be a resource, and to help in any way that I can - if I can’t help you, I will find someone who can."
  },
  {
    "objectID": "schedule/Session_template.html#before-class",
    "href": "schedule/Session_template.html#before-class",
    "title": "NAME",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/Session_template.html#reflect",
    "href": "schedule/Session_template.html#reflect",
    "title": "NAME",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/Session_template.html#slides",
    "href": "schedule/Session_template.html#slides",
    "title": "NAME",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/Session_template.html#resources-for-further-exploration",
    "href": "schedule/Session_template.html#resources-for-further-exploration",
    "title": "NAME",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/28_finalpresent.html",
    "href": "schedule/28_finalpresent.html",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/28_finalpresent.html#session-description",
    "href": "schedule/28_finalpresent.html#session-description",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/28_finalpresent.html#todays-presenters",
    "href": "schedule/28_finalpresent.html#todays-presenters",
    "title": "Final Presentations",
    "section": "Today’s Presenters",
    "text": "Today’s Presenters\nReminder - each presenter has ten minutes to share."
  },
  {
    "objectID": "schedule/28_finalpresent.html#providing-feedback",
    "href": "schedule/28_finalpresent.html#providing-feedback",
    "title": "Final Presentations",
    "section": "Providing Feedback",
    "text": "Providing Feedback\nPlease provide feedback to each other in the following form - I will consolidate your written feedback on each presentation and will send it to you."
  },
  {
    "objectID": "schedule/26_finalpresent.html",
    "href": "schedule/26_finalpresent.html",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/26_finalpresent.html#session-description",
    "href": "schedule/26_finalpresent.html#session-description",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/26_finalpresent.html#todays-presenters",
    "href": "schedule/26_finalpresent.html#todays-presenters",
    "title": "Final Presentations",
    "section": "Today’s Presenters",
    "text": "Today’s Presenters\nReminder - each presenter has ten minutes to share."
  },
  {
    "objectID": "schedule/26_finalpresent.html#providing-feedback",
    "href": "schedule/26_finalpresent.html#providing-feedback",
    "title": "Final Presentations",
    "section": "Providing Feedback",
    "text": "Providing Feedback\nPlease provide feedback to each other in the following form - I will consolidate your written feedback on each presentation and will send it to you."
  },
  {
    "objectID": "schedule/24_fieldobs.html",
    "href": "schedule/24_fieldobs.html",
    "title": "Field Observations",
    "section": "",
    "text": "So far, we have started to learn how to tell stories about places using existing indicators. With so many available sources of existing information, it can be easy to experience a disconnect between the values of those indicators and the complexity of what they represent. To think more about what grounds our analysis as planners, this week we will go explore a neighborhood in person and then think through elements of the stories that may help us describe that place through indicators.\nWe are going to spend this class session doing field observation of the West Urbana neighborhood which is designated as an American Planning Association Great Place. We will meet at Carle Park in Urbana at the intersection of Carle and Indiana streets.\nCome prepared to spend the course session exploring the neighborhood. - We will meet at at Carle Park in Urbana on Monday to to focus on field observation. - On Wednesday, we will meet at TBH and will think about our plans for storytelling and description about the West Urbana neighborhood. - Following the completion of our lab, you’ll submit your individual reflection via GitHub. Repository link."
  },
  {
    "objectID": "schedule/24_fieldobs.html#session-description",
    "href": "schedule/24_fieldobs.html#session-description",
    "title": "Field Observations",
    "section": "",
    "text": "So far, we have started to learn how to tell stories about places using existing indicators. With so many available sources of existing information, it can be easy to experience a disconnect between the values of those indicators and the complexity of what they represent. To think more about what grounds our analysis as planners, this week we will go explore a neighborhood in person and then think through elements of the stories that may help us describe that place through indicators.\nWe are going to spend this class session doing field observation of the West Urbana neighborhood which is designated as an American Planning Association Great Place. We will meet at Carle Park in Urbana at the intersection of Carle and Indiana streets.\nCome prepared to spend the course session exploring the neighborhood. - We will meet at at Carle Park in Urbana on Monday to to focus on field observation. - On Wednesday, we will meet at TBH and will think about our plans for storytelling and description about the West Urbana neighborhood. - Following the completion of our lab, you’ll submit your individual reflection via GitHub. Repository link."
  },
  {
    "objectID": "schedule/24_fieldobs.html#before-class",
    "href": "schedule/24_fieldobs.html#before-class",
    "title": "Field Observations",
    "section": "Before Class",
    "text": "Before Class\n\nWrite a short reflection on your pre-existing impressions of the West Urbana neighborhood. Some of you may be very familiar, and others may not at all. Reflect based upon what you know or have heard.\nWear comfortable clothes, and be prepared to be outside for the duration of our course session.\nYou may wish to bring a notebook with you as well as a phone or camera to document your observations."
  },
  {
    "objectID": "schedule/24_fieldobs.html#during-class",
    "href": "schedule/24_fieldobs.html#during-class",
    "title": "Field Observations",
    "section": "During Class",
    "text": "During Class\nWe will split up into groups in order to observe elements of the West Urbana Neighborhood (as a reminder, Monday’s group assignments are below). Working in groups, each group member will be responsible for focusing on one element of observation (you may double up in an area of your choice if you have more than four in your group):\n\nInfrastructure and Environment\nEconomy and Housing\nHealth and Wellbeing\nSense of Place\n\nObservation 1: In your group, spend 20 minutes walking around the neighborhood, taking in and observing your specific element. Then take 5-10 minutes to write down your overall impressions and any questions you have after this brief exploration.\nObservation 2: As a group, pick a block to systematically analyze. Spend 20 minutes observing your specific element as it is reflected on your block. You may want to take pictures, sketch a map, or use other methods to record what you observe.\nObservation 3: Spend the remaining 30 minutes comparing notes with your other group members. Record a list of shared questions you have about the block and portions of the neighborhood that you explored. Reflect as a group upon what you were able to observe.\nAt the end of our class time you can leave."
  },
  {
    "objectID": "schedule/24_fieldobs.html#groups",
    "href": "schedule/24_fieldobs.html#groups",
    "title": "Field Observations",
    "section": "Groups",
    "text": "Groups\n\nGroup 1\nTBD\n\n\nGroup 2\nTBD\n\n\nGroup 3\nTBD\n\n\nGroup 4\nTBD"
  },
  {
    "objectID": "schedule/24_fieldobs.html#slides",
    "href": "schedule/24_fieldobs.html#slides",
    "title": "Field Observations",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/24_fieldobs.html#resources-for-further-exploration",
    "href": "schedule/24_fieldobs.html#resources-for-further-exploration",
    "title": "Field Observations",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nUrban Institute: Observation\nParticipant Observation and the Development of Urban Neighborhood Policy"
  },
  {
    "objectID": "schedule/22_CheckIn.html",
    "href": "schedule/22_CheckIn.html",
    "title": "Final Project Check-In: Monday",
    "section": "",
    "text": "This week is devoted to collaboratively workshopping your final projects. During these sessions, we’ll be examining your best evidence - a fact or analysis that you believe best makes the point you’re trying to convey to your audience.\nCome to class prepared to share your best evidence with the entire class. You will each be asked to take a few minutes to present your best evidence and receive feedback on it."
  },
  {
    "objectID": "schedule/22_CheckIn.html#session-description",
    "href": "schedule/22_CheckIn.html#session-description",
    "title": "Final Project Check-In: Monday",
    "section": "",
    "text": "This week is devoted to collaboratively workshopping your final projects. During these sessions, we’ll be examining your best evidence - a fact or analysis that you believe best makes the point you’re trying to convey to your audience.\nCome to class prepared to share your best evidence with the entire class. You will each be asked to take a few minutes to present your best evidence and receive feedback on it."
  },
  {
    "objectID": "schedule/22_CheckIn.html#before-class",
    "href": "schedule/22_CheckIn.html#before-class",
    "title": "Final Project Check-In: Monday",
    "section": "Before Class",
    "text": "Before Class\nPrepare your best evidence for sharing. This might be preparing or exporting a bit of analysis or coming the class prepared to articulate and share this evidence.\nTo help expedite our workshopping of best evidence, please prepare a single slide containing your best evidence (PowerPoint, Keynote, or PDF are fine). I will compile these in advance of the class so that we can spend as much time as possible workshopping each of your arguments and evidence.\nPlease upload your best evidence slide to Box before 9am (Monday) morning. This slide does not need to be visually refined – it just needs your name and the evidence in whatever form you plan to share it (image, map, table, whatever)."
  },
  {
    "objectID": "schedule/22_CheckIn.html#reflect",
    "href": "schedule/22_CheckIn.html#reflect",
    "title": "Final Project Check-In: Monday",
    "section": "Reflect",
    "text": "Reflect\n\nWhat’s the major story you’re telling in your policy memorandum?\nWho is the audience for the story? What’s the best way to convey your point?\nWhat are the counterarguments or counterfactuals to the evidence that you are sharing. How will you engage with that?\nWhat types of technical issues are you running into that we could help address? Are there any technical concepts or issues that remain unclear that we should engage with or address as a class? If so, come prepared to share or discuss."
  },
  {
    "objectID": "schedule/20_healthequity.html",
    "href": "schedule/20_healthequity.html",
    "title": "Health Equity",
    "section": "",
    "text": "A growing public conversation recognizes that individual health and collective health of populations is determined by a range of factors, including many that are social and rooted in social relationships. As this conversation grows and evolves, planners have an important role to play helping to disentangle and describe how places matter and influence the health of populations.\nThis week, we will explore a case study strategy for linking social determinants measures rooted in neighborhood indicators with measures of health outcomes. Rather than focus on building out new methodological or analytical techniques, our focus this week will be on storytelling and in thinking about how to engage communities in further conversations about their health and wellbeing."
  },
  {
    "objectID": "schedule/20_healthequity.html#session-description",
    "href": "schedule/20_healthequity.html#session-description",
    "title": "Health Equity",
    "section": "",
    "text": "A growing public conversation recognizes that individual health and collective health of populations is determined by a range of factors, including many that are social and rooted in social relationships. As this conversation grows and evolves, planners have an important role to play helping to disentangle and describe how places matter and influence the health of populations.\nThis week, we will explore a case study strategy for linking social determinants measures rooted in neighborhood indicators with measures of health outcomes. Rather than focus on building out new methodological or analytical techniques, our focus this week will be on storytelling and in thinking about how to engage communities in further conversations about their health and wellbeing."
  },
  {
    "objectID": "schedule/20_healthequity.html#before-class",
    "href": "schedule/20_healthequity.html#before-class",
    "title": "Health Equity",
    "section": "Before Class",
    "text": "Before Class\nDougherty, Geoff B., Sherita H. Golden, Alden L. Gross, Elizabeth Colantouoni, and Lorraine T. Dean. (2020). Measuring Structural Racism and Its Association with BMI. American Journal of Preventative Medicine .\nPlease also take a look at the appendix section on variables used in their approach. .\nCome to class ready to workshop ways in which we can tell stories about health and wellbeing."
  },
  {
    "objectID": "schedule/20_healthequity.html#reflect",
    "href": "schedule/20_healthequity.html#reflect",
    "title": "Health Equity",
    "section": "Reflect",
    "text": "Reflect\n\nWhat are some tropes or conventions we use to talk about individual health? Collective health?\nHow might health disparities (at a neighborhood or population level) be connected to the many themes we’ve talked about in this class?\nIs population health a planning issue? What role do practitioners and researchers in planning have for influencing how we think and act around health at the neighborhood level?"
  },
  {
    "objectID": "schedule/20_healthequity.html#slides",
    "href": "schedule/20_healthequity.html#slides",
    "title": "Health Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/20_healthequity.html#resources-for-further-exploration",
    "href": "schedule/20_healthequity.html#resources-for-further-exploration",
    "title": "Health Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/18_transequity.html",
    "href": "schedule/18_transequity.html",
    "title": "Transit Equity",
    "section": "",
    "text": "This week, we’ll be examining principles of transit equity. Our discussions and lab will be facilitated by Ouafa, and inspired by her work."
  },
  {
    "objectID": "schedule/18_transequity.html#session-description",
    "href": "schedule/18_transequity.html#session-description",
    "title": "Transit Equity",
    "section": "",
    "text": "This week, we’ll be examining principles of transit equity. Our discussions and lab will be facilitated by Ouafa, and inspired by her work."
  },
  {
    "objectID": "schedule/18_transequity.html#before-class",
    "href": "schedule/18_transequity.html#before-class",
    "title": "Transit Equity",
    "section": "Before Class",
    "text": "Before Class\nIntroduction to Transportation Equity \nIntroduction to Measuring Transportation Equity \nTransportation Equity \nMobility justice \nMobility justice \nEquiticity"
  },
  {
    "objectID": "schedule/18_transequity.html#reflect",
    "href": "schedule/18_transequity.html#reflect",
    "title": "Transit Equity",
    "section": "Reflect",
    "text": "Reflect\n\nWhat are other urban factors that contribute to transportation and accessibility to jobs and amenities?\nHow can these measures help or put at disadvantage impacted communities and groups?\nHow can planners and policy makers facilitate sustainable (long term) interventions that assure equitable access to mobility?"
  },
  {
    "objectID": "schedule/18_transequity.html#slides",
    "href": "schedule/18_transequity.html#slides",
    "title": "Transit Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/18_transequity.html#resources-for-further-exploration",
    "href": "schedule/18_transequity.html#resources-for-further-exploration",
    "title": "Transit Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/16_opportunity.html",
    "href": "schedule/16_opportunity.html",
    "title": "Place Opportunity",
    "section": "",
    "text": "In this session, we’ll begin to examine opportunity analysis and mapping, which is a strategy used to compare multiple measures of sociodemographic wellbeing to create a single measure of how supportive a place is for economic and social mobility. Opportunity mapping in various forms has been used to craft legal remedies to discrimination and segregation, to design policy interventions, and as a source of information for planning and the allocation of resources.\nYour readings for today provide a) a contemporary overview of opportunity mapping, and b) a conceptual overview of the application of opportunity mapping by one of it’s originators, john powell.\nIn addition to our discussion of opportunity mapping, we’ll also take a few moments to check in on your final assignment progress, your detailed project description, and more generally about how things are going with the class."
  },
  {
    "objectID": "schedule/16_opportunity.html#session-description",
    "href": "schedule/16_opportunity.html#session-description",
    "title": "Place Opportunity",
    "section": "",
    "text": "In this session, we’ll begin to examine opportunity analysis and mapping, which is a strategy used to compare multiple measures of sociodemographic wellbeing to create a single measure of how supportive a place is for economic and social mobility. Opportunity mapping in various forms has been used to craft legal remedies to discrimination and segregation, to design policy interventions, and as a source of information for planning and the allocation of resources.\nYour readings for today provide a) a contemporary overview of opportunity mapping, and b) a conceptual overview of the application of opportunity mapping by one of it’s originators, john powell.\nIn addition to our discussion of opportunity mapping, we’ll also take a few moments to check in on your final assignment progress, your detailed project description, and more generally about how things are going with the class."
  },
  {
    "objectID": "schedule/16_opportunity.html#before-class",
    "href": "schedule/16_opportunity.html#before-class",
    "title": "Place Opportunity",
    "section": "Before Class",
    "text": "Before Class\nStromberg, Brian. (2016). Opportunity Mapping. National Housing Conference. \npowell, john. (2005). Remedial Phase Expert Report of john powell in Thompson v. HUD. \nBalachandran, Sowmya, and Andrew Greenlee. (2022). Examining Spatial Opportunity for Local Action: From Theory to Practice. Journal of Planning Education and Research."
  },
  {
    "objectID": "schedule/16_opportunity.html#reflect",
    "href": "schedule/16_opportunity.html#reflect",
    "title": "Place Opportunity",
    "section": "Reflect",
    "text": "Reflect\n\nWhat does place opportunity mean to you? What makes a place more (or less) opportune?\nAre there some universal dimensions of place opportunity? Some that are more specific to certain population groups?\nWhat types of practices support opportunity mapping? How can we use the outputs from opportunity mapping exercises for deliberation and policy decision-making?"
  },
  {
    "objectID": "schedule/16_opportunity.html#slides",
    "href": "schedule/16_opportunity.html#slides",
    "title": "Place Opportunity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/16_opportunity.html#resources-for-further-exploration",
    "href": "schedule/16_opportunity.html#resources-for-further-exploration",
    "title": "Place Opportunity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/14_neighborhood.html",
    "href": "schedule/14_neighborhood.html",
    "title": "Neighborhood Change",
    "section": "",
    "text": "In this session, we’ll begin exploring the measurement of neighborhood change as well as underlying theories of transition."
  },
  {
    "objectID": "schedule/14_neighborhood.html#session-description",
    "href": "schedule/14_neighborhood.html#session-description",
    "title": "Neighborhood Change",
    "section": "",
    "text": "In this session, we’ll begin exploring the measurement of neighborhood change as well as underlying theories of transition."
  },
  {
    "objectID": "schedule/14_neighborhood.html#before-class",
    "href": "schedule/14_neighborhood.html#before-class",
    "title": "Neighborhood Change",
    "section": "Before Class",
    "text": "Before Class\nWBEZ: There Goes the Neighborhood\nWho Can Live in Chicago?: A Tale of Three Cities\nThe Socioeconomic Change of Chicago’s Community Areas (1970-2010)"
  },
  {
    "objectID": "schedule/14_neighborhood.html#reflect",
    "href": "schedule/14_neighborhood.html#reflect",
    "title": "Neighborhood Change",
    "section": "Reflect",
    "text": "Reflect\n\nWhat role should planners take in managing neighborhood change?\nHow do planners balance changes across multiple neighborhoods?\nHow can value judgements placed on change influence the types of solutions we propose (and their feasibility)?"
  },
  {
    "objectID": "schedule/14_neighborhood.html#slides",
    "href": "schedule/14_neighborhood.html#slides",
    "title": "Neighborhood Change",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/14_neighborhood.html#resources-for-further-exploration",
    "href": "schedule/14_neighborhood.html#resources-for-further-exploration",
    "title": "Neighborhood Change",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nThe Three Cities Within Toronto: Income Polarization Amongst Toronto’s Neighbourhoods, 1970-2005"
  },
  {
    "objectID": "schedule/12_segregation.html",
    "href": "schedule/12_segregation.html",
    "title": "Segregation",
    "section": "",
    "text": "This week, we will begin a conversation about the nature of residential segregation, and the common ways in which it is measured. Our lab this week will build off of the general descriptions which we worked on last week in which we identified and mapped the largest racial groups by census tract."
  },
  {
    "objectID": "schedule/12_segregation.html#session-description",
    "href": "schedule/12_segregation.html#session-description",
    "title": "Segregation",
    "section": "",
    "text": "This week, we will begin a conversation about the nature of residential segregation, and the common ways in which it is measured. Our lab this week will build off of the general descriptions which we worked on last week in which we identified and mapped the largest racial groups by census tract."
  },
  {
    "objectID": "schedule/12_segregation.html#before-class",
    "href": "schedule/12_segregation.html#before-class",
    "title": "Segregation",
    "section": "Before Class",
    "text": "Before Class\nCunningham, Mary K., and Augrey Droesch. Neighborhood Quality and Racial Segregation. The Urban Institute. \nU.S. Bureau of the Census: Measures of Residential Segregation"
  },
  {
    "objectID": "schedule/12_segregation.html#reflect",
    "href": "schedule/12_segregation.html#reflect",
    "title": "Segregation",
    "section": "Reflect",
    "text": "Reflect\n\nWhy, in your opinion, does segregation remain an enduring characteristic for most American cities, despite efforts to address it?\nHow can visualization of segregation (and its consequences) make a difference?\nWhat can segregation measures capture well? What aspects of segregation are more challenging to measure?"
  },
  {
    "objectID": "schedule/12_segregation.html#slides",
    "href": "schedule/12_segregation.html#slides",
    "title": "Segregation",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/12_segregation.html#resources-for-further-exploration",
    "href": "schedule/12_segregation.html#resources-for-further-exploration",
    "title": "Segregation",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/10_census.html",
    "href": "schedule/10_census.html",
    "title": "Population and the Census",
    "section": "",
    "text": "In this session, we will talk about how planners measure basic dimensions of population and change in population at the neighborhood level. We’ll also discuss some of the basic principles and features of the main Census products which planners use to describe places and the people who live and work in them. We’ll introduce this week’s lab, which focuses on working with basic data on population characteristics."
  },
  {
    "objectID": "schedule/10_census.html#session-description",
    "href": "schedule/10_census.html#session-description",
    "title": "Population and the Census",
    "section": "",
    "text": "In this session, we will talk about how planners measure basic dimensions of population and change in population at the neighborhood level. We’ll also discuss some of the basic principles and features of the main Census products which planners use to describe places and the people who live and work in them. We’ll introduce this week’s lab, which focuses on working with basic data on population characteristics."
  },
  {
    "objectID": "schedule/10_census.html#before-class",
    "href": "schedule/10_census.html#before-class",
    "title": "Population and the Census",
    "section": "Before Class",
    "text": "Before Class\nKlosterman 2 .\nRead our lab background and bring any questions to today’s class. We will work on the lab during our class time on Wednesday."
  },
  {
    "objectID": "schedule/10_census.html#reflect",
    "href": "schedule/10_census.html#reflect",
    "title": "Population and the Census",
    "section": "Reflect",
    "text": "Reflect\n\nWhat types of characteristics might be well-represented within Census data? What characteristics are harder to measure or represent?\nAs we’ve discussed in class, the census is a dynamic and evolving survey, and the questions we ask are a valuable window into the social questions and issues at a given time. What questions do you think we should be asking in this current moment? How well are they reflected in the census (as you know it)?"
  },
  {
    "objectID": "schedule/10_census.html#slides",
    "href": "schedule/10_census.html#slides",
    "title": "Population and the Census",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/10_census.html#resources-for-further-exploration",
    "href": "schedule/10_census.html#resources-for-further-exploration",
    "title": "Population and the Census",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\n2020 Census Enumeration Form \n2020 Census Post Enumeration Survey Documentation \nDifferential Privacy and the 2020 US Census \nAmerican Community Survey Sample Size"
  },
  {
    "objectID": "schedule/08_places.html",
    "href": "schedule/08_places.html",
    "title": "Describing Places",
    "section": "",
    "text": "In this session, we’ll spend time talking about how to describe places, as well as some frameworks for making place comparisons. We will think about how the types of stories we tell about places connect with typical arguments and tropes which guide place description and analysis within urban planning and policy settings.\nIn addition to our discussion of places, we’ll debrief your two labs from last week and introduce this week’s lab."
  },
  {
    "objectID": "schedule/08_places.html#session-description",
    "href": "schedule/08_places.html#session-description",
    "title": "Describing Places",
    "section": "",
    "text": "In this session, we’ll spend time talking about how to describe places, as well as some frameworks for making place comparisons. We will think about how the types of stories we tell about places connect with typical arguments and tropes which guide place description and analysis within urban planning and policy settings.\nIn addition to our discussion of places, we’ll debrief your two labs from last week and introduce this week’s lab."
  },
  {
    "objectID": "schedule/08_places.html#before-class",
    "href": "schedule/08_places.html#before-class",
    "title": "Describing Places",
    "section": "Before Class",
    "text": "Before Class\nLynch, Kevin. (1960). The Image of the City. Chapter 3 ."
  },
  {
    "objectID": "schedule/08_places.html#reflect",
    "href": "schedule/08_places.html#reflect",
    "title": "Describing Places",
    "section": "Reflect",
    "text": "Reflect\n\nWe intuitively tell lots of stories about places. What are some of the most evocative tropes you can think of that could apply to places? What makes for a good story about place?\nWhat are some of the benefits and challenges of making place comparisons? How does this feel from the perspective of an analyst? What dangers or challenges does the analyst face when telling comparative stories to others?"
  },
  {
    "objectID": "schedule/08_places.html#slides",
    "href": "schedule/08_places.html#slides",
    "title": "Describing Places",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/08_places.html#resources-for-further-exploration",
    "href": "schedule/08_places.html#resources-for-further-exploration",
    "title": "Describing Places",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nAnti-Eviction Mapping Project \nEviction Lab Methodology \nBloomberg: What’s Really Warming the World \nDesign Justice Project \nGlobal Atlas of Environmental Justice \nFive Thirty Eight: Kidnapping of Girls in Nigeria Is Part of a Worsening Problem \nMemorandum on Transparency and Open Government"
  },
  {
    "objectID": "schedule/06_sharing.html",
    "href": "schedule/06_sharing.html",
    "title": "Sharing Your Work",
    "section": "",
    "text": "This session builds upon the work on your last lab. In that lab, you worked on developing several workflows that will support your work over the course of the semester. Coming into this lab, you should have a formatted Quarto markdown document. In this session, we’ll talk about strategies for sharing that work, will configure your computer to communicate with GitHub, and will create your first publicly facing websites."
  },
  {
    "objectID": "schedule/06_sharing.html#session-description",
    "href": "schedule/06_sharing.html#session-description",
    "title": "Sharing Your Work",
    "section": "",
    "text": "This session builds upon the work on your last lab. In that lab, you worked on developing several workflows that will support your work over the course of the semester. Coming into this lab, you should have a formatted Quarto markdown document. In this session, we’ll talk about strategies for sharing that work, will configure your computer to communicate with GitHub, and will create your first publicly facing websites."
  },
  {
    "objectID": "schedule/06_sharing.html#before-class",
    "href": "schedule/06_sharing.html#before-class",
    "title": "Sharing Your Work",
    "section": "Before Class",
    "text": "Before Class\n\nRead through the entire lab background description before approaching lab tasks.\nBe prepared to access the formatted Quarto notebook you worked on in the last lab that contains your analysis of Chicago community areas.\nBe prepared to access your Lab 1 reflection."
  },
  {
    "objectID": "schedule/06_sharing.html#in-class",
    "href": "schedule/06_sharing.html#in-class",
    "title": "Sharing Your Work",
    "section": "In Class",
    "text": "In Class\nOuafa Zoom Link\nHello everyone - live from class!"
  },
  {
    "objectID": "schedule/06_sharing.html#reflect",
    "href": "schedule/06_sharing.html#reflect",
    "title": "Sharing Your Work",
    "section": "Reflect",
    "text": "Reflect\n\nHow can planners and others engaging directly in public policy discourse and debate leverage emotion in their analysis in ways that generate meaning and connection without manipulating or leading towards particular conclusions?\nIs there such thing a “neutral” data analysis?\nCan you think of classification systems that may have unintended consequences or biases in data that you’ve used for urban analysis in the past?"
  },
  {
    "objectID": "schedule/06_sharing.html#slides",
    "href": "schedule/06_sharing.html#slides",
    "title": "Sharing Your Work",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/06_sharing.html#resources-for-further-exploration",
    "href": "schedule/06_sharing.html#resources-for-further-exploration",
    "title": "Sharing Your Work",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nGitcreds Package"
  },
  {
    "objectID": "schedule/04_tidydata.html",
    "href": "schedule/04_tidydata.html",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "",
    "text": "Our work this semester blends building capacity for data analysis and storytelling with basic data science skills. Throughout the course of the semester, we will frequently work to structure data in a tidy format - one in which we have one variable per column, and for which each row represents a unique observation. Some of you with a prior background or experience working in R will already be familiar with these principles, but some of you are not. This week, we’ll spend time reviewing the basics of what tidy data are, and will review common manipulation strategies."
  },
  {
    "objectID": "schedule/04_tidydata.html#session-description",
    "href": "schedule/04_tidydata.html#session-description",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "",
    "text": "Our work this semester blends building capacity for data analysis and storytelling with basic data science skills. Throughout the course of the semester, we will frequently work to structure data in a tidy format - one in which we have one variable per column, and for which each row represents a unique observation. Some of you with a prior background or experience working in R will already be familiar with these principles, but some of you are not. This week, we’ll spend time reviewing the basics of what tidy data are, and will review common manipulation strategies."
  },
  {
    "objectID": "schedule/04_tidydata.html#before-class",
    "href": "schedule/04_tidydata.html#before-class",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "Before Class",
    "text": "Before Class\nReview the How To Lesson 1 and Lesson 2. If you are comfortable with these principles and strategies, please take a look at Lesson 3"
  },
  {
    "objectID": "schedule/04_tidydata.html#reflect",
    "href": "schedule/04_tidydata.html#reflect",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/04_tidydata.html#slides",
    "href": "schedule/04_tidydata.html#slides",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/04_tidydata.html#resources-for-further-exploration",
    "href": "schedule/04_tidydata.html#resources-for-further-exploration",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/02_neighborhood.html",
    "href": "schedule/02_neighborhood.html",
    "title": "What is a Neighborhood?",
    "section": "",
    "text": "In this session, we’ll explore the many ways in which the concept of neighborhoods are used in various areas of urban planning and governance. We will continue to build upon our initial discussion regarding the significance of neighborhoods as a unit of analysis, planning, and policymaking, and will explore frameworks for neighborhood analysis."
  },
  {
    "objectID": "schedule/02_neighborhood.html#session-description",
    "href": "schedule/02_neighborhood.html#session-description",
    "title": "What is a Neighborhood?",
    "section": "",
    "text": "In this session, we’ll explore the many ways in which the concept of neighborhoods are used in various areas of urban planning and governance. We will continue to build upon our initial discussion regarding the significance of neighborhoods as a unit of analysis, planning, and policymaking, and will explore frameworks for neighborhood analysis."
  },
  {
    "objectID": "schedule/02_neighborhood.html#before-class",
    "href": "schedule/02_neighborhood.html#before-class",
    "title": "What is a Neighborhood?",
    "section": "Before Class",
    "text": "Before Class\nYour readings for today provide insight into some of the working definitions for value judgments regarding the qualities of neighborhoods and why planners have found these qualities to be of importance to measure and understand.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou must be logged in to your UIUC Box.com account in order to access these readings. This is to respect the copyright of the authors on a publicly accessible website.\n\n\nTalen, Emily, Sunny Menozzi, and Chloe Schaefer. (2015). What is a “Great Neighborhood”? An Analysis of APA’s Top-Rated Places. Journal of the American Planning Association, 81(2): 121-141. \nRohe, William. (2009). From Local to Global: One Hundred Years of Neighborhood Planning. Journal of the American Planning Association 75(2): 209-230."
  },
  {
    "objectID": "schedule/02_neighborhood.html#reflect",
    "href": "schedule/02_neighborhood.html#reflect",
    "title": "What is a Neighborhood?",
    "section": "Reflect",
    "text": "Reflect\n\nWhat matters about neighborhoods? How have neighborhoods shaped your life?\nWhat types of stories do we tend to tell about neighborhoods? How do these stories contextualize how neighborhoods “fit” within cities and their regions?"
  },
  {
    "objectID": "schedule/02_neighborhood.html#slides",
    "href": "schedule/02_neighborhood.html#slides",
    "title": "What is a Neighborhood?",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/02_neighborhood.html#resources-for-further-exploration",
    "href": "schedule/02_neighborhood.html#resources-for-further-exploration",
    "title": "What is a Neighborhood?",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nDahir, James. (1947). The Neighborhood Unit Plan: Its Spread and Acceptance. New York: The Russell Sage Foundation.\nMumford, Lewis. (1954). The Neighborhood and Neighborhood Unit. The Town Planning Review, 24(4): 256-270.\nSteuteville, Robert. (2019). The Once and Future Neighborhood. CNU Public Square."
  },
  {
    "objectID": "resources/textbook.html",
    "href": "resources/textbook.html",
    "title": "Text Resources",
    "section": "",
    "text": "D’Ignazio, Catherine, and Lauren F. Klein. (2020). Data Feminism. MIT Press.\n\nChapter 1 \nChapter 2 \nChapter 3 \nChapter 4 \nChapter 5 \nChapter 6 \nChapter 7"
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "This page contains resources that will help you over the course of the class. As you encounter more resources that you find useful, please share them so they can be added to this page."
  },
  {
    "objectID": "resources/analysis.html",
    "href": "resources/analysis.html",
    "title": "Neighborhood Analysis",
    "section": "",
    "text": "City of Champaign Neighborhood Wellness Vision Report and Action Plan\nChicago's Million Dollar Blocks\nDePaul Institute for Housing Studies: Mapping Displacement Pressure in Chicago, 2020\nNathalie P. Voorhees Center's Socioeconomic Change of Chicago's Community Areas (1970-2010)\nNational Neighborhood Indicators Partnership\nFind My Landlord, a tool that maps the properties in Chicago owned by a single landlord or management company.\nMetropolitan Planning Council and Urban Institute's The Cost of Segregationreport for Chicago.\nAmerican segregation, mapped at day and night"
  },
  {
    "objectID": "howto/rbasics_03.html",
    "href": "howto/rbasics_03.html",
    "title": "Lesson 3: Tidy Data",
    "section": "",
    "text": "In our previous two lessons, we’ve been working with Base R to do basic manipulation of data. These strategies are powerful and can do a lot, however, they are a bit clunky (something you may have been thinking to yourself) - there are tools and strategies that are tailored to the types of data forms and structures we tend to use to measure characteristics and dynamics of neighborhoods.\nIn this lesson, we’ll introduce principles of tidy data as well as a frequently used R package designed to help us manipulate and work more efficiently."
  },
  {
    "objectID": "howto/rbasics_03.html#lesson-overview",
    "href": "howto/rbasics_03.html#lesson-overview",
    "title": "Lesson 3: Tidy Data",
    "section": "",
    "text": "In our previous two lessons, we’ve been working with Base R to do basic manipulation of data. These strategies are powerful and can do a lot, however, they are a bit clunky (something you may have been thinking to yourself) - there are tools and strategies that are tailored to the types of data forms and structures we tend to use to measure characteristics and dynamics of neighborhoods.\nIn this lesson, we’ll introduce principles of tidy data as well as a frequently used R package designed to help us manipulate and work more efficiently."
  },
  {
    "objectID": "howto/rbasics_03.html#lesson-goals",
    "href": "howto/rbasics_03.html#lesson-goals",
    "title": "Lesson 3: Tidy Data",
    "section": "Lesson Goals",
    "text": "Lesson Goals\nBy the end of this lesson, you should be familiar with:\n\nPrinciples of tidy data\nHigh-level tools for selecting and subsetting data using dplyr syntax\nMore advanced strategies for grouping and summarizing data using dplyr syntax"
  },
  {
    "objectID": "howto/rbasics_03.html#getting-set-up",
    "href": "howto/rbasics_03.html#getting-set-up",
    "title": "Lesson 3: Tidy Data",
    "section": "Getting Set Up",
    "text": "Getting Set Up\n\nLoading Required Packages\nWe’re been working primarily in “base” R as we are getting familiar with the R language and RStudio interface. In Lesson 2, we introduced packages and made use of the readxl package to load data from an Excel file into R.\nTo review, we used install.packages() and library() to (respectively) install and load packages that extend R and RStudio’s functionality. If you remember from our last lesson, you will only need to install a package once, but you will need to load it every time you start your R session and want to use it.\nLet’s start by loading the following packages:\n\nreadxl contains tools which will help us to read Excel files into R\ntidyverse contains tools which we’ll use to subset, filter, group, and summarize our data\n\nIf you completed the last lesson, you will already have installed readxl. Let’s install the tidyverse package and then load both the readxl and tidyverse packages for use:\n\nYour TurnSolution\n\n\nTry installing the tidyverse package (if it is not already installed on your machine), and then load readxl and tidyverse for use in your R session.\n\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(readxl)\n\n\n\n\n\n\nLoading Data\nIf it’s not already loaded, load the OZ dataset we worked with in Lesson 2. You can assign the data whatever name you’d like, but we will stick with the name we used previously, ‘ozs’. A copy of the Urban Institute’s dataset is available here for download.\n\n\nYou’ll need to authenticate and log in to UIUC Box to access this file. You can also download the data directly from Urban Institute’s Opportunity Zone landing page.\n\nozs &lt;- read_excel(\"data/urbaninstitute_tractlevelozanalysis_update01142021.xlsx\")"
  },
  {
    "objectID": "howto/rbasics_03.html#an-easier-and-more-efficient-way",
    "href": "howto/rbasics_03.html#an-easier-and-more-efficient-way",
    "title": "Lesson 3: Tidy Data",
    "section": "An easier and more efficient way?",
    "text": "An easier and more efficient way?\nWe could keep building queries in base R to describe or summarize other variables in our data. Looking at the code you’ve created in Lesson 2 in particular, you’re probably thinking that it looks fairly illegible. Part of the challenge of code like this is that you have to read from the inside out.\nLet’s learn a whole different way of constructing this same thing.\n\nPrinciples of Tidy Data\nThis lesson focuses on introducing the tidyverse, a series of packages designed specifically to make data science easier in R and RStudio. The functionality of the tidyverse is largely described in the accompanying book R for Data Science.\nData are structured for tidy analysis when columns each contain one individual variable, each row represents a unique observation, and there is only one value for each variable and observation:\n\nThe majority of the data which we will encounter in this class, and the majority of data we work with as planners already conforms to these principles.\nIn the case of the Opportunity Zone data we first looked at in Lesson 2, here’s what that looked like:\n\nEach column represented a different variable, for instance, whether an observation was designated an Opportunity Zone, the poverty rate, or the median household income.\nEach row represented a unique observation, in this case a unique census tract.\nEach value was unique and there was only one value for every variable and observation.\n\n\n\nIf you want to understand some of the rationale behind tidy data, Hadley Wickham’s article is a good resource."
  },
  {
    "objectID": "howto/rbasics_03.html#your-first-tidy-coding",
    "href": "howto/rbasics_03.html#your-first-tidy-coding",
    "title": "Lesson 3: Tidy Data",
    "section": "Your First Tidy Coding",
    "text": "Your First Tidy Coding\nAt this point, you should have your data loaded and available and you should also have the tidyverse and readxl packages loaded.\nIn Lesson 2, you worked on solving the following two data manipulation and description problems:\n\nReport average poverty rates for designated opportunity zones in metropolitan, micropolitan, and non-CBSA areas.\nFor Illinois, how different are the average vacancy rates for designated and undesignated census tracts?\n\nLet’s compare how to do that using base R and using commands from the tidyverse suite.\n\nPoverty Rates\n\nThe ProblemBase RTidy\n\n\nReport average poverty rates for designated opportunity zones in metropolitan, micropolitan, and non-CBSA areas.\n\n\n\nmean(ozs$PovertyRate[ozs$DesignatedOZ == 1 & ozs$Metro == 1], na.rm=TRUE)\n\n[1] 0.3335197\n\nmean(ozs$PovertyRate[ozs$DesignatedOZ == 1 & ozs$Micro == 1], na.rm=TRUE)\n\n[1] 0.2803457\n\nmean(ozs$PovertyRate[ozs$DesignatedOZ == 1 & ozs$NoCBSAType == 1], na.rm=TRUE)\n\n[1] 0.2357986\n\n\n\n\n\nozs |&gt; \n  filter(DesignatedOZ ==1, Metro == 1) |&gt;\n  summarise(mean(PovertyRate, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(PovertyRate, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             0.334\n\nozs |&gt; \n  filter(DesignatedOZ ==1, Micro == 1) |&gt;\n  summarise(mean(PovertyRate, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(PovertyRate, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             0.280\n\nozs |&gt; \n  filter(DesignatedOZ ==1, NoCBSAType == 1) |&gt;\n  summarise(mean(PovertyRate, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(PovertyRate, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             0.236\n\n\n\n\n\nWe get the same values out, but note the code we input in order to get these outputs is very different!\nLet’s break this down further.\nIn Base R…\n\nWe first specified the statistic we wanted mean().\nWe then specified the dataset and columns we wanted that mean for ozs$PovertyRate.\nWe then specified we only wanted a subset of the poverty rate variable where observations were designated opportunity zones and then based upon a metropolitan criterion. [ozs$DesignatedOZ == 1 & ozs$Metro == 1]\nWe also specified that we wanted to remove NA values from our calculation of the average mean(na.rm=TRUE).\n\nAs a reminder, when put together, these things looked like this:\n\nozs |&gt; \n  filter(DesignatedOZ ==1, Metro == 1) |&gt;\n  summarise(mean(PovertyRate, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(PovertyRate, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             0.334\n\n\nNext, let’s look at the structure of the tidy command to do the same thing:\n\n1ozs |&gt;\n2  filter(DesignatedOZ ==1, Metro == 1) |&gt;\n3  summarise(mean(PovertyRate, na.rm=TRUE))\n\n\n1\n\nFrom the ‘ozs’ dataset;\n\n2\n\nFilter (select rows from) the dataset where the DesignatedOZ column is equal to 1 (designated) AND the Metropolitan area flag is equal to 1 (a metropolitan area);\n\n3\n\nFor the filtered data from ‘ozs’, summarize (report back) the mean value for the PovertyRate column, removing NA values.\n\n\n\n\n# A tibble: 1 × 1\n  `mean(PovertyRate, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             0.334\n\n\nThis is still complex, but we gain a major benefit - where in our Base R strategy the code is nested and hard to read, the Tidy syntax offers a more logical workflow. We used something called a pipe |&gt; to pass results of previous commands along a data analysis pipeline. This allows us to code steps in a logical order and makes it much easier to read and interpret what we’re doing step-by-step.\n\n\nVacancy Rates\nLet’s now compare code for our second challenge - examining vacancy rates.\n\nThe ProblemBase RTidy\n\n\nFor Illinois, how different are the average vacancy rates for designated and undesignated census tracts?\n\n\n\nozs$DesignatedOZ[is.na(ozs$DesignatedOZ)]&lt;-0\n\nmean(ozs$vacancyrate[ozs$DesignatedOZ == 1], na.rm=TRUE)\n\n[1] 0.1583661\n\nmean(ozs$vacancyrate[ozs$DesignatedOZ == 0], na.rm=TRUE)\n\n[1] 0.1367463\n\n\n\n\n\n111ozs |&gt;\n222  replace_na(list(DesignatedOZ = 0)) |&gt;\n333  group_by(DesignatedOZ) |&gt;\n4  summarise(mean(vacancyrate, na.rm=TRUE))\n\n\n1\n\nFrom the ‘ozs’ dataset,\n\n2\n\nReplace any values that at NA in the DesignatedOZ column with the value 0,\n\n3\n\nTreat our data as being grouped by the unique values of DesignatedOZ,\n\n4\n\nSummarize for us the mean value for vacancy rate, removing any NA values from our calculation.\n\n\n\n\n# A tibble: 2 × 2\n  DesignatedOZ `mean(vacancyrate, na.rm = TRUE)`\n         &lt;dbl&gt;                             &lt;dbl&gt;\n1            0                             0.137\n2            1                             0.158\n\n\n\n\n\nLots going on, but let’s pay attention to some cool things we just saw.\n\nAs we had with the poverty rate we started with our ‘ozs’ dataset and then sequentially modified the dataset to get to our final output - a summary output with values for the average vacancy rate for designated and eligible but not designated tracts.\nWe were able to substitute NA values with 0 using a special command in line with our data modification workflow.\nWe used something we haven’t seen before - group_by() to tell R to treat our data as grouped by the values of the DesignatedOZ variable.\nWe used summarise() to create an output table containing the average values for the vacancy rate grouped by the values in DesignatedOZ.\n\nThis quick illustration helps you understand some of the basics of how dplyr works. Two major improvements, in addition to specific commands for filtering rows and selecting columns are the use of pipes |&gt; and the ability to summarize data. You’ll also notice that the output is rendered in a minimally formatted table."
  },
  {
    "objectID": "howto/rbasics_03.html#basic-dplyr-verbs",
    "href": "howto/rbasics_03.html#basic-dplyr-verbs",
    "title": "Lesson 3: Tidy Data",
    "section": "Basic dplyr verbs",
    "text": "Basic dplyr verbs\n\nFiltering Data\nWe can use dplyr to filter out rows that meet certain criteria.\nFor instance, here’s how we’re filter out all records for tracts in Illinois:\n\n1ozs |&gt;\n2  filter(state == \"Illinois\")\n\n\n1\n\nFrom the ozs data object;\n\n2\n\nFilter out those rows in the column “state” for which state is equal to “Illinois”\n\n\n\n\n# A tibble: 1,659 × 27\n   geoid       state    DesignatedOZ county   Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 17001000201 Illinois            0 Adams C… Non-…         7      NA       1937\n 2 17001000202 Illinois            0 Adams C… Low-…         1      NA       2563\n 3 17001000400 Illinois            0 Adams C… Low-…         1      NA       3403\n 4 17001000500 Illinois            0 Adams C… Low-…         1      NA       2298\n 5 17001000700 Illinois            0 Adams C… Low-…         1      NA       1259\n 6 17001000800 Illinois            1 Adams C… Low-…         1      NA       2700\n 7 17001000900 Illinois            0 Adams C… Low-…         5      NA       2671\n 8 17001010100 Illinois            0 Adams C… Non-…         2      NA       4323\n 9 17001010200 Illinois            0 Adams C… Low-…         2      NA       3436\n10 17001010300 Illinois            0 Adams C… Non-…         8      NA       6038\n# ℹ 1,649 more rows\n# ℹ 19 more variables: medhhincome &lt;dbl&gt;, PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;,\n#   medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;,\n#   vacancyrate &lt;dbl&gt;, pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;,\n#   pctAAPIalone &lt;dbl&gt;, pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;,\n#   BAorhigher &lt;dbl&gt;, Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\n\n\nSelecting Columns\nSimilar to filter, we can use select() to select specific columns in our data frame:\n\n1ozs |&gt;\n2  select(state, DesignatedOZ)\n\n\n1\n\nFrom the ozs dataset;\n\n2\n\nSelect the columns named “state” and “DesignatedOZ”.\n\n\n\n\n# A tibble: 42,178 × 2\n   state   DesignatedOZ\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 Alabama            0\n 2 Alabama            0\n 3 Alabama            1\n 4 Alabama            0\n 5 Alabama            0\n 6 Alabama            0\n 7 Alabama            0\n 8 Alabama            1\n 9 Alabama            0\n10 Alabama            1\n# ℹ 42,168 more rows\n\n\n\n\nCombining filter() and select()\nYour turn - create a table containing the variables state, Designated, and Metro, for Illinois:\n\nYour TurnSolution\n\n\nFor Illinois, create a table containing the variables state, Designated, and Metro.\n\n\n\nozs |&gt;\n  select(state, DesignatedOZ, Metro) |&gt;\n  filter(state == \"Illinois\")\n\n\n1\n\nFrom the ‘ozs’ dataset;\n\n2\n\nSelect the columns “state”, “Designated OZ”, and “Metro”;\n\n3\n\nFrom the state column, select the subset of values where state is equal to “Illinois”.\n\n\n\n\n# A tibble: 1,659 × 3\n   state    DesignatedOZ Metro\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1 Illinois            0    NA\n 2 Illinois            0    NA\n 3 Illinois            0    NA\n 4 Illinois            0    NA\n 5 Illinois            0    NA\n 6 Illinois            1    NA\n 7 Illinois            0    NA\n 8 Illinois            0    NA\n 9 Illinois            0    NA\n10 Illinois            0    NA\n# ℹ 1,649 more rows\n\n\n\n\n\nYou should return a data frame with three columns and 1,659 rows.\n\nYour TurnSolution\n\n\nHow would you modify your code to limit this to tracts that were Metropolitan (Metro equal to 1)?\n\n\n\nozs |&gt;\n  select(state, DesignatedOZ, Metro) |&gt;\n  filter(state == \"Illinois\", Metro == 1)\n\n\n1\n\nFrom the ‘ozs’ dataset;\n\n2\n\nSelect the columns “state”, “Designated OZ”, and “Metro”;\n\n3\n\nFrom the state column, select the subset of values where state is equal to “Illinois” AND where the Metro column is equal to 1.\n\n\n\n\n# A tibble: 1,344 × 3\n   state    DesignatedOZ Metro\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1 Illinois            0     1\n 2 Illinois            0     1\n 3 Illinois            0     1\n 4 Illinois            1     1\n 5 Illinois            1     1\n 6 Illinois            0     1\n 7 Illinois            1     1\n 8 Illinois            0     1\n 9 Illinois            0     1\n10 Illinois            0     1\n# ℹ 1,334 more rows\n\n\n\n\n\nIf you do this successfully, you should end up with 1,344 observations.\n\nozs |&gt;\n  select(state, DesignatedOZ, Metro) |&gt;\n  filter(state == \"Illinois\", Metro == 1) |&gt; \n  nrow()\n\n[1] 1344\n\n\n\n\nGroup By and Summarise\nIn the vacancy rate illustration that we saw above, we were able to group our data by a particular categorical variable and then summarize based upon another variable, in that case then average vacancy rate.\nLet’s see what that looks like again, this time, finding the average median household income for designated and not designated but eligible opportunity zone tracts:\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt;  summarise(mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 2 × 2\n  DesignatedOZ `mean(medhhincome, na.rm = TRUE)`\n         &lt;dbl&gt;                             &lt;dbl&gt;\n1            0                            44446.\n2            1                            33346.\n\n\nA little tip here - we can easily change the name of the column label for our summarized values as follows:\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt;  summarise(income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 2 × 2\n  DesignatedOZ income\n         &lt;dbl&gt;  &lt;dbl&gt;\n1            0 44446.\n2            1 33346.\n\n\nWithin our summarise() code, we can create multiple columns with each separated by a comma.\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt;  summarise(\n    tracts = n(),\n    income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 2 × 3\n  DesignatedOZ tracts income\n         &lt;dbl&gt;  &lt;int&gt;  &lt;dbl&gt;\n1            0  33414 44446.\n2            1   8764 33346.\n\n\nn() returns the count of the number of records within each group.\n\nYour TurnSolution\n\n\nYour turn - add to our above summary table the average poverty rate (PovertyRate) and the average proportion of the population facing severe rent burden (severerentburden). You can name them whatever you want\n\n\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt;  summarise(\n    tracts = n(),\n    income = mean(medhhincome, na.rm=TRUE),\n    poverty = mean(PovertyRate, na.rm=TRUE),\n    rent_burden = mean(severerentburden, na.rm=TRUE))\n\n# A tibble: 2 × 5\n  DesignatedOZ tracts income poverty rent_burden\n         &lt;dbl&gt;  &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1            0  33414 44446.   0.211       0.243\n2            1   8764 33346.   0.317       0.265\n\n\n\n\n\nIt looks like designated opportunity zones have lower incomes, higher poverty rates, and higher levels of severe rent burden.\nThis is a big step up from what we were doing earlier. We know how different designated and undesignated tracts are throughout the US, but how different are they for each state in the US?\nHow would we go about modifying our code to create this grouping?\n\nYour TurnSolution\n\n\nModify your above code to group your data by state and designation status in order to be able to examine state-to-state differences.\n\n\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt;  summarise(\n    tracts = n(),\n    income = mean(medhhincome, na.rm=TRUE),\n    poverty = mean(PovertyRate, na.rm=TRUE),\n    rent_burden = mean(severerentburden, na.rm=TRUE))\n\n`summarise()` has grouped output by 'state'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 108 × 6\n# Groups:   state [57]\n   state          DesignatedOZ tracts income poverty rent_burden\n   &lt;chr&gt;                 &lt;dbl&gt;  &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 Alabama                   0    677 36542.   0.239       0.212\n 2 Alabama                   1    158 30044.   0.328       0.246\n 3 Alaska                    0     43 54784.   0.149       0.180\n 4 Alaska                    1     25 49840.   0.167       0.178\n 5 American Samoa            1     16   NaN  NaN         NaN    \n 6 Arizona                   0    702 40961.   0.246       0.236\n 7 Arizona                   1    168 34373.   0.315       0.237\n 8 Arkansas                  0    435 37814.   0.221       0.192\n 9 Arkansas                  1     85 31254.   0.301       0.228\n10 California                0   3464 50858.   0.207       0.298\n# ℹ 98 more rows\n\n\n\n\n\nIf you modified this correctly, you should now have an output table with 108 rows, each reflecting summaries for a state and unique OZ designation status.\nThere are other fairly interesting things that we can do with our grouping and summarizing. We figured out how to use multiple groups to summarize our data in useful ways. What we probably want is to get that all into the same table.\nOne strategy for doing this is to include conditions in our summary statements. The code below summarizes the average median income by state, but then includes conditions on summarizing means income. This allows us to get the incomes of designated and undesignated tracts on the same row.\n\nozs |&gt;  \n  group_by(state) |&gt; \n  summarise(\n    tracts = n(), \n    income = mean(medhhincome, na.rm=TRUE), \n    Des_inc = mean(medhhincome[DesignatedOZ == 1], na.rm=TRUE), \n    Not_Des_Inc = mean(medhhincome[DesignatedOZ == 0], na.rm=TRUE))\n\n# A tibble: 57 × 5\n   state                tracts income Des_inc Not_Des_Inc\n   &lt;chr&gt;                 &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 Alabama                 835 35311.  30044.      36542.\n 2 Alaska                   68 52911.  49840.      54784.\n 3 American Samoa           16   NaN     NaN         NaN \n 4 Arizona                 870 39692.  34373.      40961.\n 5 Arkansas                520 36740.  31254.      37814.\n 6 California             4343 47878.  36134.      50858.\n 7 Colorado                657 47976.  41138.      49601.\n 8 Connecticut             344 48318.  36760.      51389.\n 9 Delaware                118 48200.  40971.      50143.\n10 District of Columbia    116 57672.  38291.      62840.\n# ℹ 47 more rows\n\n\n\nYour TurnSolution\n\n\nHow would you modify the above code to produce the same table for counties in Illinois?\n\n\n\nozs |&gt;  \n  filter(state == \"Illinois\") |&gt; \n  group_by(county) |&gt; \n  summarise(\n    tracts = n(), \n    income = mean(medhhincome, na.rm=TRUE), \n    Des_inc = mean(medhhincome[DesignatedOZ == 1], na.rm=TRUE), \n    Not_Des_Inc = mean(medhhincome[DesignatedOZ == 0], na.rm=TRUE))\n\n# A tibble: 95 × 5\n   county           tracts income Des_inc Not_Des_Inc\n   &lt;chr&gt;             &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adams County         10 38254   26012       39614.\n 2 Alexander County      4 29982.  21500       32809 \n 3 Bond County           2 50950   49590       52310 \n 4 Boone County          3 44028.  40599       45742 \n 5 Bureau County         4 57275.  48083       60339.\n 6 Calhoun County        2 55290     NaN       55290 \n 7 Carroll County        2 47063   35184       58942 \n 8 Cass County           3 43787.  37679       46840.\n 9 Champaign County     30 39063.  13989.      45604.\n10 Christian County      8 44723.  36164       45945.\n# ℹ 85 more rows\n\n\n\n\n\n\n\nMutate\nWe’re getting pretty good at passing data along using pipes (|&gt;). We’ve learned how to use group_by() and summarise() to quickly create summary tables. What if we wanted to modify these tables? One thing that might help us better understand our summary table would be to calculate the difference in the average median income for our designated and not designated tracts.\nmutate() allows us to add new columns to our existing data (this will work on non-summarized data too). The code below adds a column called “Inc_Diff” to our summary table, and places into this column the difference between the income in designated and not designated census tracts:\n\nozs |&gt;  \n  filter(state == \"Illinois\") |&gt;  \n  group_by(county) |&gt;  \n  summarise(\n    tracts = n(), \n    income = mean(medhhincome, na.rm=TRUE), \n    Des_inc = mean(medhhincome[DesignatedOZ == 1], na.rm=TRUE), \n    Not_Des_Inc = mean(medhhincome[DesignatedOZ == 0], na.rm=TRUE)) |&gt; \n  mutate(Inc_Diff = Des_inc - Not_Des_Inc)\n\n# A tibble: 95 × 6\n   county           tracts income Des_inc Not_Des_Inc Inc_Diff\n   &lt;chr&gt;             &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 Adams County         10 38254   26012       39614.  -13602.\n 2 Alexander County      4 29982.  21500       32809   -11309 \n 3 Bond County           2 50950   49590       52310    -2720 \n 4 Boone County          3 44028.  40599       45742    -5143 \n 5 Bureau County         4 57275.  48083       60339.  -12256.\n 6 Calhoun County        2 55290     NaN       55290      NaN \n 7 Carroll County        2 47063   35184       58942   -23758 \n 8 Cass County           3 43787.  37679       46840.   -9162.\n 9 Champaign County     30 39063.  13989.      45604.  -31615.\n10 Christian County      8 44723.  36164       45945.   -9781.\n# ℹ 85 more rows\n\n\nNotice that we needed to add another pipe here so that we were mutating our summary table and not our original data. Notice that most designated tracts have much lower median household incomes when compared to eligible but not designated places - that would suggest that the program is targeting neighborhoods with greater need."
  },
  {
    "objectID": "howto/rbasics_03.html#time-for-practice",
    "href": "howto/rbasics_03.html#time-for-practice",
    "title": "Lesson 3: Tidy Data",
    "section": "Time for Practice!",
    "text": "Time for Practice!\nLet’s spend a little time practicing filtering, grouping, and summarizing data using dplyr commands.\n\nYour TurnSolution\n\n\nCreate a summary table of the racial characteristics of designated and not designated tracts at the nation level.\nRacial characteristics are pctwhitw, pctBlack, pctHispanic, pctAAPIalone.\n\n\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt; \n  summarise(\n    White = mean(pctwhite, na.rm=TRUE), \n    Black = mean(pctBlack, na.rm=TRUE), \n    Hispanic = mean(pctHispanic, na.rm=TRUE), \n    AAPI = mean(pctAAPIalone, na.rm=TRUE))\n\n# A tibble: 2 × 5\n  DesignatedOZ White Black Hispanic   AAPI\n         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1            0 0.554 0.172    0.200 0.0404\n2            1 0.396 0.240    0.299 0.0292\n\n\n\n\n\n\nYour TurnSolution\n\n\nLooking at the state level (by each state), how different are the poverty rates of designated opportunity zones in metropolitan, micropolitan, and non-CBSA areas?\n\n\n\nozs |&gt; \n  filter(DesignatedOZ == 1) |&gt; \n  group_by(state) |&gt;  \n  summarise(\n    Metro = mean(PovertyRate[Metro == 1], na.rm=TRUE),\n    Micro = mean(PovertyRate[Micro == 1], na.rm=TRUE),\n    Non_CBSA = mean(PovertyRate[NoCBSAType == 1], na.rm=TRUE))\n\n# A tibble: 56 × 4\n   state                  Metro   Micro Non_CBSA\n   &lt;chr&gt;                  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 Alabama                0.347   0.282    0.275\n 2 Alaska                 0.156 NaN        0.178\n 3 American Samoa       NaN     NaN      NaN    \n 4 Arizona                0.319   0.311    0.239\n 5 Arkansas               0.334   0.287    0.262\n 6 California             0.332   0.311    0.209\n 7 Colorado               0.245   0.169    0.201\n 8 Connecticut            0.284   0.319  NaN    \n 9 Delaware               0.262 NaN      NaN    \n10 District of Columbia   0.322 NaN      NaN    \n# ℹ 46 more rows\n\n\n\n\n\n\nYour TurnSolution\n\n\nLooking at the state level (by state), what’s the average age dependence ratio for designated and non-designated tracts?\nTip: The age dependence ratio is the proportion of the population under 18 or over 64 compared to the population between 18 and 64. In our dataset, we have the proportion under 18 (pctunder18) and the proportion over 64 (pctover64)\n\n\n\nozs |&gt;  \n  select(state, DesignatedOZ, pctunder18, pctover64) |&gt;  \n  mutate(\n    adr = (pctunder18+pctover64)/(1-(pctunder18+pctover64))) |&gt; \n  group_by(state) |&gt;   \n  summarise(Designated_ADR = mean(adr[DesignatedOZ == 1], na.rm=TRUE),\n          NotDesignated_ADR = mean(adr[DesignatedOZ == 0], na.rm=TRUE))\n\n# A tibble: 57 × 3\n   state                Designated_ADR NotDesignated_ADR\n   &lt;chr&gt;                         &lt;dbl&gt;             &lt;dbl&gt;\n 1 Alabama                       0.592             0.644\n 2 Alaska                        0.540             0.556\n 3 American Samoa              NaN               NaN    \n 4 Arizona                       0.651             0.771\n 5 Arkansas                      0.643             0.681\n 6 California                    0.606             0.590\n 7 Colorado                      0.590             0.572\n 8 Connecticut                   0.543             0.560\n 9 Delaware                      0.601             0.614\n10 District of Columbia          0.535             0.486\n# ℹ 47 more rows\n\n\n\n\n\n\nYour Turn\n\n\nLooking the state of Illinois, whats the average poverty and income for tracts based upon their level of investment flows (the dec_score variable)? #### Solution\n\nozs |&gt;  \n  filter(state == \"Illinois\") |&gt; \n  group_by(dec_score) |&gt;   \n  summarise(Count = n(),\n            Poverty = mean(PovertyRate, na.rm=TRUE),\n            Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 11 × 4\n   dec_score Count Poverty Income\n       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1         1   165   0.199 41642.\n 2         2   165   0.227 40782.\n 3         3   165   0.278 36666.\n 4         4   164   0.277 37705.\n 5         5   165   0.252 39391.\n 6         6   165   0.224 43224.\n 7         7   164   0.233 42253.\n 8         8   165   0.230 43776.\n 9         9   165   0.215 47064.\n10        10   164   0.209 51795.\n11        NA    12   0.459 32213.\n\n\n\n\n\nCongratulations! You are well on your way to being able to do some very powerful things in R! Take a moment to relish in your accomplishment! ## Lesson 2 Summary and Debrief\nIn this lesson, you …"
  },
  {
    "objectID": "howto/rbasics_03.html#core-concepts-and-terminology",
    "href": "howto/rbasics_03.html#core-concepts-and-terminology",
    "title": "Lesson 3: Tidy Data",
    "section": "Core Concepts and Terminology",
    "text": "Core Concepts and Terminology\n\nR Script\nNotebook\nCode Chunk\nVariables\nLists\nVectors\nData Frame"
  },
  {
    "objectID": "howto/rbasics_01.html#getting-familiar",
    "href": "howto/rbasics_01.html#getting-familiar",
    "title": "Lesson 1: Basic Principles",
    "section": "Getting Familiar",
    "text": "Getting Familiar\nLet’s start off by simply getting familiar with the R console. As you learned in the introduction, R is command line software focused on statistical computing, and RStudio is a user interface which enhances your ability to interact with R.\nFor most new useRs, R may be intimidating because of the need to learn a basic language to interact with it. When most of our interaction with computers happens through “point and click” graphical interfaces, language-driven approaches seem less intuitive.\nAt the same time, there are some really good reasons to gain familiarity with R and RStudio, particularly within the context of urban planning analysis.\n\nR and RStudio are open source tools, and are therefore available to be downloaded and used without cost (this does not negate questions regarding accessibility of the language, given its steep learning curve).\nR and RStudio are supported by a wide range of users who develop packages for specific use cases, including those that are useful for urban planning analysis.\nR and RStudio provide a framework for reproducible data manipulation and analysis - rather than sharing output with others, we can share raw data and code and they can reproduce our output.\n\nThinking about the type of analysis we will do in this class, there are some additional rationales for learning and working in R and RStudio:\n\nR has a powerful set of functions for aggregating and manipulating many data records into a smaller number of summary records - we use these types of functions frequently to summarize neighborhood characteristics\nR can natively read from and write to many types of data sources - this allows us to perform most or all of our analysis within a single application rather than passing data between applications for different types of manipulation or analysis\nR can help us to automate elements of data visualization, which can be useful when we need to reproduce forms of analysis for different places or other data categories\nLooking beyond the reasons to use R and RStudio as a platform for analysis, these tools represent one of several programming languages frequently used for data science (the other main language being Python) - learning these languages prepares you for future interface with other data science tools and strategies"
  },
  {
    "objectID": "howto/rbasics_01.html#lesson-goals",
    "href": "howto/rbasics_01.html#lesson-goals",
    "title": "Lesson 1: Basic Principles",
    "section": "Lesson Goals",
    "text": "Lesson Goals\nBy the end of this lesson, you should be familiar with:\n\nThe RStudio console and R language\nR data types and structures\nBasic data manipulation and querying"
  },
  {
    "objectID": "howto/rbasics_01.html#r-basics",
    "href": "howto/rbasics_01.html#r-basics",
    "title": "Lesson 1: Basic Principles",
    "section": "R Basics",
    "text": "R Basics\nAs a programming language, R was initially designed to be run in a terminal console. You can still run R in this fashion, if you wish. RStudio is an integrated development environment (IDE) for R - in addition to providing us with a terminal window in which we could run commands, it also provides additional windows for viewing data and visualizations.\nIt is important that we keep these things in mind, because they will help us to understand what its like to interact with R via RStudio. Let’s start with a very basic way of interacting with R. Here is what the basic RStudio interface looks like:\n\nThe Source pane is where you will write and view scripts that provide instructions to the console.\nThe Console / Terminal pane is the where you can view the execution of commands. You can directly type commands here and then click control / command + enter to run. More typically, you’ll execute lines of code from within your scripts or the source pane.\nTHe Environment pane allows you to view datasets loaded into memory and other objects which you have defined. You can also see a few basic characteristics of existing R objects.\nThe Auxiliary pane which has multiple tabs. You will most often use this to view output, or load help documentation."
  },
  {
    "objectID": "howto/rbasics_01.html#r-scripts-vs-notebooks",
    "href": "howto/rbasics_01.html#r-scripts-vs-notebooks",
    "title": "Lesson 1: Basic Principles",
    "section": "R Scripts vs Notebooks",
    "text": "R Scripts vs Notebooks\nThere’s two main strategies for creating reproducible commands (code) to tell R what to do. Scripts are text files where the majority of language used will be R commands. Everything in a script is interpreted as code unless you explicitly denote that it is a comment. Notebooks are powerful tools in that they allow you to blend together text that’s meant to be read by humans with code chunks that are meant to be read and interpreted by R (or another computing language). Notebooks are particularly powerful because they allow you to format and publish your text while embedding chunks of analysis at strategically appropriate places.\nThe benefit of working in a notebook is that you can run code in line with your text, and see the results integrated with your writing kind of like a scientific lab notebook. Some people will use R Notebooks to write reports, since they can render tables and figures in line with their text, and these can easily be updated if new data or parameters are supplied.\nCode Chunks\nThis is what a code chunk looks like:\n\n# Code Chunk\n\nAny content inside of this code chunk will be interpreted by your R session in the terminal when you hit the green play button to the right. You can also step through each line of code by putting your cursor to the right of it and hitting command+enter (Mac) or control+enter (PC) - I strongly recommend you get into the habit of running code this way at first.\nYou can create new code chunks by pressing control+option+I (Mac) or control+alt+I (PC).\nComments\nIn the above code chunk, we have some text preceded by a hashtag (#). Any content to the right of a hashtag (# groundbreaking insightful comment) will be considered a comment and will not be interpreted as code. Comments are a great way to make short notes to remind yourself or others of what you’re doing:\n\n# This is a comment\n\n1+2 # This is a comment in line with some active code\n\n[1] 3"
  },
  {
    "objectID": "howto/rbasics_01.html#your-first-commands",
    "href": "howto/rbasics_01.html#your-first-commands",
    "title": "Lesson 1: Basic Principles",
    "section": "Your First Commands",
    "text": "Your First Commands\nLet’s start by entering a simple command - let’s add together 2 and 2 in the console and ask R to return the product.\n\n2+2\n\n[1] 4\n\n\nEntering 2+2 into our console window and then hitting command/control+enter asks R to process the request we have given it - it then gives us back an answer to our request. We could of course do the same thing with other simple numeric operators:\n\n\n+ Addition\n\n- Subtraction\n\n* Multiplication\n\n/ Division\n\n^ Exponents (e.g. 2^3 = 8)\n\n() Parentheses - to control order of operations (e.g. ```(2+3)/5 = 1)\n\nWe can do basic math in a console - not terribly exciting, but at least this helps you to see how R will respond to basic commands:\n\n2+2\n\n[1] 4\n\n2^3\n\n[1] 8\n\n(2+3)/5\n\n[1] 1\n\n\nNow your turn - create a script (File -&gt; New File -&gt; R Script) and perform some simple math operations. Also explore how R handles order of operations.\nGet at it!\nVariables\nIn most cases, we don’t want to just type things into the console and then get an answer - we’d be just as well served with a calculator. Our next step is to understand that R can store the output of a command for later use. The most basic way to do this is to assign our output to an object. we can do this using the &lt;- assignment operator:\n\nx &lt;- 2+2\n\nLet’s learn how to speak this out. We just told R, into an object we have (arbitrarily) named “x”, store the output of 2 + 2. Because this is now stored, we can retrieve it and use it later. If we simply ask for “x” R will share with us the previously assigned output - 2+2\nOption + - (Mac) or Alt + - (PC) is the shortcut for inserting the assignment operator.\n\nx\n\n[1] 4\n\n\nThis means that we could also use this output in other formulas. Let’s see what happens if we square X:\n\nx^2\n\n[1] 16\n\n\nSince X is 4, we get the output that is the equivalent of typing 4^2.\nIt is important to note here that we can provide any type of label we’d like for an R variable. Instead of using “x” as a variable name, we could use anything else.\nAssign the sum of 4+6 into a variable named “cat”.\n\ncat&lt;-4+6\n\nWe just assigned to a variable called “cat” the product of 4+6. To retrieve the value of your assigned variable, you can just call it by name:\n\ncat\n\n[1] 10\n\n\nR allows you to name variables as you wish. Note that variables need to start with a character, cannot start with a number (e.g. 1_Numbers would not work), and cannot include spaces (e.g. “variables squared” would not work but you could use an underscore - “variables_squared” which would work). Also note that you will want to avoid variable names that are the same as R functions (so naming a variable “mean” for instance, would not be a good idea, as this would cause confusion with the function mean() which calculates the average of a vector).\nWe can of course work with multiple variables at once:\n\ncat+x\n\n[1] 14\n\n\nIn this case, cat is 10 and x is 4 (you can see the values stored in objects in the environment pane). Let’s divide cat+x by x:\n\n(cat+x)/x\n\n[1] 3.5\n\n\nThis is great (i guess…) - we have a calculator that can store and make use of values as objects. Not so exciting for neighborhood analysis just yet, though…\nLists\nThe next thing to note is that objects don’t have to be single values. We could also assign lists of values to an object:\n\ncol1 &lt;- c(2, 3, 4, 5, 6)\n\nNote here that c() (formally the concatenate function) is used to denote that we have a list. Each list item it separated by commas. If we call up this object, we can have a look at our list:\n\ncol1\n\n[1] 2 3 4 5 6\n\n\nWorking with a single object, we could do things by using the object in a formula. We can do the same with a list:\n\ncol1+2\n\n[1] 4 5 6 7 8\n\n\nTo each list item, we added 2. We could even store this as a new object if we wanted 2\n\ncol2 &lt;- col1+2\n\nWriting this out, we told r “Place into an object called”col2” the product of adding 2 to each item in the list contained in “col1”.\n\ncol2\n\n[1] 4 5 6 7 8\n\n\nCool! We can manipulate our list items all at once.\nSequences\nWe can also have R automatically create sequences of numbers for us, if they follow a regular pattern using the seq() command:\n\nseq(0,100, 5)\n\n [1]   0   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90\n[20]  95 100\n\n\nThis says create a list containing the sequence of numbers from 0 to 100 counting by fives.\nTry creating your own sequence - count up from 4 to 24 by 4\n\n# Your Work Here\nseq(4, 24, 4)\n\n[1]  4  8 12 16 20 24\n\n\nTry creating your another sequence - count down from 50 to 2 by 4. How would you do this?\n\n# Your Work Here\nseq(50, 2, -4)\n\n [1] 50 46 42 38 34 30 26 22 18 14 10  6  2\n\n\nList Interaction\nBut we digress - back to our existing lists. What would happen if we decided to multiply col1 by col2?\n\ncol1*col2\n\n[1]  8 15 24 35 48\n\n\nCan you see what happens here? Since our lists are the same size, R multiples the first item in col1 by the first item in col2, the second item in col1 by the second item in col2, and so on - e.g. (24, 35 …)\nVector Types\nLists, however, don’t have to be just numeric - they can be other types of things as well:\n\n\nNumeric: Values containing integers (positive or negative whole numbers such as 1, 10, 25840) or double values (any real number such as 1, 2.14, 3.254, -12). Double values may also include some special classes such as Inf, -Inf, and NaN - “positive infinity”, “negative infinity”, and “not a number”.\n\nLogical: Logical values including TRUE, FALSE, and NA. TRUE and FALSE are self-explanatory. NA stands for “not available”, which should not be confused with NULL, which is no value.\n\nCharacter: Also known as strings, these consist of text or text-like information. In R, we tend to surround strings by quotation marks to denote them. For instance, c(\"Black Cat\", \"Brown Dog\", \"Dappled Donkey\", \"Red Rooster\") is a character vector containing four items.\nList Manipulation\nWe’ll talk about some other types of vectors later, but these are sufficient to get you started. In addition to numeric vectors, probably the most common other type of vectors we will encounter are character vectors. Let’s make a list of the items we need to make a single serving of oatmeal (your professor is hungry as he writes this tutorial):\n\nc(\"Oatmeal\", \"Water\", \"Salt\", \"Sugar\")\n\n[1] \"Oatmeal\" \"Water\"   \"Salt\"    \"Sugar\"  \n\n\nIn a separate list, let’s place the quantity of ingredients:\n\nc(1/2, 1, .25, 1)\n\n[1] 0.50 1.00 0.25 1.00\n\n\nPerhaps it would be useful to make a third list with the unit of measure for the quantity of ingredients:\n\nc(\"Cup\", \"Cup\", \"Teaspoon\", \"Tablespoon\")\n\n[1] \"Cup\"        \"Cup\"        \"Teaspoon\"   \"Tablespoon\"\n\n\nOkay, we have three lists, that we might be able to use for different things. In your script, write code that assigns the list of ingredients to a new object called “ingredients”, write the quantities to a new object called “quantity”, and write the units to a new object called “units”.\n\n\nYour Work\nSolution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\ningredients &lt;- c(\"Oatmeal\", \"Water\", \"Salt\", \"Sugar\")\nquantity &lt;- c(1/2, 1, .25, 1)\nunits &lt;- c(\"Cup\", \"Cup\", \"Teaspoon\", \"Tablespoon\")\n\n\n\n\nWe could do some interesting things here, like paste together the different list items into something approximating a recipe:\n\npaste(quantity, units, ingredients, sep=\" \")\n\n[1] \"0.5 Cup Oatmeal\"    \"1 Cup Water\"        \"0.25 Teaspoon Salt\"\n[4] \"1 Tablespoon Sugar\"\n\n\nWhat is sep = \" \" doing here? What would happen if you changed sep to a comma? Try it.\nYou can take a look at the documentation for the paste() command by typing ?paste.\nVerbalizing what we just asked R to do (a valuable habit for problem solving more complex functions and data manipulation later on), we said “paste together the list items contained in the variables quantity, units, and ingredients, placing a space between each of the list items.”\nWe can also manipulate list objects - let’s say you volunteer to host a community meeting and need to make 45 portions of your oatmeal recipe - how would you go about constructing your grocery list? Below, write out the operations that you would need to do to modify your existing list of quantities to account for 45 portions (let’s assume that ingredient quantities remain the same when we scale up our recipe):\nSince you’re new at this, here are few ways to do this (I hope you’ve tried on your own to figure it out on your own before reading on) - you could either modify the quantities in the quantity vector by multiplying them directly and creating a new vector:\n\nquantity45 &lt;- quantity*45\npaste(quantity45, units, ingredients, sep=\" \")\n\n[1] \"22.5 Cup Oatmeal\"    \"45 Cup Water\"        \"11.25 Teaspoon Salt\"\n[4] \"45 Tablespoon Sugar\"\n\n\nAlternately, you could modify the list directly in your paste command:\n\npaste(quantity*45, units, ingredients, sep=\" \")\n\n[1] \"22.5 Cup Oatmeal\"    \"45 Cup Water\"        \"11.25 Teaspoon Salt\"\n[4] \"45 Tablespoon Sugar\"\n\n\nThe outputs are exactly the same.\nSelecting List Items\nR can also help us to pick out list items. The brackets [] allow us to return list items by position (left to right).\n\nex_list&lt;-c(\"Jane\", \"Jacobs\", \"beats\", \"Robert\", \"Moses\", \"in\", \"a\", \"fight\", \"for\", \"New York\")\n\nex_list[4]\n\n[1] \"Robert\"\n\n\nWhat would happen if we put [-4] instead of 4?\n\n# Your Work Here\nex_list[-4]\n\n[1] \"Jane\"     \"Jacobs\"   \"beats\"    \"Moses\"    \"in\"       \"a\"        \"fight\"   \n[8] \"for\"      \"New York\"\n\n\nNow you try selecting the tenth element from the list.\n\n\nYour Work\nSolution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\nex_list[10]\n\n[1] \"New York\"\n\n\n\n\n\nWe can also select multiple elements at the same time:\n\nex_list[c(4, 5, 3, 1, 2)]\n\n[1] \"Robert\" \"Moses\"  \"beats\"  \"Jane\"   \"Jacobs\"\n\n\nWe created a list c() and placed it in brackets which told R that we wanted to return the values of ex_list that corresponded to the positions in our other list c(4, 5, 3, 1, 2).\nNow you try creating and manipulating a list.\n\n\nYour Work\nExample Solution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\nex_list_2&lt;-c(\"I have eaten\", \"the plums\", \"that were in\", \"the icebox\", \"and which\", \"you were probably\", \"saving\", \"for breakfast\", \"Forgive me\", \"they were delicious\", \"so sweet\", \"and so cold\")\n\nex_list_2[c(1, 4, 8, 9)]\n\n[1] \"I have eaten\"  \"the icebox\"    \"for breakfast\" \"Forgive me\"   \n\n\n\n\n\nNow re-create the sequence you crafted earlier (count up from 4 to 24 by 4) and subset out the fifth element from that numeric sequence:\n\n\nYour Work\nExample Solution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\nseq(4, 24, 4)[5]\n\n[1] 20\n\n\n\n\n\nFun (maybe), but not yet particularly useful. You didn’t take this class because you wanted to scale oatmeal recipes or identify numbers in a sequence. The more powerful stuff is coming up! - these are building blocks to teach some of the logic of the language.\nData Frames\nThe next thing for us to think about is how we might combine lists together. Thinking back to our oatmeal recipe, right now we have three separate lists each (respectively) with our quantity, units, and ingredients. We’ve figured out that we can paste together items from different lists, but it might be nice to be able to store them in one object rather than three. This is a good time to introduce the R data frame, which is the object you’ll be dealing with the most.\nStart by looking at R’s internal documentation on data frames (?data.frame):\n\n# Your Work Here\n?data.frame\n\nNow lets coerce our three lists into a single data frame called “oatmeal”:\n\noatmeal &lt;- data.frame(quantity, units, ingredients, stringsAsFactors = FALSE)\noatmeal\n\n  quantity      units ingredients\n1     0.50        Cup     Oatmeal\n2     1.00        Cup       Water\n3     0.25   Teaspoon        Salt\n4     1.00 Tablespoon       Sugar\n\n\nWe created a new object called “oatmeal” that has bound our three lists together into a data frame. We need to specify stringsAsFactors = FALSE to keep R from turning our strings (characters) into a special data type called factors (more on these later). R assumes that we want our columns labeled with the original list object names.\nWe can now look at our list as a series of columns that have been given the name of the variable they were stored in as a list, and each row represents one of the list items. An important concept to keep in mind is that a data frame is a series of lists that are in essence glued together.\nWe can refer to and access rows and columns in our data frame in several ways. If we want to return those items in a specific column if the list, we can use the $ operator to refer to that item:\n\noatmeal$ingredients\n\n[1] \"Oatmeal\" \"Water\"   \"Salt\"    \"Sugar\"  \n\n\nWe just returned the list items that were in the column named ingredients. We could also use our subset notation to retrieve the same things. This subset notation builds upon what we learned when we accessed list items by position (e.g. units[2]):\n\n#knitr::include_graphics(\"Images/04_guru99_dataframe.png\")\n\nWhile subsets of lists require one number corresponding to the index position, data frames have two dimensions - rows and columns, so we need to be able to differentiate between each. R does this using a comma in the subset notation [row, column]. If we want all rows or columns, we can just leave that portion of the bracket empty. For instance, the code below is the equivalent of typing oatmeal$ingredients since ingredients are the third column in the oatmeal data frame:\n\noatmeal[,3]\n\n[1] \"Oatmeal\" \"Water\"   \"Salt\"    \"Sugar\"  \n\n\nThis is because ingredients is the third column in the oatmeal data frame.\nNow you try: query the second row of the oatmeal data frame:\n\n\nYour Work\nExample Solution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\noatmeal[2,]\n\n  quantity units ingredients\n2        1   Cup       Water\n\n\n\n\n\nHow would you query the third row of the second column?\n\n\nYour Work\nExample Solution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\noatmeal[3,2]\n\n[1] \"Teaspoon\"\n\n\n\n\n\nOk, so we see that we can create subsets fairly easily, either using column names or index positions in our dataset.\nWe can add new columns to our data frame. Oftentimes when we are working with data, we’ll need to calculate a new column based upon the values in other columns. We have our data frame with a recipe for 1 serving of oatmeal. Let’s say we frequently need to make 45 servings of oatmeal (its famous, and the reason why people show up to your 7am neighborhood meetings…), so you want to include that quantity alongside the single serving quantity.\nLet’s create a new column called “quantity45” and add to it the quantity of ingredients for a 45 serving batch of oatmeal:\n\noatmeal$quantity45&lt;-oatmeal$quantity*45\n\noatmeal\n\n  quantity      units ingredients quantity45\n1     0.50        Cup     Oatmeal      22.50\n2     1.00        Cup       Water      45.00\n3     0.25   Teaspoon        Salt      11.25\n4     1.00 Tablespoon       Sugar      45.00\n\n\nNote that we need to refer to the original quantity by pointing to the oatmeal data frame as well. Let’s verbalize this to think about what we’re doing. “Into a new column in the oatmeal data frame called quantity45 (oatmeal$quantity45 &lt;-), write the value contained in the oatmeal data frame called quantity multiplied by 45 (oatmeal$quantity*45).”\nNow you try - create a new column called “instructions” in the oatmeal data table that contains our recipe quantity for 45 portions of oatmeal, units, and ingredients pasted together (this will require you reference data using concepts we learned earlier):\n\n\nYour Work\nExample Solution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\noatmeal$instructions&lt;-paste(quantity*45, units, ingredients, sep=\" \")\n\n\n\n\nNote again that we need to refer to each of the specific columns in the oatmeal data frame using their appropriate vector (e.g. oatmeal$ingredients. Note that in this case if we omitted the pointer to oatmeal, R would assume we wanted to do something with the list called ingredients. In this case, that would actually work, but in most cases, we will just have a data frame and won’t have a separate list stored as an R object - we’d get an error.\nWe can also pull out all items meeting a specific criteria in our data frame - let’s say we want to look at those ingredients that are measured in cups:\n\noatmeal[oatmeal$units == \"Cup\",]\n\n  quantity units ingredients quantity45     instructions\n1      0.5   Cup     Oatmeal       22.5 22.5 Cup Oatmeal\n2      1.0   Cup       Water       45.0     45 Cup Water\n\n\nThis looks weird - we’ve introduced some new notation here. Let’s first speak this out and then we can learn more about the notation. “From the oatmeal data frame, return those rows from within the oatmeal data frame for which the value of the units column is equal to”Cup” (the equal sign in R is == two equal signs together). Notice also that the word “Cup” has parentheses surrounding it, denoting that it is a character string. The square brackets [] denote that we’re looking for something (or some things) within the oatmeal data frame. We continue to follow the [Row, Column] logic within the brackets.\nWhile in this case, we’re looking for rows that meet a specific criteria based upon the word “Cup” (searching for a character string), we could return subsets of numeric records in other ways:\n\noatmeal[oatmeal$quantity &gt; .5,]\n\n  quantity      units ingredients quantity45        instructions\n2        1        Cup       Water         45        45 Cup Water\n4        1 Tablespoon       Sugar         45 45 Tablespoon Sugar\n\n\nReturns those records from the oatmeal data frame for which the quantity value is greater than .5. If we wanted to include our .5 cups of water, we could specify &gt;= (greater than or equal to).\n\noatmeal[oatmeal$quantity &gt;= .5,]\n\n  quantity      units ingredients quantity45        instructions\n1      0.5        Cup     Oatmeal       22.5    22.5 Cup Oatmeal\n2      1.0        Cup       Water       45.0        45 Cup Water\n4      1.0 Tablespoon       Sugar       45.0 45 Tablespoon Sugar"
  },
  {
    "objectID": "howto/rbasics_01.html#lesson-1-summary-and-debrief",
    "href": "howto/rbasics_01.html#lesson-1-summary-and-debrief",
    "title": "Lesson 1: Basic Principles",
    "section": "Lesson 1 Summary and Debrief",
    "text": "Lesson 1 Summary and Debrief\nIn this lesson, you became more familiar with the R console and RStudio interface, learned about scrips and notebooks, and started to explore some of the basic functionality for how to store and retrieve variables, construct lists, and perform calculations. This may seem like a lot of details to internalize at this point (and it is), but these very basic building blocks will prove useful as you start to understand some of the more advanced functions for data manipulation.\nMoving forward, we’ll start to build on these basic building blocks by looking more at how to manipulate tabular data.\nCore Concepts and Terminology\n\nR Script\nNotebook\nCode Chunk\nVariables\nLists\nVectors\nData Frame\nHelpful Practices\n\nTake your time with mastering and feeling comfortable with basic concepts. While you may be eager to move ahead to move advanced (and interesting) things, if you don’t have a good hold of the underlying logic behind the basics, you’ll struggle to identify and solve problems in the future.\nBegin to internalize the practice of speaking or writing out what the code is doing. R is a language and you are learning to “speak” R. Being able to speak out in plain language what you think your code is doing is the first step to becoming “fluent” as a coder. This will also help you greatly when it comes time to debug code in the future.\nBegin the practice of developing good habits about object names in R - you now know the basic rules about names. Start to think about schemes or personal conventions that you might use to help you stay organized, partiuclarly when you have a lot of named objects in your environment."
  },
  {
    "objectID": "howto/getstarted.html",
    "href": "howto/getstarted.html",
    "title": "Get Started",
    "section": "",
    "text": "Here you will find a range of “how to” resources and code base to help you advance tasks in Neighborhood Analysis."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html",
    "href": "howto/PWBTN/PWBTN.html",
    "title": "Communicating Quantitative Information",
    "section": "",
    "text": "This document draws heavily from Dr. Ed. Feser’s Professional Writing by the Numbers: For Planners and Policy Analysts, Version 4.0 (2006). Updated content focuses on digital-first communication strategies."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#basic-communication-strategies",
    "href": "howto/PWBTN/PWBTN.html#basic-communication-strategies",
    "title": "Communicating Quantitative Information",
    "section": "Basic Communication Strategies",
    "text": "Basic Communication Strategies\n\nMemo Format\nMemos are often used by urban planners to share information within their agency or with other planners and local government officials. Memos tend to be concise documents that may contain either requested information for staff or recommendations on how to debate or take action around a particular policy issue. Memos are built around clear and concise writing with a logical structure, clear illustrations and visualizations, proper formatting and spelling.\nMemos start with a heading that includes the name of the memo author, name of the intended recipient, date, and a descriptive subject heading:\nMemorandum\n\nTo: Janet Jacobs, Planning Director\nFrom: Mae Q. Plannington, Planner 1\nDate: January 7, 2024\nSubject: Recommendation on Zoning Change for Mumford Parcel\nA memo is typically written in response to something. The first paragraph of the memo should describe what the memo is responding to, and should summarize what the writer of the memo has done to prepare the response. The first paragraph should also summarize any key findings or recommendations.\nInformation within a memo should be organized in a logical fashion. Section headings with descriptive titles help your audience find particular information more easily.\nGeneral information should be presented first followed by more detailed information. As you support information with evidence, you should follow the same progression of general to specific.\nMemos should conclude with a summary statement that encapsulates the key findings which you have come up with. If you are providing recommendations in your memo, this is also an appropriate place to re-state your recommendations, and provide instructions for how your memo recipient should follow up.\nBecause a memo is designed to be a concise document, you may have important additional information which would be useful to share, but which does not fit within the scope of the memorandum. Such items can be included in an appendix or attachment to the memo, and should be referenced as appropriate within the memo (e.g. “See the attached document for the property surveyor’s description of the Mumford parcel”).\n\n\nShort Report Format\nPresent your analysis in the format of a short report or “data brief.”\n\n\nDo Not Include Title Page\nTitle pages have their place in academic papers and larger, formal technical reports. However, they are inappropriate for memos and short data or policy briefs. Keep it simple.\n\n\nAvoid Headers and Footers in Memos\nRunning footers and headers also have their place for certain types of documents, but not for memos.\n\n\nNo Conclusion Needed\nIn other contexts, you have likely been taught to include both introductions and conclusions as ways to help transition in and out of different sections. The memo format is more direct, and does not require the use of opening and closing paragraphs. State you case and let it go at that.\n\n\nNotes and Citations\n\nUse Notes for Technical Explanations\nFootnotes or endnotes are best for stating brief technical explanations for methods and data. Reports may require a longer methodological appendix, but this is typically not appropriate for a memorandum.\n\n\nUse Notes for Citations\nUse an endnote or footnote to cite sources for memos and report briefs. Bibliographies or “Works Cited” lists are appropriate for longer reports or academic papers.\n\n\nCombine Data Source Citations\nIt makes good sense to use endnotes or footnotes to discuss data used in an analysis in a memo or report brief. But you do not necessarily need to use a separate note for each data source or series. Instead, combine them by introducing one note early in the document. Something like: “The data used in this analysis are from the U.S. Census Bureau’s 2020 5-year American Community Survey and 2020 Census Household Pulse Survey”.\n\n\n\nFormatting Considerations for Printing\nIncreasingly, memos and other routine planning communications are communicated digitally, however, it is important to prepare documents that will be legible when printed. In most circumstances, formatting considerations for printing will also apply to documents intended to be communicated digitally.\n\nLeft Justify Text\nUse left justification for text, and not full justification.\n\n\nInclude Page Numbers\nInclude page numbers in the bottom center or bottom right of your page.\n\n\nOne Inch Margins\nUse one inch margins on the left, right, top and bottom, including on all pages with tables and figures.\n\n\nTwelve point font\nUse a twelve point font, preferably something like Times for your text. You can use a ten point font for footnotes and endnotes. No text should be smaller than ten points."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#style",
    "href": "howto/PWBTN/PWBTN.html#style",
    "title": "Communicating Quantitative Information",
    "section": "Style",
    "text": "Style\n\nAwkward Sentence Construction\nA catch-all category for a painful-to-read but not necessarily grammatically incorrect sentence. Sentence may be an affront to good writing style. Admittedly, professors often put AWK in places where they know something is amiss, but not being writing instructors, they’re not sure technically how to explain what is wrong. Bottom line: the sentence doesn’t work. Proofread yourself and then have someone else read to ensure clarity.\n\n\nWrite Directly\nDon’t mince words: come out and say it. Instead of “The BLS and BEA data show that employment in the microchip industry is very small,” write “Employment in the microchip industry is very small.” Or, instead of “It is very important to note that data disclosure rules preclude reporting employment for all sectors,” write “Data disclosure rules preclude. . .”\n\n\nAvoid Excessive Use of Jargon\nAvoid the planner’s pitfall: too much use of all those fancy planning terms and acronyms. It’s not impressive or erudite, just boring.\n\n\nAvoid Passive Voice\nThe extensive use of passive voice may be the single most common reason why a majority of sane individuals would rather walk on hot coals than read a technical document. Doesn’t “The region’s population growth dramatically outpaced the nation’s over the study period” sound better than “Using population data, it was found that the region grew much faster than the US over the study period”?\nAnother example: replace “A three-part analysis will be conducted in this paper” with “This paper will present a three-part analysis” or “In this paper, I will conduct a three-part analysis.” Reject passivity. Be active.\n\n\nUse Active Headings and Subheadings\nUse headings and subheadings to help organize your findings and discussion. In addition, keep ’em active and efficient. Instead of “Location Quotient Analysis” as a sub-heading, try a short title that conveys findings, e.g., “Region Specialized in Manufacturing and Services.” Headings and subheadings should never extend beyond one line of text.\n\n\nAvoid Unnecessary Equivocation\nSometimes it makes sense to offer caveats or otherwise “hedge your bets” when discussing a finding. However, don’t overdo it. Population growth doesn’t “seem fast.” It either is or is not fast, relative to something else (which you should be comparing it to). A location quotient indicates whether a region is or is not specialized in a given industry; it does not indicate that the region “seems specialized” or “may be specialized.” On the other hand, location quotients do not say much beyond specialization (you would not damn the torpedoes and argue, on the basis of a manufacturing LQ above 1.2, that the region is “highly competitive” in manufacturing).\nIf you find yourself making equivocations, it’s worthwhile to revisit the evidence you’ve provided to make a point to ensure you’re making the strongest argument possible. Communicate that argument confidently, and provide appropriate caveats when necessary.\n\n\nIn Memos, Don’t Stay What You’re Going to Say\nIn academic papers or longer reports it often makes sense to provide a roadmap to the document (e.g., “This report begins by summarizing major trends in population. It then. . .”). The adage that you should “say what you are going to say, say it, then say what you said” doesn’t apply for short policy and analysis pieces. Just get on with the analysis and findings. Strong organization and active headings will help your reader infer how you’ll make your points.\n\n\nPlaces Don’t Have Agency\nIn discussing social, economic, and demographic trends for neighborhoods, cities and regions, avoid implying that places have “agency.” Example:\nEl Paso shifted its population mix in response to major changes in Federal immigration policy.\nIn this case, El Paso as a collective, is being treated as an actor when it is actually simply a place with a collection of actors (people, households, businesses, organizations) who are reacting to the federal policy change in various ways.\nIt would be more accurate to say something like this:\nEl Paso’s population mix shifted in response to major changes in Federal immigration policy.\nThere may be cases when implying the place is an actor makes sense, for instance when you are discussing a community-wide strategy or policy.\n\n\nDiscuss Findings, Not Exhibits\nPeople don’t want to read about figures and tables. They want to read about trends that matter from your analysis. Avoid discussing exhibits. Instead, discuss findings, referring to figures and tables as supporting evidence.\nNo: “Figure 1 shows that poverty in center city neighborhoods in Cleveland is increasing.”\nYes: “Poverty in center city Cleveland is increasing (see Table 1).”"
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#usage",
    "href": "howto/PWBTN/PWBTN.html#usage",
    "title": "Communicating Quantitative Information",
    "section": "Usage",
    "text": "Usage\n\nPrint and Proof\nWe increasingly have an imperative to produce, edit, and disseminate our work using a completely electronic workflow. This has its benefits, but you need to develop a workflow that allows you to edit your draft with diligence. Printing and proofing a document by hand can often catch errors that would otherwise become lost in digital copy - such mistakes often reveal themselves in embarrassingly stark relief once disseminated.\n\n\nDefine Acronyms on First Use\nThe first time you use an acronym (e.g., HUD), spell it out, followed by the acronym in parentheses. “Data are from the U.S. Department of Housing and Urban Development (HUD).” Then use the acronym to your heart’s content.\n\n\nWrite out Numbers Less than 11\nAlthough sometimes the convention is that numbers one through ninety-nine should be spelled out. Numerals should be used for rates, percentages and other “data” indicators. Thus we would write “there are eight counties in the MSA,” but “the region grew by 8 percent.”\n\n\nSpell out Numbers Starting Sentences\nAny number that begins a sentence (or a bullet point) should be spelled out.\n\n\nWatch Your Capitalization\nNo need to capitalize industries, occupations, or other sectoral-type categories. Do not over-capitalize. Note that when referring to a single county, write “Tehama County.” But writing about multiple counties, it is “Tehama and Shasta counties.” Also: the “City of New York” but the “cities of Palo Alto and San Jose.”\n\n\nNo Apostrophe on Dates\nWhen referring to decades (e.g., 1990s), do not use an apostrophe.\n\n\nData are Plural\nNo: “The data is hard to find.”\nYes: “The data are hard to find.”\n\n\nUse Proper Note Punctuation\nNote numbers are best placed at the end of a sentence outside the punctuation.\nYes: This is a sentence that requires a citation.1\nNo: This is a sentence that requires a citation2.\n\n\nComplete Sentences in Notes\nFootnotes and endnotes should be complete sentences. Complete sentences have punctuation at the end.\n\n\nPunctuation Inside Quotes\nPunctuation generally goes inside quotes.\nYes: “Run, Tom, run,” said Jane.\nNo: “Down, Spot, down”, said Percival, Dick’s little-known cousin from Topeka.\n\n\nWrite out Percent\nOne of the few times the “less ink is better” rule is violated. Write “8 percent,” not “8%.” It’s less distracting to the eye.\n\n\nUse Arabic Numbers for Notes\nMicrosoft Word often defaults to the use of Roman numerals for notes and endnotes (i, ii, iii. . .). Change this option and use Arabic numerals. More efficient.\n\n\nCiting the URL is Not Enough\nIn the Internet age it has become distressingly common practice to cite only the URL for online documents. But consider this: if you quoted Tolstoy’s War and Peace using a copy you checked out from the New York Public Library, you would not cite the library as the source. The same principle applies to the Internet. For web sites, which are inherently Internet-based, you should list the name of the cite and then the URL. For documents accessed online, you should cite in the usual way (author, date, title, etc.) and then include the URL. Note that you are not obligated to list the URL for freestanding documents if you include the complete citation otherwise.\n\n\nAvoid Ampersand (&) in Text\nThe symbol “&”, known as the ampersand, should not be used in your text write-up. It is ok to use it for labels in tables and figures."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#numbers",
    "href": "howto/PWBTN/PWBTN.html#numbers",
    "title": "Communicating Quantitative Information",
    "section": "Numbers",
    "text": "Numbers\n\nInterpret Quantities by Comparison\nDo not just report growth rates, quantities and other indicators for individual places. They are hard to interpret by themselves. For example, to back up a claim that your region has faced substantial population growth in the last decade, contrast its growth rate with the national average growth rate.\n\n\nExplain Regional Geography\nThe first time you mention your region, explain–either in the text or in a note–what its geographic composition is (e.g., its counties).\n\n\nSignificant Digits\nUse numbers with levels of precision that match the realistic precision in the underlying data. Should a location quotient be expressed as 1.709? No. Round to the nearest tenth (1.7). Percentage growth rates for subnational areas are also usually best expressed with one decimal place. Shares can be converted to percentages and expressed to one decimal place to make them more readable.\n\n\nOnly Include Exhibits You Reference in Text\nNo table, figure, chart or line drawing should be included in the report or report appendix unless it is referenced somewhere in the text discussion. That reference may be very brief (“see Table 4”), but it has to be there (it could also be in an endnote or footnote). Think of it this way: if it wasn’t important enough for you to mention it, why did you include it?\n\n\nReport the Quantity\nSometimes it is easy to forget to report the variable levels when we are analyzing variable trends. For example, it is common in analyses of wage trends (“wages are on the up and up”) to find nary a mention of wage levels. As a reader, you are left wondering “wages are going up, but are they high or low?” It is better to ground an analysis of changes in a given variable with a mention of the levels of the variable. So “The current annual average wage in River City for production workers is $27,500. That is up 12 percent in real terms since 1997. By contrast, the average production worker wage nationwide increased by 16.5 percent. . .”\n\n\nEmphasize Significant Findings\nWhen analyzing data we are looking for the most significant findings and often a “story” that helps explain those findings. In descriptive analysis, significance is often first assessed not in a statistical sense, but by looking for high and low values or major changes. But not all high/low values or large changes are necessarily important from a policy or planning point of view. Be careful to think through the potential implications of a finding before discussing it. Ask yourself: “So what?” If you can’t think of an answer, leave it out. (Example: “Since 1990, River City’s unemployment rate has registered below the U.S. rate in every quarter except IIQ 1996 and IVQ 1998” might be better stated as “River City’s unemployment rate has registered below the U.S. rate in 54 of the last 56 quarters.” A subtle change but the reader is not left asking: “Hmm, I wonder if I’m supposed to know why it was higher in those two periods?”\n\n\nPercent versus Percentage Points\nLet’s say you’re comparing the U.S. unemployment rate of 5.0 percent to the Peoria unemployment rate of 4.0 percent. Is the Peoria rate 1 percent lower than the national rate? No. It is 1 percentage point lower. A Peoria rate 1 percent lower than the national average would be 4.95 percent.\n\n\nRefer to Exhibits in Text\nRefer to your figures and tables directly in your text. Example: “Table 2 reports employment growth figures for the 1995 to 2003 period.” Or: “Employment growth was particularly strong in the retail and construction sectors (see Table 4).” If you are going to send readers to a table in a parenthetical phrase (like the last example), be sure to include the word “see.” So, you would not write: “Employment growth was particularly strong in the retail and construction sectors (Table 4).”"
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#tables-figures-and-graphics",
    "href": "howto/PWBTN/PWBTN.html#tables-figures-and-graphics",
    "title": "Communicating Quantitative Information",
    "section": "Tables, Figures and Graphics",
    "text": "Tables, Figures and Graphics\n\nMake ’Em so they Stand Alone\nFigures, graphs and tables should be constructed so that they can stand alone (as much as feasible). That is, someone could pick up and read the table without the accompanying text and get the gist of what it is trying to say. It goes without saying that sources of all data and calculations should be clearly indicated.\n\n\nAvoid Use of Grids in Tables\nThere is almost never any reason to include gridlines on a data table. Putting a line below the column headings and one below the last row of data, followed by the data source, is usually the best approach.\n\n\nDecimal Justify Data Columns\nLine up columns of numbers in tables on their explicit or implicit decimal points. Do not center justify numeric data.\n\n\nUse Descriptive Column Labels in Tables\nRemember the golden rule in table and figure construction: make it stand alone. That means column headings that someone can understand without reading the report body. Sometimes this is hard to do efficiently (headings can get too long). In such cases an alternative is to use short-hand headings but explain what they mean in a footnote to the table.\n\n\nUse Label Hierarchies in Tables\nA good way to make tables more efficient is to use hierarchical labeling of columns. In the table below, employment and payroll data for Illinois were available in 2003. But national and regional data were available only for 2001. The table efficiently reports the 2003 Illinois numbers along with national and Midwest growth rates for 1990-2001 and location quotients for 2001. A note at the bottom of the table should clarify how the Midwest is defined as well as what the reference area is for the location quotient.\n\n\n\nIndicate Source on Figures and Tables\nData sources should be indicated clearly on all figures and tables.\n\n\nUse Detailed Sources\nFor some series simply listing the data provider agency is not enough. For example, BLS reports several employment series. Therefore, list the series within BLS (or other) that you are using. For example, Regional Economic Information System, US Bureau of Economic Analysis, or Covered Wages and Employment, US Bureau of Labor Statistics.\n\n\nPie Chart? No Thanks\nWhile seductive to the eye, pie charts are far less interpretable than a simple bar chart. Go for conveying your findings clearly, not spicing up your document with spurious graphical devices.\n\n\nShun 3-D\nThree dimensional bar charts and other graphics generally should be avoided. They usually compromise proper interpretation of the findings. Keep it simple: no 3-D.\n\n\nCall Tables “Tables” and Figures “Figures”\nBy convention, tables are called “tables” and graphics such as data charts or line drawings are called “figures.” Sometimes people use the word “exhibit” to label a line drawing. Avoid use of the terms “chart” and “graphic” for labels (e.g., Chart 1, Graphic 1). Also, never call a table a figure or a figure a table. Note that exhibit labels are generally capitalized: “Population trends for River City are summarized in Table 4.”\n\n\nSequence Tables and Figures in Order in the Text\nIf you have three tables, do not discuss Table 1, then Table 3, then Table 2. If the results in Table 3 are mentioned prior to those in Table 2, renumber the tables. Same for figures or any other exhibit.\n\n\nUse Commas for Numbers\nIn data tables, include commas in the number formatting to denote significant digits. So: $47,500 instead of $47500.\n\n\nAvoid Grids on Figures\nMany graphical packages default settings include gridlines on charts. Most charts are simple enough not to need them to be clearly interpreted. Get rid of the clutter and save some ink!\n\n\nDon’t Rely on Color\nRemember that your best intents for people to view your document in color may be foiled - some individuals may print documents in grayscale, and others may be colorblind and unable to perceive all colors. That means your document should not depend on color to convey its findings. An example of depending on color would be to use red text in your tables to indicate negative values or any color to highlight interesting trends.\n\n\nReport Numbers in 000s\nIt is ok to report very large numbers in thousands or even millions in tables if the precision in the original units is unnecessary. It is probably not critical in most cases for people to know that the US population changed by 1,456,789 over a given period (1.46 million will probably suffice). For small areas, such as counties and regions, reporting the original units is usually preferred except for large number variables (e.g., total dollar income).\n\n\nNo Superfluous Material\nAvoid the practice of tacking on government documents or tables to your reports as “general points of information.” The idea in most professional writing is to convey maximum information with minimum material; tossing in nonessential material from secondary sources defeats that aim."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#footnotes",
    "href": "howto/PWBTN/PWBTN.html#footnotes",
    "title": "Communicating Quantitative Information",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere’s the citation↩︎\nA citation though the note number is incorrectly placed↩︎"
  },
  {
    "objectID": "howto/index.html",
    "href": "howto/index.html",
    "title": "How To",
    "section": "",
    "text": "R is an open source programming language that is a common tool used for data analysis across a range of disciplines. This means that in addition to being free and available for a range of operating systems and environments, R is also directly supported by a diverse user community who continually develop approaches for specialized applications or data. Need to download U.S. Census Data? There’s an R package for that. Need to perform common data cleaning tasks? There’s an R package for that too. We’ll be exploring a range of these specialized applications over the course of the semester.\nOf course there are alternate languages which we could employ in service of neighborhood analysis. Python, for example, is an even more ubiquitous programming language with its own set of tools for data science. R was originally built as a statistical computing language, and that brings some important benefits for the types of data science we’ll be learning this semester. R is also fairly prevalent among the user community working in public policy analysis and urban data science - this is the user community which you will be joining. Finally, R has a high learning curve, but also a very active user community, meaning that abundant documentation of problems and their solutions is available.\nAs we get started, let’s be clear - you are going to experience some frustration and challenges as you learn the R programming language. This class assumes no prior background in R or any other programming language for that matter, and we’ll work to quickly build your “vocabulary” and the ability to get results. We will spend some time picking up basics, and will then use our exploration of specific analysis approaches to reinforce our use of the grammar and structure of the language and to build more complex scripts over time.\nIt’s fair to equate learning R with learning to drive a manual car. Increasingly, people learn how to drive in automatic cars - essentially allowing the car to handle the function of switching gears - you put the car into drive, press the accelerator pedal and the car moves forward. Your past exposure to computer-based analytic tools has probably followed a similar strategy - you likely learned using software that had graphical user interfaces that allow them to call up and run programs and then spit out results. Most of us learn to point and click in order to accomplish a particular set of analytic tasks, meaning that if we want to generate the same results in the future, we would have to repeat all of those same steps over again.\nSo why learn on a manual? For many car enthusiasts, manuals are both more efficient and more engaging to drive - they offer additional control, and come with a heightened awareness of what the car is doing. Of course, they also come with a steep learning curve.\nSome of the same attributes apply to our use of R as a tool for analysis:\n\nFirst, a manual approach forces you to explicitly understand more of the requirements and assumptions that go into the analysis that you’re doing.\nSecond, you have to know your data (and its strengths and limitations) well in order to get it set up for analysis and to produce useful output.\nThird, this approach emphasizes reproducible analysis, meaning that you will develop workflows that can be repeated over and over again producing the same results - important for sharing your work with others and for accountability, especially within contexts related to public deliberation of the strengths and weaknesses of policy arguments.\n\nKeeping these three benefits in mind, we’ll be using the R software coupled with the RStudio Integrated Development Environment (IDE) to counter some of the downsides of a purely manual approach. RStudio will help us implement R code more effectively and efficiently. Hopefully at this point, this prospect leaves you excited rather than daunted!"
  },
  {
    "objectID": "howto/index.html#introduction",
    "href": "howto/index.html#introduction",
    "title": "How To",
    "section": "",
    "text": "R is an open source programming language that is a common tool used for data analysis across a range of disciplines. This means that in addition to being free and available for a range of operating systems and environments, R is also directly supported by a diverse user community who continually develop approaches for specialized applications or data. Need to download U.S. Census Data? There’s an R package for that. Need to perform common data cleaning tasks? There’s an R package for that too. We’ll be exploring a range of these specialized applications over the course of the semester.\nOf course there are alternate languages which we could employ in service of neighborhood analysis. Python, for example, is an even more ubiquitous programming language with its own set of tools for data science. R was originally built as a statistical computing language, and that brings some important benefits for the types of data science we’ll be learning this semester. R is also fairly prevalent among the user community working in public policy analysis and urban data science - this is the user community which you will be joining. Finally, R has a high learning curve, but also a very active user community, meaning that abundant documentation of problems and their solutions is available.\nAs we get started, let’s be clear - you are going to experience some frustration and challenges as you learn the R programming language. This class assumes no prior background in R or any other programming language for that matter, and we’ll work to quickly build your “vocabulary” and the ability to get results. We will spend some time picking up basics, and will then use our exploration of specific analysis approaches to reinforce our use of the grammar and structure of the language and to build more complex scripts over time.\nIt’s fair to equate learning R with learning to drive a manual car. Increasingly, people learn how to drive in automatic cars - essentially allowing the car to handle the function of switching gears - you put the car into drive, press the accelerator pedal and the car moves forward. Your past exposure to computer-based analytic tools has probably followed a similar strategy - you likely learned using software that had graphical user interfaces that allow them to call up and run programs and then spit out results. Most of us learn to point and click in order to accomplish a particular set of analytic tasks, meaning that if we want to generate the same results in the future, we would have to repeat all of those same steps over again.\nSo why learn on a manual? For many car enthusiasts, manuals are both more efficient and more engaging to drive - they offer additional control, and come with a heightened awareness of what the car is doing. Of course, they also come with a steep learning curve.\nSome of the same attributes apply to our use of R as a tool for analysis:\n\nFirst, a manual approach forces you to explicitly understand more of the requirements and assumptions that go into the analysis that you’re doing.\nSecond, you have to know your data (and its strengths and limitations) well in order to get it set up for analysis and to produce useful output.\nThird, this approach emphasizes reproducible analysis, meaning that you will develop workflows that can be repeated over and over again producing the same results - important for sharing your work with others and for accountability, especially within contexts related to public deliberation of the strengths and weaknesses of policy arguments.\n\nKeeping these three benefits in mind, we’ll be using the R software coupled with the RStudio Integrated Development Environment (IDE) to counter some of the downsides of a purely manual approach. RStudio will help us implement R code more effectively and efficiently. Hopefully at this point, this prospect leaves you excited rather than daunted!"
  },
  {
    "objectID": "howto/rbasics_02.html#lesson-goals",
    "href": "howto/rbasics_02.html#lesson-goals",
    "title": "Lesson 2: Manipulating Data Frames",
    "section": "Lesson Goals",
    "text": "Lesson Goals\nBy the end of this lesson, you should be familiar with:\n\nBasic tools for selecting and subsetting data\nStrategies for describing data"
  },
  {
    "objectID": "howto/rbasics_02.html#getting-set-up",
    "href": "howto/rbasics_02.html#getting-set-up",
    "title": "Lesson 2: Manipulating Data Frames",
    "section": "Getting Set Up",
    "text": "Getting Set Up\nThe Data\nSo far, we’ve worked with “tiny” bits of data - mainly things that we input manually. It’s time to start working with some real world data!\nWe’re going to work with data on those census tracts that were designated as Opportunity Zones as part of the federal Tax Cuts and Jobs Act of 2017. These incentives are designed to spur investment in low-income and undercapitalized cities, by providing investors with tax incentives to invest capital in these locations. Each state had the opportunity to designate specific census tracts as Opportunity Zones. Practitioners and researchers have many questions about the efficacy of the program and the designations made by governors.\nTake a look here to see a map of where designated opportunity zones are located. The orange geometries reflected on the map are census tracts, which we often use as a proxy for neighborhoods, especially in urban areas. Find a place you know, and take a look at which areas are designated.\nThe specific data we’ll work with comes from the Urban Institute - they have joined the IRS’s list of Opportunity Zones designated by the Tax Cuts and Jobs Act to a series of indicators focused on investment potential. A copy of the Urban Institute’s dataset is available here for download.\n\n\nYou’ll need to authenticate and log in to UIUC Box to access this file. You can also download the data directly from Urban Institute’s Opportunity Zone landing page.\nExtending Base R With Packages\nWe’re been working exclusively in “base” R as we are getting familiar with the R language and RStudio interface. This means we’ve been working with the commands and functions that come with every new installation of R.\nThere’s a whole world of other functions that we can use to extend R’s functionality - these packages can help us do things like load data with specific formats, make visualizations, and run specific types of statistical models. There’s more than 20,000 packages which users have created to extend R’s functionality - you can find a list here. We’ll introduce common packages which help with neighborhood analysis tasks, but your favorite search engine is probably the easiest way to find packages or functions that may help you read specific data or perform certain types of tasks.\nReadxl\nOur first exploration with packages will help us read in our Opportunity Zone data. We will use a package called readxl which is designed to, well, read Microsoft Excel files such as the Opportunity Zone data.\nInstalling Readxl\nWe will use the readxl package to help us read our Opportunity Zone data from an Excel workbook into Rstudio so we can view it as a data frame. The first time you want to make use of a package you have not used before, you’ll need to install it, which essentially means R needs to download it from a repository on the internet and prepare it to be accessed on your computer.\nWe use the install.packages() command to automatically download and prepare the package for use:\n\ninstall.packages(\"readxl\")\n\n\n\nNote that the name of the package is in quotes here. Any guesses as to why?\nNote that we only need to install the package one time. It will remain available on your machine for use in the future.\nLoading Readxl\nIn our previous step, we installed the readxl package which involved downloading it form an internet repository and preparing it for use. There’s an additional step - now we need to load the package, which tells R that we want to make the package’s functions available for use in our current R session. The library() command loads packages for use in our current R session. While we only need to install a package one time, each time you start a new R session, you’ll need to load the packages you want to use for a particular kind of additional functionality.\n\nlibrary(readxl)\n\n\n\nNote that the name of the package is not in quotes here. Any guesses as to why?\nNow that the readxl package is loaded, we can make use of it to read our Opportunity Zone data into our R session.\nTakeaways\n\nIn general, you will only need to use install.packages() once (once a package is installed, you can use it for all R sessions moving forward).\nTo load packages, we use the library() command. This will load an already installed package and make it accessible within our current R session. You will need to load already installed packages at the beginning of each new R session. Typically, it is a good practice to load the packages you’ll use in a script at the beginning of the script.\n\nNote that to install the package, you need to treat the package name as a character vector \"readxl\", but when you load it in your R session, it does not need to be treated as a character vectorreadxl."
  },
  {
    "objectID": "howto/rbasics_02.html#read-in-data",
    "href": "howto/rbasics_02.html#read-in-data",
    "title": "Lesson 2: Manipulating Data Frames",
    "section": "Read in Data",
    "text": "Read in Data\nNow that we’ve loaded the readxl package, we can import the Excel file containing data on tracts designated as opportunity zones in the United States. To learn about the functions in the package, you can either do a Google search for Readxl, or you can use R’s built in documentation by typing ?readxl\n\n?readxl\n\nNote that running this command pops open documentation for readxl in the Help tab of the auxiliary pane. As the documentation states, readxl imports excel files. Looking at the documentation, the read_excel() command will read a single excel sheet, or we can optionally select a sheet by name or number from an excel workbook with multiple sheets. In this case, the Urban Institute data is in a workbook with a single sheet, so we just need to tell R where the file is to load.\nThe dataset we want to load is called “urbaninstitute_tractlevelozanalysis_update01142021.xlsx” (oof! - that’s a descriptive but way too long file name!). We can point R to the correct location. Since our R project file sets the relative path for all of the work within, the path to the data is:\"data/urbaninstitute_tractlevelozanalysis_update1242018.xlsx\". Wrapped into the command to read the excel file, it looks like this:\n\nread_excel(\"data/urbaninstitute_tractlevelozanalysis_update01142021.xlsx\")\n\nR read the data and is displaying it to us. But one problem - it read the data in and we can look at it, but it’s not really available for our continued use. We typically want to read data and store it as an object so that we can refer back to it and even modify it.\nLet’s go ahead and read the Excel data in again, but this time, we’ll assign it to an object called “ozs”:\n\nozs&lt;-read_excel(\"data/urbaninstitute_tractlevelozanalysis_update01142021.xlsx\")\n\nLook at your Environment window (top right quadrant of RStudio) - a data frame containing information on opportunity zones should be loaded in an object called “ozs”.\nThe environment window tells us that the object ozs contains 42,176 observations (rows) and 27 variables (columns).\nIf we type the name of the object, we can view it’s contents:\n\nozs\n\n# A tibble: 42,178 × 27\n   geoid       state   DesignatedOZ county    Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 01001020200 Alabama           NA Autauga … Low-…         4      NA       2196\n 2 01001020300 Alabama           NA Autauga … Non-…         6      NA       3136\n 3 01001020700 Alabama            1 Autauga … Low-…         8      NA       3047\n 4 01001020802 Alabama           NA Autauga … Non-…        10      NA      10743\n 5 01001021000 Alabama           NA Autauga … Non-…         5      NA       2899\n 6 01001021100 Alabama           NA Autauga … Low-…         6      NA       3247\n 7 01003010100 Alabama           NA Baldwin … Non-…         6      NA       4013\n 8 01003010200 Alabama            1 Baldwin … Low-…         9      NA       3067\n 9 01003010300 Alabama           NA Baldwin … Non-…        10      NA       8079\n10 01003010400 Alabama            1 Baldwin … Non-…         9      NA       4578\n# ℹ 42,168 more rows\n# ℹ 19 more variables: medhhincome &lt;dbl&gt;, PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;,\n#   medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;,\n#   vacancyrate &lt;dbl&gt;, pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;,\n#   pctAAPIalone &lt;dbl&gt;, pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;,\n#   BAorhigher &lt;dbl&gt;, Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\nYou could also inspect the dataset using the View() command. This will allow us to look at the data in a tabular format.\n\nView(ozs)\n\nNow, use the str() (structure) command to gain a better understanding of the types of data in each column\n\nstr(ozs)\n\ntibble [42,178 × 27] (S3: tbl_df/tbl/data.frame)\n $ geoid           : chr [1:42178] \"01001020200\" \"01001020300\" \"01001020700\" \"01001020802\" ...\n $ state           : chr [1:42178] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ DesignatedOZ    : num [1:42178] NA NA 1 NA NA NA NA 1 NA 1 ...\n $ county          : chr [1:42178] \"Autauga County\" \"Autauga County\" \"Autauga County\" \"Autauga County\" ...\n $ Type            : chr [1:42178] \"Low-Income Community\" \"Non-LIC Contiguous\" \"Low-Income Community\" \"Non-LIC Contiguous\" ...\n $ dec_score       : num [1:42178] 4 6 8 10 5 6 6 9 10 9 ...\n $ SE_Flag         : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n $ Population      : num [1:42178] 2196 3136 3047 10743 2899 ...\n $ medhhincome     : num [1:42178] 41107 51250 45234 61242 49567 ...\n $ PovertyRate     : num [1:42178] 0.24 0.107 0.19 0.153 0.151 ...\n $ unemprate       : num [1:42178] 0.0775 0.051 0.1407 0.0459 0.0289 ...\n $ medvalue        : num [1:42178] 95300 113800 93500 160400 102900 ...\n $ medrent         : num [1:42178] 743 817 695 1018 546 ...\n $ pctown          : num [1:42178] 0.628 0.703 0.711 0.823 0.83 ...\n $ severerentburden: num [1:42178] 0.3269 0.3223 0.3887 0.1994 0.0994 ...\n $ vacancyrate     : num [1:42178] 0.0584 0.1399 0.0619 0.0609 0.2182 ...\n $ pctwhite        : num [1:42178] 0.439 0.671 0.833 0.814 0.726 ...\n $ pctBlack        : num [1:42178] 0.5187 0.205 0.0922 0.1572 0.2456 ...\n $ pctHispanic     : num [1:42178] 0.01275 0.0727 0.0338 0.01368 0.00448 ...\n $ pctAAPIalone    : num [1:42178] 0.01093 0.01052 0 0.00959 0 ...\n $ pctunder18      : num [1:42178] 0.218 0.224 0.249 0.27 0.245 ...\n $ pctover64       : num [1:42178] 0.124 0.175 0.149 0.122 0.156 ...\n $ HSorlower       : num [1:42178] 0.581 0.464 0.544 0.45 0.621 ...\n $ BAorhigher      : num [1:42178] 0.162 0.219 0.113 0.229 0.136 ...\n $ Metro           : num [1:42178] 1 1 1 1 1 1 1 1 1 1 ...\n $ Micro           : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n $ NoCBSAType      : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n\n\nWe get a list of the columns in the data, along with their types (in this case character or numeric), and then we see the values associated with the first few observations.\nA few things to note after your preliminary inspection:\n\nThese data are at the census tract level and include geographic identifiers including geoid, the combined, state-county-tract FIPS code, state the state name, and county the county name.\nThese data include a field named Designated which is 1 when an eligible tract was designated as an opportunity zone, and NA where the tract was not designated.\nThe dataset also includes some other tract-level demographic measures, as well as additional geographic flags."
  },
  {
    "objectID": "howto/rbasics_02.html#describing-the-data",
    "href": "howto/rbasics_02.html#describing-the-data",
    "title": "Lesson 2: Manipulating Data Frames",
    "section": "Describing the Data",
    "text": "Describing the Data\nWhen we load a new dataset that we’re not familiar with, it’s a good idea to spend a few minutes describing the data. This allows us to understand a bit more about it’s structure and may also help us ask some basic questions about the validity and reliability of the data (at least compared to what we are expecting to see). R has several functions for determining the structure of data frames and tibbles. See below:\nSize\n\n\ndim(ozs): returns a vector with the number of rows in the first element, and the number of columns as the second element (the dimensions of the object)\n\n\ndim(ozs)\n\n[1] 42178    27\n\n\n\n\nnrow(ozs): returns the number of rows\n\n\nnrow(ozs)\n\n[1] 42178\n\n\n\n\nncol(ozs): returns the number of columns\n\n\nncol(ozs)\n\n[1] 27\n\n\nContent\n\n\nhead(ozs): shows the first 6 rows\n\n\nhead(ozs)\n\n# A tibble: 6 × 27\n  geoid state DesignatedOZ county Type  dec_score SE_Flag Population medhhincome\n  &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 0100… Alab…           NA Autau… Low-…         4      NA       2196       41107\n2 0100… Alab…           NA Autau… Non-…         6      NA       3136       51250\n3 0100… Alab…            1 Autau… Low-…         8      NA       3047       45234\n4 0100… Alab…           NA Autau… Non-…        10      NA      10743       61242\n5 0100… Alab…           NA Autau… Non-…         5      NA       2899       49567\n6 0100… Alab…           NA Autau… Low-…         6      NA       3247       40801\n# ℹ 18 more variables: PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;, medvalue &lt;dbl&gt;,\n#   medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;, vacancyrate &lt;dbl&gt;,\n#   pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;, pctAAPIalone &lt;dbl&gt;,\n#   pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;, BAorhigher &lt;dbl&gt;,\n#   Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\n\n\ntail(ozs): shows the last 6 rows\n\n\ntail(ozs)\n\n# A tibble: 6 × 27\n  geoid state DesignatedOZ county Type  dec_score SE_Flag Population medhhincome\n  &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 7803… &lt;NA&gt;            NA &lt;NA&gt;   Non-…        NA      NA         NA          NA\n2 7803… Virg…            1 &lt;NA&gt;   Non-…        NA      NA         NA          NA\n3 7803… Virg…            1 &lt;NA&gt;   Low-…        NA      NA         NA          NA\n4 7803… Virg…            1 &lt;NA&gt;   Low-…        NA      NA         NA          NA\n5 7803… Virg…            1 &lt;NA&gt;   Low-…        NA      NA         NA          NA\n6 7803… Virg…            1 &lt;NA&gt;   Low-…        NA      NA         NA          NA\n# ℹ 18 more variables: PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;, medvalue &lt;dbl&gt;,\n#   medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;, vacancyrate &lt;dbl&gt;,\n#   pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;, pctAAPIalone &lt;dbl&gt;,\n#   pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;, BAorhigher &lt;dbl&gt;,\n#   Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\nNames\n\n\nnames(ozs): returns the column names as a list\n\n\nnames(ozs)\n\n [1] \"geoid\"            \"state\"            \"DesignatedOZ\"     \"county\"          \n [5] \"Type\"             \"dec_score\"        \"SE_Flag\"          \"Population\"      \n [9] \"medhhincome\"      \"PovertyRate\"      \"unemprate\"        \"medvalue\"        \n[13] \"medrent\"          \"pctown\"           \"severerentburden\" \"vacancyrate\"     \n[17] \"pctwhite\"         \"pctBlack\"         \"pctHispanic\"      \"pctAAPIalone\"    \n[21] \"pctunder18\"       \"pctover64\"        \"HSorlower\"        \"BAorhigher\"      \n[25] \"Metro\"            \"Micro\"            \"NoCBSAType\"      \n\n\nSummaries\n\n\nstr(ozs): structure of the object and information about the class, length and content of each column\n\n\nstr(ozs)\n\ntibble [42,178 × 27] (S3: tbl_df/tbl/data.frame)\n $ geoid           : chr [1:42178] \"01001020200\" \"01001020300\" \"01001020700\" \"01001020802\" ...\n $ state           : chr [1:42178] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ DesignatedOZ    : num [1:42178] NA NA 1 NA NA NA NA 1 NA 1 ...\n $ county          : chr [1:42178] \"Autauga County\" \"Autauga County\" \"Autauga County\" \"Autauga County\" ...\n $ Type            : chr [1:42178] \"Low-Income Community\" \"Non-LIC Contiguous\" \"Low-Income Community\" \"Non-LIC Contiguous\" ...\n $ dec_score       : num [1:42178] 4 6 8 10 5 6 6 9 10 9 ...\n $ SE_Flag         : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n $ Population      : num [1:42178] 2196 3136 3047 10743 2899 ...\n $ medhhincome     : num [1:42178] 41107 51250 45234 61242 49567 ...\n $ PovertyRate     : num [1:42178] 0.24 0.107 0.19 0.153 0.151 ...\n $ unemprate       : num [1:42178] 0.0775 0.051 0.1407 0.0459 0.0289 ...\n $ medvalue        : num [1:42178] 95300 113800 93500 160400 102900 ...\n $ medrent         : num [1:42178] 743 817 695 1018 546 ...\n $ pctown          : num [1:42178] 0.628 0.703 0.711 0.823 0.83 ...\n $ severerentburden: num [1:42178] 0.3269 0.3223 0.3887 0.1994 0.0994 ...\n $ vacancyrate     : num [1:42178] 0.0584 0.1399 0.0619 0.0609 0.2182 ...\n $ pctwhite        : num [1:42178] 0.439 0.671 0.833 0.814 0.726 ...\n $ pctBlack        : num [1:42178] 0.5187 0.205 0.0922 0.1572 0.2456 ...\n $ pctHispanic     : num [1:42178] 0.01275 0.0727 0.0338 0.01368 0.00448 ...\n $ pctAAPIalone    : num [1:42178] 0.01093 0.01052 0 0.00959 0 ...\n $ pctunder18      : num [1:42178] 0.218 0.224 0.249 0.27 0.245 ...\n $ pctover64       : num [1:42178] 0.124 0.175 0.149 0.122 0.156 ...\n $ HSorlower       : num [1:42178] 0.581 0.464 0.544 0.45 0.621 ...\n $ BAorhigher      : num [1:42178] 0.162 0.219 0.113 0.229 0.136 ...\n $ Metro           : num [1:42178] 1 1 1 1 1 1 1 1 1 1 ...\n $ Micro           : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n $ NoCBSAType      : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n\n\n\n\nsummary(ozs): summary statistics for each column\n\n\nsummary(ozs)\n\n    geoid              state            DesignatedOZ      county         \n Length:42178       Length:42178       Min.   :1       Length:42178      \n Class :character   Class :character   1st Qu.:1       Class :character  \n Mode  :character   Mode  :character   Median :1       Mode  :character  \n                                       Mean   :1                         \n                                       3rd Qu.:1                         \n                                       Max.   :1                         \n                                       NA's   :33414                     \n     Type             dec_score         SE_Flag        Population   \n Length:42178       Min.   : 1.000   Min.   :1       Min.   :    0  \n Class :character   1st Qu.: 3.000   1st Qu.:1       1st Qu.: 2752  \n Mode  :character   Median : 5.000   Median :1       Median : 3897  \n                    Mean   : 5.495   Mean   :1       Mean   : 4147  \n                    3rd Qu.: 8.000   3rd Qu.:1       3rd Qu.: 5224  \n                    Max.   :10.000   Max.   :1       Max.   :40616  \n                    NA's   :1239     NA's   :41113   NA's   :112    \n  medhhincome      PovertyRate       unemprate          medvalue      \n Min.   :  2499   Min.   :0.0000   Min.   :0.00000   Min.   :   9999  \n 1st Qu.: 32014   1st Qu.:0.1381   1st Qu.:0.05900   1st Qu.:  85700  \n Median : 41094   Median :0.2055   Median :0.08735   Median : 122400  \n Mean   : 42153   Mean   :0.2331   Mean   :0.10063   Mean   : 165663  \n 3rd Qu.: 50833   3rd Qu.:0.2996   3rd Qu.:0.12600   3rd Qu.: 191300  \n Max.   :181406   Max.   :1.0000   Max.   :1.00000   Max.   :2000001  \n NA's   :249      NA's   :141      NA's   :141       NA's   :1106     \n    medrent           pctown       severerentburden  vacancyrate     \n Min.   :  99.0   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.: 655.0   1st Qu.:0.3833   1st Qu.:0.1662   1st Qu.:0.07116  \n Median : 800.0   Median :0.5728   Median :0.2403   Median :0.11661  \n Mean   : 860.9   Mean   :0.5436   Mean   :0.2476   Mean   :0.14121  \n 3rd Qu.:1010.0   3rd Qu.:0.7316   3rd Qu.:0.3206   3rd Qu.:0.18011  \n Max.   :3501.0   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n NA's   :395      NA's   :1035     NA's   :189      NA's   :167      \n    pctwhite         pctBlack        pctHispanic       pctAAPIalone    \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.2039   1st Qu.:0.01071   1st Qu.:0.02603   1st Qu.:0.00000  \n Median :0.5614   Median :0.06655   Median :0.09304   Median :0.00883  \n Mean   :0.5210   Mean   :0.18651   Mean   :0.22063   Mean   :0.03806  \n 3rd Qu.:0.8294   3rd Qu.:0.24998   3rd Qu.:0.32018   3rd Qu.:0.03532  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :0.91144  \n NA's   :131      NA's   :131       NA's   :131       NA's   :131      \n   pctunder18       pctover64         HSorlower        BAorhigher    \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.1908   1st Qu.:0.09436   1st Qu.:0.4150   1st Qu.:0.1120  \n Median :0.2300   Median :0.13604   Median :0.5182   Median :0.1680  \n Mean   :0.2295   Mean   :0.14340   Mean   :0.5067   Mean   :0.2035  \n 3rd Qu.:0.2719   3rd Qu.:0.18057   3rd Qu.:0.6113   3rd Qu.:0.2536  \n Max.   :0.6468   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n NA's   :131      NA's   :131       NA's   :132      NA's   :132     \n     Metro          Micro         NoCBSAType   \n Min.   :1      Min.   :1       Min.   :1      \n 1st Qu.:1      1st Qu.:1       1st Qu.:1      \n Median :1      Median :1       Median :1      \n Mean   :1      Mean   :1       Mean   :1      \n 3rd Qu.:1      3rd Qu.:1       3rd Qu.:1      \n Max.   :1      Max.   :1       Max.   :1      \n NA's   :9111   NA's   :37450   NA's   :37795  \n\n\n\n\n\n\n\n\nYour Turn!\n\n\n\nTry your hand at some of these summarization methods to see what they produce.\n\n\nSelective Summaries\nHow would we run summaries just for population, median household income, and poverty rate (think back to how we created subsets using lists)?\n\nsummary(ozs[, c(\"Population\", \"medhhincome\", \"PovertyRate\")])\n\n   Population     medhhincome      PovertyRate    \n Min.   :    0   Min.   :  2499   Min.   :0.0000  \n 1st Qu.: 2752   1st Qu.: 32014   1st Qu.:0.1381  \n Median : 3897   Median : 41094   Median :0.2055  \n Mean   : 4147   Mean   : 42153   Mean   :0.2331  \n 3rd Qu.: 5224   3rd Qu.: 50833   3rd Qu.:0.2996  \n Max.   :40616   Max.   :181406   Max.   :1.0000  \n NA's   :112     NA's   :249      NA's   :141     \n\n\n\n\nYour Turn!\nSolution\n\n\n\nPractice your querying skills - how would we return only those records for census tracts with a median household income above $100,000 per year?\n\n\n\nozs[ozs$medhhincome&gt;=100000,]\n\n# A tibble: 399 × 27\n   geoid       state   DesignatedOZ county    Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 2 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 3 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 4 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 5 04013815600 Arizona           NA Maricopa… Non-…        10      NA       5762\n 6 04013816800 Arizona           NA Maricopa… Non-…        10       1       3727\n 7 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 8 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 9 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n10 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n# ℹ 389 more rows\n# ℹ 19 more variables: medhhincome &lt;dbl&gt;, PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;,\n#   medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;,\n#   vacancyrate &lt;dbl&gt;, pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;,\n#   pctAAPIalone &lt;dbl&gt;, pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;,\n#   BAorhigher &lt;dbl&gt;, Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\n\n\n\nTracts in Illinois\nOftentimes, we’ll want to query out a subset of observations based upon their geographic location. Let’s try selecting all tracts in Illinois based upon their designation status.\n\n\nYour Turn!\nSolution\n\n\n\nHow would we query out tracts in Illinois? Experiment in your own script.\n\n\n\nozs[ozs$state == \"Illinois\",]\n\n# A tibble: 1,682 × 27\n   geoid       state    DesignatedOZ county   Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 17001000201 Illinois           NA Adams C… Non-…         7      NA       1937\n 2 17001000202 Illinois           NA Adams C… Low-…         1      NA       2563\n 3 17001000400 Illinois           NA Adams C… Low-…         1      NA       3403\n 4 17001000500 Illinois           NA Adams C… Low-…         1      NA       2298\n 5 17001000700 Illinois           NA Adams C… Low-…         1      NA       1259\n 6 17001000800 Illinois            1 Adams C… Low-…         1      NA       2700\n 7 17001000900 Illinois           NA Adams C… Low-…         5      NA       2671\n 8 17001010100 Illinois           NA Adams C… Non-…         2      NA       4323\n 9 17001010200 Illinois           NA Adams C… Low-…         2      NA       3436\n10 17001010300 Illinois           NA Adams C… Non-…         8      NA       6038\n# ℹ 1,672 more rows\n# ℹ 19 more variables: medhhincome &lt;dbl&gt;, PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;,\n#   medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;,\n#   vacancyrate &lt;dbl&gt;, pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;,\n#   pctAAPIalone &lt;dbl&gt;, pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;,\n#   BAorhigher &lt;dbl&gt;, Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\n\n\n\nWe can see in our table output that there are 1,682 eligible or designated tracts in Illinois. We could also use the nrow() command to count the number of rows.\n\n\nYour Turn!\nSolution\n\n\n\nTry crafting code that would count the number of rows for Illinois\n\n\n\nnrow(ozs[ozs$state == \"Illinois\",])\n\n[1] 1682\n\n\n\n\n\nGrouped Means\nWe might also want to calculate statistics like averages for subsets. mean() will calculate the mean of a list or column. What’s the average income for tracts with a vacancy rate above 20 percent?\n\n\nYour Turn\nSolution\n\n\n\nWhat’s the average income for tracts with a vacancy rate below 20 percent?\n\n\n\nmean(ozs$medhhincome[ozs$vacancyrate &gt; .2], na.rm=TRUE)\n\n[1] 35375.68\n\nmean(ozs$medhhincome[ozs$vacancyrate &lt; .2], na.rm=TRUE)\n\n[1] 43849.1\n\n\n\n\n\n\n\nYou might need to check out the documentation for mean() in order to return an answer here. R will not calculate the mean if an NA values are present in the vector for which you’ve requested the mean - a good safety feature if you’re expecting all values to be present.\nIn imperfect data like what we’re dealing with, you can instruct R to remove those NAs and find the mean for remaining values. You may also want to make sure you’ve counted the number of NA values so you know what proportion of your data the mean is actually representing.\nSubsetting\nWe might also be interested in combining query criteria. R can make use of logical statements. & is equivalent to AND and | is equivalent to OR. Now give it a go!\n\n\nYour Turn\nSolution\n\n\n\nWhat is the average income for tracts in Illinois with a poverty rate of greater than 20 percent.\n\n\n\nmean(ozs$medhhincome[ozs$state == \"Illinois\" & ozs$PovertyRate &gt; .2], na.rm=TRUE)\n\n[1] 33526\n\n\n\n\n\nTo confirm we got the query correct, it may be useful to have a look at the returned data without calculating the mean:\n\nozs$medhhincome[ozs$state == \"Illinois\" & ozs$PovertyRate &gt; .2]\n\n  [1] 28819 32313 17850 26012 40475 35387 40714 22326 21500 49590 40599 37679\n [13] 26676  7004    NA  7273  5736 38056 38083 33873 49597 22813 30994 49303\n [25] 48125 46364  7234 22688 44800 35904 21107 37823 36164 44460 30406 30682\n [37] 22945 15020 38922 39000 29861 38861 29432 26750 29870 17569 36464 51458\n [49] 64073 39349 40913 45349 51840 48450 40272 36848 39096 35463 43681 29091\n [61] 27931 34671 35286 27018 36412 22647 23700 42316 38571 31696 43895    NA\n [73] 42077 36159 42280 45417 51250 52074 47652 31364 42762 42232 36522 44750\n [85] 46905 39926 37284 36607 36644 48316 40662 38693 49236 47000 44393 43912\n [97] 55741 34375 43607 34340 45208 47344 52063 28558 52833 43717 56250 30503\n[109] 32317 34933 36338 32232 26773 27788 51542 35698 40515 43650 69348 52096\n[121] 28487 37227 31004 33873 31160 35625 34583 23516 43011 28631 28363 22708\n[133] 34030 28311 20405 20620 22289 23264 33508 31029 24914 12036 25921 29306\n[145] 22097 28867 21607 23214 20912 20563 30817 16989 23929 22150 32717 31181\n[157] 17652 21250 20278 20565 58047 45625 71250 61389 23834 21516 25291 27019\n[169] 36250 38054 40511 29120 29446 25099 26234 26375 31216 30205 37013 35764\n[181] 33088 29914 45096 48083 29980 40719 34922 31875 33173 20990 11964 41645\n[193] 11250 42324 11310 10942 23603 24500 19747 18063 21250 26000 34327    NA\n[205]    NA 32500 31602 18808 24178 20250 31938 25346 18859 21250 51875 35636\n[217] 38260 52817 82667 19034 37610 29960 14500 22353 17731 20573 29688 21658\n[229] 33849 20889 23606 15723 18125 19872 28214 19688 42292 32500 21362 22407\n[241] 21727 22306 22619 33125 28984 22371 33281 31008 27353 29125 20944 43341\n[253] 31518 20118 13536 23684 36938 41917 44514 51723 38750 36631 32430 35469\n[265] 43083 26641 39276 26630 26443 39348 33304 37679 32277 34154 33750 35240\n[277] 38857 24604 27073 42684 39500 41625 35089 25956 14556 14273 52485 44706\n[289] 40199 39722 40743 40254 41081 38452 31688 49358 43698 32682 32377 31982\n[301] 33631 21678 20761 22969 26727 26289 32949 27622 31250 26985 27880 25609\n[313] 25855 23906 45000 32331 33509 40054 52879 40901 22422 32083 39942 27712\n[325] 27156 35338 25846 31329 45313 35647 24408 22381 25786 27589 20119 24911\n[337] 22000 34583 27266 19205 33947 26094 25133 24427 36042 42670 22844 27841\n[349] 15485 15139 16862 22163 22972 35476 18875 26070 29844 34609 20431 28472\n[361] 36818 25461 22250 14622 26180 24806 32961 27864 24045 22250 19561 31330\n[373] 39005 32788 35573 35991 31867 51681 47656 50913 40125 43872 57692 61991\n[385] 50726 49116 49518 48011 65357 55523 56975 41051 51699 61806 49268 30532\n[397] 46988 62132 35500 44678 32027 40268 43021 44778 34835 35810 39139 40308\n[409] 40201 38163 36250 47943 46172 39212 40083 39965 28508 22261 45901 44083\n[421] 37941 40099 37885 39534 39766 46728 41196 49572 46739 27861 37374 32196\n[433] 37334 44048 36693 37500 41449 44653 31683 41405 30769 31474 37281 17425\n[445] 22857 23048 22650 41250 20500 17633 23761 30487 36613 37659 43529 39593\n[457] 27214 14722 22242 44722 22956 31188 35577 33220 44592 46533 99375 21456\n[469] 41902 42786 40286 35179 32500 34231 58500 50521 51442 23750 37179 31238\n[481] 30161 38883 32204 20898 26071 21827 19054 26514 37723 19207 23667    NA\n[493] 55813 18627 52717 25699 52308 30946 28806 43203 26319 13429 26023 27283\n[505] 33250 38750 56908 17377 17370 38382 43516 13281 20294 19228 84780 29831\n[517] 22279 42200 33942 45694 37891 31389 30759 50417 27185 32315 43409 25694\n[529] 19000 22446 18582 32895 35528 39652  9485 35041 21053 17197 28000 42254\n[541] 37353 24886 35875 31788 38060 27012 43780 74038 23302 16844 15242 54444\n[553] 52368 38551 39306 34028 34375 41646 37772 39049 43156 36469 24922 28448\n[565] 31907 38279 30656 35469 54293 43860 31845 42463 34167 46731 24886 29831\n[577] 37917 10127 13101 15671 22108 33514 18411 28791 15000 47733 54483 44872\n[589] 35329 40491 47273 36543 47908 49904 44732 49091 47981 38281 46639 49101\n[601] 36830 26627 30705 26406 24423 21488 42832 23569 36806 41053 17938 22813\n[613] 33579 23011 34970 38380 36824 60630 38188 36127 48339 22500 29408 37883\n[625] 51121 22312 30547 34063 32134 52981 37361 45333 46020 32725 26861 45385\n[637] 38625 35123 33250 41824 21739    NA 37443 33056 47813 55861 11053 37266\n[649] 62966 37356 26224 40188 42537 51354 29479 26250 31250 40556 21613 26103\n[661] 26667 14434 17577 28866 32083 48706 35191 39013 41739 23222 26432 25377\n[673] 32083 55313 16010 45078 34382 22121 21701 41958 37261 44291 36618 40408\n[685] 39514 20000 39792 30735 74554 36917 25781 37875 44094 29304 28125 24932\n[697] 35179 32198 33144 31446 38144 30464 34314 46034 12474 29773 22121 21582\n[709] 26591 12247 16337 18815 27599 27225 29762 44844 67000 31653 35291 41462\n[721] 45929 20942 31635 42583 32396 28884 36902 24671 40298 38750 29677 36979\n[733] 41037 32434 42917 42344 20741 36960 21969 25463  9836 17120 15394 28065\n[745] 21849 24095 29875 41964 37750 28077 37357 34449 19650 27019 29250 33240\n[757] 25134 29779 24486  9967 15036 27005 16353 20439 35398 28750 63750 36333\n[769] 32188 38750 41886 30930 25994 38480 28294 20500 16667 31716 16750 16513\n[781] 17500 24821 41966 31002 23214 32956 54387 32697 32260 52788 52768 25078\n[793] 23371 29978 40506 19145 27165 32034 23514 43007 31639 33981 40532 39006\n[805] 34708 37185 31250 37414 51767 31772 34732 31674 37386 27448 19518 22899\n[817] 50052 40212 37792 40676 45962 35028 56190 38083 36794 30932 46587 27457\n[829] 18983 14731 23594 34420 36053 27566 28004 14853 40737 20000 27397 19167\n[841] 25791 26979 12816 21091 21330 27432 31477 34939 44792 35660    NA    NA\n[853]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n[865]    NA    NA    NA    NA    NA    NA    NA    NA    NA\n\n\nDealing with Missing Values\nYou’ll note that there are several flag variables in the data for which values are either 1 or NA. We have flags for whether a tract was designated as an Opportunity Zone, Whether it is located in a Metropolitan, Micropolitan, or Non-Core-Based Statistical Area.\nWe can use logical tests in R to identify those values that are NA. We can use is.na() to test whether a value is NA (TRUE) or is not NA (FALSE). We could also use the negation sign ! to determine whether a value is not NA (!is.na()).\nThe following code returns a vector of logical values (TRUE / FALSE) regarding whether the value for the Designated column is NA or not.\n\nis.na(ozs$Designated)\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\nlogical(0)\n\n\nFor logical values, R codes 1 as TRUE and 0 as false, meaning if we wanted to count the number of undesginated tracts, we could ask for the sum of the values for which the logical test is true (the sum of the values that are NA):\n\nsum(is.na(ozs$Designated))\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] 0\n\n\n33,414 tracts were not designated.\n\n\nYour Turn\nSolution\n\n\n\nNow count the number of tracts that were designated (where the value is not NA).\n\n\n\nsum(!is.na(ozs$Designated))\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] 0\n\n\n\n\n\nWe might also want to recode those NA values to something else. We can use assignment and subset notation to replace na values with something else. Let’s replace those NAs in the Designated column with 0.\n\nozs$DesignatedOZ[is.na(ozs$DesignatedOZ)]&lt;-0\nozs\n\n# A tibble: 42,178 × 27\n   geoid       state   DesignatedOZ county    Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 01001020200 Alabama            0 Autauga … Low-…         4      NA       2196\n 2 01001020300 Alabama            0 Autauga … Non-…         6      NA       3136\n 3 01001020700 Alabama            1 Autauga … Low-…         8      NA       3047\n 4 01001020802 Alabama            0 Autauga … Non-…        10      NA      10743\n 5 01001021000 Alabama            0 Autauga … Non-…         5      NA       2899\n 6 01001021100 Alabama            0 Autauga … Low-…         6      NA       3247\n 7 01003010100 Alabama            0 Baldwin … Non-…         6      NA       4013\n 8 01003010200 Alabama            1 Baldwin … Low-…         9      NA       3067\n 9 01003010300 Alabama            0 Baldwin … Non-…        10      NA       8079\n10 01003010400 Alabama            1 Baldwin … Non-…         9      NA       4578\n# ℹ 42,168 more rows\n# ℹ 19 more variables: medhhincome &lt;dbl&gt;, PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;,\n#   medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;,\n#   vacancyrate &lt;dbl&gt;, pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;,\n#   pctAAPIalone &lt;dbl&gt;, pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;,\n#   BAorhigher &lt;dbl&gt;, Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\nCan you inspect the table here to see what happened? In plain language, we told R “for those values of the column named Designated in the ozs data table where the values are NA, assign a new value of 0.”\n\n\nYour Turn\nSolution\n\n\n\nGo ahead and do the same thing for the Metro, Micro, and NoCBSAType columns.\n\n\n\nozs$Metro[is.na(ozs$Metro)]&lt;-0\nozs$Micro[is.na(ozs$Micro)]&lt;-0\nozs$NoCBSAType[is.na(ozs$NoCBSAType)]&lt;-0"
  },
  {
    "objectID": "howto/rbasics_02.html#independent-exploration",
    "href": "howto/rbasics_02.html#independent-exploration",
    "title": "Lesson 2: Manipulating Data Frames",
    "section": "Independent Exploration:",
    "text": "Independent Exploration:\nNow answer the following questions:\n\n\nYour Turn\nSolution\n\n\n\nReport average poverty rates for designated opportunity zones in metropolitan, micropolitan, and non-CBSA areas\n\n\n\n# Your Work Here\nmean(ozs$PovertyRate[ozs$Designated ==1 & ozs$Metro == 1], na.rm=TRUE)\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] NaN\n\nmean(ozs$PovertyRate[ozs$Designated ==1 & ozs$Micro == 1], na.rm=TRUE)\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] NaN\n\nmean(ozs$PovertyRate[ozs$Designated ==1 & ozs$NoCBSAType == 1], na.rm=TRUE)\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] NaN\n\n\n\n\n\n\n\nYour Turn\nSolution\n\n\n\nFor Illinois, how different are the average vacancy rates for designated and undesignated census tracts?\n\n\n\nmean(ozs$vacancyrate[ozs$state == \"Illinois\" & ozs$Designated == 1])\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] NaN\n\nmean(ozs$vacancyrate[ozs$state == \"Illinois\" & ozs$Designated == 0], na.rm=TRUE)\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] NaN\n\n\n\n\n\nThink of another question you’d like to ask of these data - write it down and work the problem out."
  },
  {
    "objectID": "howto/setupr.html",
    "href": "howto/setupr.html",
    "title": "Setting up R and RStudio",
    "section": "",
    "text": "R can be downloaded from the Comprehensive R Archive Network (CRAN), a network of mirrored servers throughout the world that host the R software as well as user-developed packages. Visit https://cloud.r-project.org which will automatically direct you to the best download server.\nOnce here, you can then download and install R using the version that matches your operating system:\n\nGo ahead and download R and install following the installer defaults. After R is downloaded and installed, run R to confirm. You should see a single window that looks something like this:\nThat’s it for R. This should be the first and last time you’ll need to open the R console directly. We typically will not access R directly, but instead will use RStudio as our primary interface. Go ahead and close the R console and proceed by downloading and installing RStudio, which is the IDE we will use to interface and write code with R."
  },
  {
    "objectID": "howto/setupr.html#download-r",
    "href": "howto/setupr.html#download-r",
    "title": "Setting up R and RStudio",
    "section": "",
    "text": "R can be downloaded from the Comprehensive R Archive Network (CRAN), a network of mirrored servers throughout the world that host the R software as well as user-developed packages. Visit https://cloud.r-project.org which will automatically direct you to the best download server.\nOnce here, you can then download and install R using the version that matches your operating system:\n\nGo ahead and download R and install following the installer defaults. After R is downloaded and installed, run R to confirm. You should see a single window that looks something like this:\nThat’s it for R. This should be the first and last time you’ll need to open the R console directly. We typically will not access R directly, but instead will use RStudio as our primary interface. Go ahead and close the R console and proceed by downloading and installing RStudio, which is the IDE we will use to interface and write code with R."
  },
  {
    "objectID": "howto/setupr.html#downlad-rstudio",
    "href": "howto/setupr.html#downlad-rstudio",
    "title": "Setting up R and RStudio",
    "section": "Downlad RStudio",
    "text": "Downlad RStudio\nNext up, let’s download RStudio. Go to https://rstudio.com/products/rstudio/download. Click on “Download” under RStudio Desktop. The website will suggest the most appropriate current version of the software based upon the computer you are loading RStudio on. You may also choose from an alternate installer if you believe it is more appropriate for your operating system.\nDownload and install RStudio, again using the default installation settings.\nOnce you have RStudio installed, open RStudio. You should see something that looks like this (the information in your console window will likely describe a newer version of R than what is displayed here).\n\nYou’ll note that what appears in the portion of the console to the left looks very similar to the R console window which you opened before you started your install of RStudio. This console functions in exactly the same way, however, you’ll also note that there are other areas which you have access to as well."
  },
  {
    "objectID": "howto/setupr.html#rtools-error-pc-users",
    "href": "howto/setupr.html#rtools-error-pc-users",
    "title": "Setting up R and RStudio",
    "section": "Rtools error (PC users)",
    "text": "Rtools error (PC users)\nSome PC users may encounter an error message regarding Rtools not being installed. This would come up with you first try to install a package such as tidyverse either using R’s package manager or using the install.packages() command. To remedy this error if it occurs, PC users need to close R, manually download a patch, install it, and restart your computer. This error should then be remedied.\nIf you are a PC user who encounters this error, please do the following: 1. Save any work and close your RStudio session 2. Download Rtools (download link) + the link above contains instructions for putting Rtools on the PATH - you should not need to do this. 3. Run the downloaded executable file to install Rtools 4. Restart your computer 5. Re-open RStudio - the problem should be resolved"
  },
  {
    "objectID": "howto/setupr.html#run-r-in-the-cloud",
    "href": "howto/setupr.html#run-r-in-the-cloud",
    "title": "Setting up R and RStudio",
    "section": "Run R in the Cloud",
    "text": "Run R in the Cloud\nUIUC AnyWare provides cloud-based options for running software including RStudio."
  },
  {
    "objectID": "howto/setupr.html#the-rstudio-interface",
    "href": "howto/setupr.html#the-rstudio-interface",
    "title": "Setting up R and RStudio",
    "section": "The RStudio Interface",
    "text": "The RStudio Interface\n\nThe R Console is the place where code you write is executed. Typically we’ll write code in a script or R Notebook (more on those later) and active pieces of code will them be executed in the R console. You can also type code directly into the console and execute it by hitting the return or enter key.\nThe Environment Window provides information on variables, data tables, and other objects you create and define as you work.\nThe Auxiliary Window provides a range of information, and includes a file browser, plot visualization window, and access to help documentation.\n\nWith those basics in mind, you can start to explore the basic functionality of R."
  },
  {
    "objectID": "resources/data.html",
    "href": "resources/data.html",
    "title": "Data Analysis",
    "section": "",
    "text": "covdata R Package\nData Science Central Big data sets available for free\nEconomic Innovation Group Distressed Communities Index\nEviction Lab data on residential evictions summarized to various administrative geographies\nGlobal Human Settlements Dataset\nHUD Location Affordability Index\nHUD Urbanization Perceptions Small Area Index\nHUD USPS Vacancy Data\nLongitudinal Tract Database (LTDB): 1970 - 2010 Census data normalized to 2010 census tracts\nMIT Senseable Cities Lab Treepedia\nNational Housing Preservation Database (NHPD)\nPolicy Surveillance Program Eviction Laws database\nUSDA Rural-Urban Commuting Area Codes\nStanford Open Policing Project\nSURGO Foundation U.S. COVID Community Vulnerability Index (CCVI)"
  },
  {
    "objectID": "resources/inspiration.html",
    "href": "resources/inspiration.html",
    "title": "Inspiration",
    "section": "",
    "text": "BBC: How the BBC Visual and Data Journalism Team Works with Graphics in R\nBloomberg: Visualizing U.S. Immigration History as Tree Rings\nCraig Dalton and Timm Stallmann: Counter-mapping data science\nGapminder (check out in particular their temporal animations)\nGeoDa Center U.S. COVID Atlas\nR at the ACLU: Joining Tables to Reunite Families\nLondon: The Information Capital: Stunning book featuring 100 visualizations about the city of London. Many visualizations were created using R.\nMIT: Atlas of Inequality\nNew York City Neighborhood Planning Playbook\nR Graph Gallery\nWashington Post: America is more diverse than ever - but still segregated\nW.E.B. Du Bois’s Data Portraits: Visualizing Black America and related background from Smithsonian Magazine"
  },
  {
    "objectID": "schedule/01_introduction.html",
    "href": "schedule/01_introduction.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Welcome to UP 570: Neighborhood Analysis! This session serves as our introduction to the class and an opportunity to start exploring how we will work together over the course of the semester. We will introduce ourselves and discuss how we will learn together. We’ll also think through two foundational questions:\n\nWhat is a neighborhood?\nWhy do neighborhoods matter?"
  },
  {
    "objectID": "schedule/01_introduction.html#session-description",
    "href": "schedule/01_introduction.html#session-description",
    "title": "Course Introduction",
    "section": "",
    "text": "Welcome to UP 570: Neighborhood Analysis! This session serves as our introduction to the class and an opportunity to start exploring how we will work together over the course of the semester. We will introduce ourselves and discuss how we will learn together. We’ll also think through two foundational questions:\n\nWhat is a neighborhood?\nWhy do neighborhoods matter?"
  },
  {
    "objectID": "schedule/01_introduction.html#before-class",
    "href": "schedule/01_introduction.html#before-class",
    "title": "Course Introduction",
    "section": "Before Class",
    "text": "Before Class\n\nWatch this video which provides an overview of our course website and resources:\n\n\n\nBookmark this website https://up570s24.netlify.app so you can access it easily.\nIntroduce yourself on our course Github Discussion Forum\nRead the course syllabus and bring questions with you.\nComplete the course introductory survey."
  },
  {
    "objectID": "schedule/01_introduction.html#reflect",
    "href": "schedule/01_introduction.html#reflect",
    "title": "Course Introduction",
    "section": "Reflect",
    "text": "Reflect\n\nWhat are your goals for taking this class? What would you like to learn about neighborhoods?\nWhat matters about neighborhoods? How have neighborhoods shaped your life?\nWhat types of stories do we tend to tell about neighborhoods? How do these stories contextualize how neighborhoods “fit” within cities and their regions?"
  },
  {
    "objectID": "schedule/01_introduction.html#slides",
    "href": "schedule/01_introduction.html#slides",
    "title": "Course Introduction",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/01_introduction.html#resources-for-further-exploration",
    "href": "schedule/01_introduction.html#resources-for-further-exploration",
    "title": "Course Introduction",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\n\nJoin the Data Science in Planning (DSIP) Listserv (dsip-durp-l@lists.illinois.edu) where members of our community can share opportunities things of interest.\n\nChoose the address with which you want to subscribe to the list - you should choose an address you can check frequently.\nSend a message to sympa@lists.illinois.edu from the address you want to subscribe to the list.\nIn the subject line of your message, type in subscribe dsip-durp-l Firstname Name (replace Firstname and Name with your preferred first name and last name).\nLeave the message body blank.\nAfter this you will recieve a message telling you whether your request was accepted or not.\n\nComprehensive Development Plan for Champaign - Urbana (1950)"
  },
  {
    "objectID": "schedule/03_datapipeline.html",
    "href": "schedule/03_datapipeline.html",
    "title": "Building a Data Pipeline",
    "section": "",
    "text": "In this session, we’ll explore some of the basic workflow which we’ll use over the course of the semester to package and share analysis. We’ll develop familiarity with Quarto, and basic operations in Github so that you are able to share code and analysis over the course of the semester.\nLab 1 Link"
  },
  {
    "objectID": "schedule/03_datapipeline.html#session-description",
    "href": "schedule/03_datapipeline.html#session-description",
    "title": "Building a Data Pipeline",
    "section": "",
    "text": "In this session, we’ll explore some of the basic workflow which we’ll use over the course of the semester to package and share analysis. We’ll develop familiarity with Quarto, and basic operations in Github so that you are able to share code and analysis over the course of the semester.\nLab 1 Link"
  },
  {
    "objectID": "schedule/03_datapipeline.html#before-class",
    "href": "schedule/03_datapipeline.html#before-class",
    "title": "Building a Data Pipeline",
    "section": "Before Class",
    "text": "Before Class\nReview today’s lab guide.\nEnsure that your computer has the latest stable versions of R and RStudio installed.\nAccept the GitHub invitation to our Lab 1 repository and download the repository to your local computer (we will set up more advanced tools for interacting with GitHub in our next lab session."
  },
  {
    "objectID": "schedule/03_datapipeline.html#reflect",
    "href": "schedule/03_datapipeline.html#reflect",
    "title": "Building a Data Pipeline",
    "section": "Reflect",
    "text": "Reflect\n\nWorkflows\n\nWhat are the types of common tasks in your workflows that you think would benefit from a data pipeline?\nHow do we hold ourselves accountable for our analysis?\n\n\n\nReadings\n\nWhose interests and goals do you seek to represent through your work?\nWhat missing datasets (akin to the Library of Missing Datasets) have you observed?1"
  },
  {
    "objectID": "schedule/03_datapipeline.html#slides",
    "href": "schedule/03_datapipeline.html#slides",
    "title": "Building a Data Pipeline",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/03_datapipeline.html#resources-for-further-exploration",
    "href": "schedule/03_datapipeline.html#resources-for-further-exploration",
    "title": "Building a Data Pipeline",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/03_datapipeline.html#footnotes",
    "href": "schedule/03_datapipeline.html#footnotes",
    "title": "Building a Data Pipeline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAt the beginning of our session, we’ll catalog some of these datasets - it may help to write down some of your thoughts to share.↩︎"
  },
  {
    "objectID": "schedule/05_tidydata.html",
    "href": "schedule/05_tidydata.html",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "",
    "text": "This week, we’ll spend time reviewing the basics of what tidy data are, and will review common manipulation strategies. We will continue the work that we began on Tuesday and will take a few moments to reflect upon how these principles help us with some of our foundational approaches to information in this class."
  },
  {
    "objectID": "schedule/05_tidydata.html#session-description",
    "href": "schedule/05_tidydata.html#session-description",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "",
    "text": "This week, we’ll spend time reviewing the basics of what tidy data are, and will review common manipulation strategies. We will continue the work that we began on Tuesday and will take a few moments to reflect upon how these principles help us with some of our foundational approaches to information in this class."
  },
  {
    "objectID": "schedule/05_tidydata.html#before-class",
    "href": "schedule/05_tidydata.html#before-class",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "Before Class",
    "text": "Before Class\nReview the How To Lesson 3."
  },
  {
    "objectID": "schedule/05_tidydata.html#reflect",
    "href": "schedule/05_tidydata.html#reflect",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/05_tidydata.html#slides",
    "href": "schedule/05_tidydata.html#slides",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/05_tidydata.html#resources-for-further-exploration",
    "href": "schedule/05_tidydata.html#resources-for-further-exploration",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/07_communicating.html",
    "href": "schedule/07_communicating.html",
    "title": "Communicating Complex Information",
    "section": "",
    "text": "In this lab session, you’ll do some initial work on analyzing and communicating “real world” data drawing from your prior knowledge of R and from our initial work on building data pipelines and communication streams. This session marks out transition from setting up some basic workflows to focusing on data analysis techniques and applications. This lab is designed to help us learn more about your familiarity and proficiency with basic data manipulation using common dplyr functions.\nYour goal is to use our lab session plus an additional 3-4 hours to work through lab prompts. Address the prompts if you can. If you can’t, provide written descriptions about what you’re trying to do, pointers about how you’ve tried to address the problems, and insights into where you’re getting stuck. To repeat, the overall goal isn’t to complete the lab, but rather to share your process and insights. This will help us to refine future labs and instructions based upon the collective knowledge and understanding of the class."
  },
  {
    "objectID": "schedule/07_communicating.html#session-description",
    "href": "schedule/07_communicating.html#session-description",
    "title": "Communicating Complex Information",
    "section": "",
    "text": "In this lab session, you’ll do some initial work on analyzing and communicating “real world” data drawing from your prior knowledge of R and from our initial work on building data pipelines and communication streams. This session marks out transition from setting up some basic workflows to focusing on data analysis techniques and applications. This lab is designed to help us learn more about your familiarity and proficiency with basic data manipulation using common dplyr functions.\nYour goal is to use our lab session plus an additional 3-4 hours to work through lab prompts. Address the prompts if you can. If you can’t, provide written descriptions about what you’re trying to do, pointers about how you’ve tried to address the problems, and insights into where you’re getting stuck. To repeat, the overall goal isn’t to complete the lab, but rather to share your process and insights. This will help us to refine future labs and instructions based upon the collective knowledge and understanding of the class."
  },
  {
    "objectID": "schedule/07_communicating.html#before-class",
    "href": "schedule/07_communicating.html#before-class",
    "title": "Communicating Complex Information",
    "section": "Before Class",
    "text": "Before Class\n\nTake a look at these instructions.\nAccept the lab repository, link to Github, and create a local version of the repository.\nCome to class with any initial questions you have about the Learner’s Permit of what you’re being asked."
  },
  {
    "objectID": "schedule/07_communicating.html#reflect",
    "href": "schedule/07_communicating.html#reflect",
    "title": "Communicating Complex Information",
    "section": "Reflect",
    "text": "Reflect\n\nHow do you typically approach exploring unfamiliar data? What types of questions help you find a direction?\nWhat kinds of stories might data on code violations help us tell?\nWhat types of information are missing from these datasets? What questions come up as you complete your labs?"
  },
  {
    "objectID": "schedule/07_communicating.html#resources-for-further-exploration",
    "href": "schedule/07_communicating.html#resources-for-further-exploration",
    "title": "Communicating Complex Information",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nNew York City Code Violations \nNew York City PLUTO Data"
  },
  {
    "objectID": "schedule/09_places.html",
    "href": "schedule/09_places.html",
    "title": "Describing Places",
    "section": "",
    "text": "Today’s lab session focuses on the basic description of places. In your prior labs, you have developed basic workflows for communicating using principles of reproducible data analysis. Today’s lab asks you to apply those skills to basic description of place characteristics.\nTo add depth to our example, we will explore not only basic place descriptions, but will also think about how these descriptions might change in relation to an applied policy problem."
  },
  {
    "objectID": "schedule/09_places.html#session-description",
    "href": "schedule/09_places.html#session-description",
    "title": "Describing Places",
    "section": "",
    "text": "Today’s lab session focuses on the basic description of places. In your prior labs, you have developed basic workflows for communicating using principles of reproducible data analysis. Today’s lab asks you to apply those skills to basic description of place characteristics.\nTo add depth to our example, we will explore not only basic place descriptions, but will also think about how these descriptions might change in relation to an applied policy problem."
  },
  {
    "objectID": "schedule/09_places.html#before-class",
    "href": "schedule/09_places.html#before-class",
    "title": "Describing Places",
    "section": "Before Class",
    "text": "Before Class\nAccept the GitHub Classroom invitation to our lab repository and use RStudio to pull the repository to your local computer.\nRead our lab background so you are prepared to start working through the lab repository notebook."
  },
  {
    "objectID": "schedule/09_places.html#reflect",
    "href": "schedule/09_places.html#reflect",
    "title": "Describing Places",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/09_places.html#resources-for-further-exploration",
    "href": "schedule/09_places.html#resources-for-further-exploration",
    "title": "Describing Places",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/11_census.html",
    "href": "schedule/11_census.html",
    "title": "Population and the Census",
    "section": "",
    "text": "In this session, we will work on our lab focused on exploring population and the census."
  },
  {
    "objectID": "schedule/11_census.html#session-description",
    "href": "schedule/11_census.html#session-description",
    "title": "Population and the Census",
    "section": "",
    "text": "In this session, we will work on our lab focused on exploring population and the census."
  },
  {
    "objectID": "schedule/11_census.html#before-class",
    "href": "schedule/11_census.html#before-class",
    "title": "Population and the Census",
    "section": "Before Class",
    "text": "Before Class\nAccept the GitHub Classroom invitation to our lab repository and use RStudio to pull the repository to your local computer.\nRead our lab background so you are prepared to start working through the lab repository notebook.\nIf you do not already have one, please register for a U.S. Census Bureau API Key which we will need for the tidycensus package."
  },
  {
    "objectID": "schedule/11_census.html#reflect",
    "href": "schedule/11_census.html#reflect",
    "title": "Population and the Census",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/11_census.html#slides",
    "href": "schedule/11_census.html#slides",
    "title": "Population and the Census",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/11_census.html#resources-for-further-exploration",
    "href": "schedule/11_census.html#resources-for-further-exploration",
    "title": "Population and the Census",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/13_segregation.html",
    "href": "schedule/13_segregation.html",
    "title": "Segregation",
    "section": "",
    "text": "In this session, we will begin to work on a lab focused on measures of residential segregation. We will also spend some time workshopping your approach to the Population Memo.\nWe’ll take around 20 minutes at the beginning of class for you to share with each other what you’re thinking and your approach.\nWe’ll then engage any questions you have about this week’s lab or last week’s lab."
  },
  {
    "objectID": "schedule/13_segregation.html#session-description",
    "href": "schedule/13_segregation.html#session-description",
    "title": "Segregation",
    "section": "",
    "text": "In this session, we will begin to work on a lab focused on measures of residential segregation. We will also spend some time workshopping your approach to the Population Memo.\nWe’ll take around 20 minutes at the beginning of class for you to share with each other what you’re thinking and your approach.\nWe’ll then engage any questions you have about this week’s lab or last week’s lab."
  },
  {
    "objectID": "schedule/13_segregation.html#before-class",
    "href": "schedule/13_segregation.html#before-class",
    "title": "Segregation",
    "section": "Before Class",
    "text": "Before Class\nGitHub Classroom Link\nPlease come ready to share your approach to the population memo in small groups. Please bring in examples of analysis or visualization that we can look at, either in small groups or collectively."
  },
  {
    "objectID": "schedule/13_segregation.html#reflect",
    "href": "schedule/13_segregation.html#reflect",
    "title": "Segregation",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/13_segregation.html#code",
    "href": "schedule/13_segregation.html#code",
    "title": "Segregation",
    "section": "Code",
    "text": "Code\nIn class, we spent some time looking at code that would select tracts within census place geographies. Here’s that code for your reference.\nLet’s start by downloading tract and place geometries and selecting Chicago (as an example):\nFirst, download tracts, subset to Cook County, and plot to confirm:\n\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidyverse)\n\n\nil_trt &lt;- tracts(state = \"IL\") |&gt; \n  filter(COUNTYFP == \"031\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nggplot()+\n  geom_sf(data = il_trt) \n\n\n\n\nNext, download census place boundaries, filter to Chicago, and plot to confirm:\n\nil_place &lt;- places(state = \"IL\") |&gt; \n  filter(NAME == \"Chicago\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |======================================================================| 100%\n\nggplot()+\n  geom_sf(data = il_place, cex = 1) \n\n\n\n\nPlot together to see overlay:\n\nggplot()+\n  geom_sf(data = il_trt, cex=.1)+\n  geom_sf(data = il_place, color = \"blue\",cex = .5, fill = NA)\n\n\n\n\n\nOption 1: Spatial Join\nPerform a spatial join to identify all tracts that overlap with Chicago’s municipal boundaries, plot to confirm:\n\ntest &lt;- st_join(il_trt, il_place) |&gt; \n  filter(NAME.y == \"Chicago\")\n\nggplot() +\n  geom_sf(data = test) +\n  geom_sf(data = il_place, color = \"blue\", fill = NA)\n\n\n\n\n\n\nOption 2: Intersection\nAnother option is to find the spatial intersection of the two objects:\n\ntest2 &lt;- st_intersection(il_trt, il_place)\n\nggplot() +\n  geom_sf(data = test2)+\n  geom_sf(data = il_place, color = \"blue\", fill = NA)\n\n\n\n\nAs we discussed in class, there may be good reasons for pursuing either method. Intersection may produce visually “cleaner” output, but we may need to spend more time explaining that there are portions of tract geometry that fall outside of the city boundary. We may even want to do some analysis to estimate where uncertainty may come from due to our strategy."
  },
  {
    "objectID": "schedule/13_segregation.html#slides",
    "href": "schedule/13_segregation.html#slides",
    "title": "Segregation",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/13_segregation.html#resources-for-further-exploration",
    "href": "schedule/13_segregation.html#resources-for-further-exploration",
    "title": "Segregation",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/15_neighborhood.html#before-class",
    "href": "schedule/15_neighborhood.html#before-class",
    "title": "Neighborhood Change",
    "section": "Before Class",
    "text": "Before Class\nGithub Classroom Link"
  },
  {
    "objectID": "schedule/15_neighborhood.html#reflect",
    "href": "schedule/15_neighborhood.html#reflect",
    "title": "Neighborhood Change",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/15_neighborhood.html#slides",
    "href": "schedule/15_neighborhood.html#slides",
    "title": "Neighborhood Change",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/15_neighborhood.html#resources-for-further-exploration",
    "href": "schedule/15_neighborhood.html#resources-for-further-exploration",
    "title": "Neighborhood Change",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/17_opportunity.html#before-class",
    "href": "schedule/17_opportunity.html#before-class",
    "title": "Place Opportunity",
    "section": "Before Class",
    "text": "Before Class\nGitHub Classroom Link ## Reflect"
  },
  {
    "objectID": "schedule/17_opportunity.html#slides",
    "href": "schedule/17_opportunity.html#slides",
    "title": "Place Opportunity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/17_opportunity.html#resources-for-further-exploration",
    "href": "schedule/17_opportunity.html#resources-for-further-exploration",
    "title": "Place Opportunity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/19_transequity.html#before-class",
    "href": "schedule/19_transequity.html#before-class",
    "title": "Transit Equity",
    "section": "Before Class",
    "text": "Before Class\nGithub Classroom Link"
  },
  {
    "objectID": "schedule/19_transequity.html#reflect",
    "href": "schedule/19_transequity.html#reflect",
    "title": "Transit Equity",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/19_transequity.html#slides",
    "href": "schedule/19_transequity.html#slides",
    "title": "Transit Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/19_transequity.html#resources-for-further-exploration",
    "href": "schedule/19_transequity.html#resources-for-further-exploration",
    "title": "Transit Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/21_healthequity.html",
    "href": "schedule/21_healthequity.html",
    "title": "Health Equity",
    "section": "",
    "text": "We will be joined for our class session today by Dr. Christine Joseph, of Henry Ford Health. Dr. Joseph does work that examines the social determinants of health and health disparities. She will share some of her perspective on why neighborhoods matter and form an important framework for analysis within her work."
  },
  {
    "objectID": "schedule/21_healthequity.html#session-description",
    "href": "schedule/21_healthequity.html#session-description",
    "title": "Health Equity",
    "section": "",
    "text": "We will be joined for our class session today by Dr. Christine Joseph, of Henry Ford Health. Dr. Joseph does work that examines the social determinants of health and health disparities. She will share some of her perspective on why neighborhoods matter and form an important framework for analysis within her work."
  },
  {
    "objectID": "schedule/21_healthequity.html#before-class",
    "href": "schedule/21_healthequity.html#before-class",
    "title": "Health Equity",
    "section": "Before Class",
    "text": "Before Class\nGithub Classroom Link ## Reflect\n\nWhat are some tropes or conventions we use to talk about individual health? Collective health?\nHow might health disparities (at a neighborhood or population level) be connected to the many themes we’ve talked about in this class?\nIs population health a planning issue? What role do practitioners and researchers in planning have for influencing how we think and act around health at the neighborhood level?"
  },
  {
    "objectID": "schedule/21_healthequity.html#slides",
    "href": "schedule/21_healthequity.html#slides",
    "title": "Health Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/21_healthequity.html#resources-for-further-exploration",
    "href": "schedule/21_healthequity.html#resources-for-further-exploration",
    "title": "Health Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/23_CheckIn.html",
    "href": "schedule/23_CheckIn.html",
    "title": "Final Project Check-In: Wednesday",
    "section": "",
    "text": "This week is devoted to collaboratively workshopping your final projects. During these sessions, we’ll be examining your best evidence - a fact or analysis that you believe best makes the point you’re trying to convey to your audience.\nCome to class prepared to share your best evidence with the entire class. You will each be asked to take a few minutes to present your best evidence and receive feedback on it."
  },
  {
    "objectID": "schedule/23_CheckIn.html#session-description",
    "href": "schedule/23_CheckIn.html#session-description",
    "title": "Final Project Check-In: Wednesday",
    "section": "",
    "text": "This week is devoted to collaboratively workshopping your final projects. During these sessions, we’ll be examining your best evidence - a fact or analysis that you believe best makes the point you’re trying to convey to your audience.\nCome to class prepared to share your best evidence with the entire class. You will each be asked to take a few minutes to present your best evidence and receive feedback on it."
  },
  {
    "objectID": "schedule/23_CheckIn.html#before-class",
    "href": "schedule/23_CheckIn.html#before-class",
    "title": "Final Project Check-In: Wednesday",
    "section": "Before Class",
    "text": "Before Class\nPrepare your best evidence for sharing. This might be preparing or exporting a bit of analysis or coming the class prepared to articulate and share this evidence.\nTo help expedite our workshopping of best evidence, please prepare a single slide containing your best evidence (PowerPoint, Keynote, or PDF are fine). I will compile these in advance of the class so that we can spend as much time as possible workshopping each of your arguments and evidence.\nPlease upload your best evidence slide to Box before 9am (Monday) morning. This slide does not need to be visually refined – it just needs your name and the evidence in whatever form you plan to share it (image, map, table, whatever)."
  },
  {
    "objectID": "schedule/23_CheckIn.html#reflect",
    "href": "schedule/23_CheckIn.html#reflect",
    "title": "Final Project Check-In: Wednesday",
    "section": "Reflect",
    "text": "Reflect\n\nHow does this evidence support the point you’re trying to make?\nHow do you plan to use the evidence in your argument?\nWhat critiques might others have regarding your evidence? How are you planning to preemptively engage these critiques in your analysis?"
  },
  {
    "objectID": "schedule/23_CheckIn.html#slides",
    "href": "schedule/23_CheckIn.html#slides",
    "title": "Final Project Check-In: Wednesday",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/23_CheckIn.html#resources-for-further-exploration",
    "href": "schedule/23_CheckIn.html#resources-for-further-exploration",
    "title": "Final Project Check-In: Wednesday",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/25_fieldobs.html",
    "href": "schedule/25_fieldobs.html",
    "title": "Field Observations",
    "section": "",
    "text": "During this session, we will focus on translating our group field observations into structured information - a memorandum that blends your direct observation and knowledge of the neighborhood developed through existing indicators."
  },
  {
    "objectID": "schedule/25_fieldobs.html#session-description",
    "href": "schedule/25_fieldobs.html#session-description",
    "title": "Field Observations",
    "section": "",
    "text": "During this session, we will focus on translating our group field observations into structured information - a memorandum that blends your direct observation and knowledge of the neighborhood developed through existing indicators."
  },
  {
    "objectID": "schedule/25_fieldobs.html#session-goals",
    "href": "schedule/25_fieldobs.html#session-goals",
    "title": "Field Observations",
    "section": "Session Goals",
    "text": "Session Goals\n\nCollaboratively build a story about the West Urbana neighborhood that leverages both your knowledge of existing indicators.\nReflect upon the tactical and design choices you have made in telling your story."
  },
  {
    "objectID": "schedule/25_fieldobs.html#before-class",
    "href": "schedule/25_fieldobs.html#before-class",
    "title": "Field Observations",
    "section": "Before Class",
    "text": "Before Class\n\nReview your notes and materials from your visit work on Monday.\n\nGithub Classroom Link"
  },
  {
    "objectID": "schedule/25_fieldobs.html#during-class",
    "href": "schedule/25_fieldobs.html#during-class",
    "title": "Field Observations",
    "section": "During Class",
    "text": "During Class\nWork collaboratively with your group members to develop a short memorandum describing the West Urbana neighborhood. Your memo should include demographic information coming from census data or other sources of secondary data (please start with the same selected census indicators we have worked with for the past few weeks).\nYour memo should outline the following:\n\nNeighborhood character, identity, and assets - drawing from secondary data, describe the character, identity, and assets of the neighborhood:\n\nInfrastructure and Environment\nEconomy and Housing\nHealth and Wellbeing\nSense of Place\n\nInformation Gaps - based upon your group’s description above, what information gaps exist? What types of information do you need to prioritize observation of on the ground in the West Urbana neighborhood?\nProposed Strategy for Systematic Examination - based upon your assessment of information gaps, how do you propose collecting that information, and how would you integrate it into your report?"
  },
  {
    "objectID": "schedule/25_fieldobs.html#groups",
    "href": "schedule/25_fieldobs.html#groups",
    "title": "Field Observations",
    "section": "Groups",
    "text": "Groups\n\nGroup 1\nTBD\n\n\nGroup 2\nTBD\n\n\nGroup 3\nTBD\n\n\nGroup 4\nTBD"
  },
  {
    "objectID": "schedule/25_fieldobs.html#after-class",
    "href": "schedule/25_fieldobs.html#after-class",
    "title": "Field Observations",
    "section": "After Class",
    "text": "After Class\nDraw upon your shared memo and your individual observations to develop a short reflection on the process.\n\nShare your initial reflection on what you knew about the neighborhood before you visited.\nShare a brief summary of your group’s thoughts and information priorities coming from Monday’s pre-field work preparation.\nShare your field observations, focusing on the element which you were responsible for describing.\nShare your reflections on what you know now and what you experienced in attempting to systematically observe the neighborhood and communicate its qualities.\nThink carefully about what your next steps might (hypothetically be) following your initial field observation.\n\nPush your individual reflections to your lab GitHub repository."
  },
  {
    "objectID": "schedule/27_indwork.html",
    "href": "schedule/27_indwork.html",
    "title": "Independent Work and Advising",
    "section": "",
    "text": "This session is devoted to individual advising and independent work. There are no course sessions scheduled for today. You should use this time to continue working, to take stock of your progress towards your course contract, and to solidify plans for your final project."
  },
  {
    "objectID": "schedule/27_indwork.html#session-description",
    "href": "schedule/27_indwork.html#session-description",
    "title": "Independent Work and Advising",
    "section": "",
    "text": "This session is devoted to individual advising and independent work. There are no course sessions scheduled for today. You should use this time to continue working, to take stock of your progress towards your course contract, and to solidify plans for your final project."
  },
  {
    "objectID": "schedule/27_indwork.html#before-class",
    "href": "schedule/27_indwork.html#before-class",
    "title": "Independent Work and Advising",
    "section": "Before Class",
    "text": "Before Class\nProfessor Greenlee has availability for office hours appointments. If you have not already scheduled an appointment and wish to, please sign up."
  },
  {
    "objectID": "schedule/27_indwork.html#reflect",
    "href": "schedule/27_indwork.html#reflect",
    "title": "Independent Work and Advising",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/27_indwork.html#slides",
    "href": "schedule/27_indwork.html#slides",
    "title": "Independent Work and Advising",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/27_indwork.html#resources-for-further-exploration",
    "href": "schedule/27_indwork.html#resources-for-further-exploration",
    "title": "Independent Work and Advising",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/29_finalpresent.html",
    "href": "schedule/29_finalpresent.html",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/29_finalpresent.html#session-description",
    "href": "schedule/29_finalpresent.html#session-description",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/29_finalpresent.html#todays-presenters",
    "href": "schedule/29_finalpresent.html#todays-presenters",
    "title": "Final Presentations",
    "section": "Today’s Presenters",
    "text": "Today’s Presenters\nReminder - each presenter has ten minutes to share."
  },
  {
    "objectID": "schedule/29_finalpresent.html#providing-feedback",
    "href": "schedule/29_finalpresent.html#providing-feedback",
    "title": "Final Presentations",
    "section": "Providing Feedback",
    "text": "Providing Feedback\nPlease provide feedback to each other in the following form - I will consolidate your written feedback on each presentation and will send it to you."
  },
  {
    "objectID": "schedule/index.html",
    "href": "schedule/index.html",
    "title": "Schedule",
    "section": "",
    "text": "Please find descriptions for our course sessions here - you will also find resources that will help you prepare for each session."
  },
  {
    "objectID": "schedule/index.html#course-introduction",
    "href": "schedule/index.html#course-introduction",
    "title": "Schedule",
    "section": "Course Introduction",
    "text": "Course Introduction\n\n\n\nWeek\nSession\nDate\nDescription\n\n\n\n\nWeek 1\nSession 1\nJanuary 17\nCourse Introduction\n\n\nWeek 2\nSession 2\nJanuary 22\nWhat is a Neighborhood?\n\n\nWeek 2\nSession 3\nJanuary 24\nBuilding a Data Pipeline\n\n\nWeek 3\nSession 4\nJanuary 29\nWorking with Tidy Data\n\n\nWeek 3\nSession 5\nJanuary 31\nWorking with Tidy Data\n\n\nWeek 4\n\nFebruary 5\nPlace Selection Memorandum\n\n\nWeek 4\nSession 6\nFebruary 5\nSharing Your Work\n\n\nWeek 4\nSession 7\nFebruary 7\nCommunicating Complex Information"
  },
  {
    "objectID": "schedule/index.html#strategies-for-analysis",
    "href": "schedule/index.html#strategies-for-analysis",
    "title": "Schedule",
    "section": "Strategies for Analysis",
    "text": "Strategies for Analysis\n\n\n\nWeek\nSession\nDate\nDescription\n\n\n\n\nWeek 5\nSession 8\nFebruary 12\nDescribing Places\n\n\nWeek 5\nSession 9\nFebruary 14\nDescribing Places\n\n\nWeek 6\n\nFebruary 19\nPlace Background Memorandum\n\n\nWeek 6\nSession 10\nFebruary 19\nPopulation and the Census\n\n\nWeek 6\nSession 11\nFebruary 21\nPopulation and the Census\n\n\nWeek 7\nSession 12\nFebruary 26\nSegregation\n\n\nWeek 7\nSession 13\nFebruary 28\nSegregation\n\n\nWeek 8\n\nMarch 4\nPopulation Memorandum\n\n\nWeek 8\nSession 14\nMarch 4\nNeighborhood Change\n\n\nWeek 8\nSession 15\nMarch 6\nNeighborhood Change\n\n\nWeek 9\n\nMarch 11\nSpring Break\n\n\nWeek 9\n\nMarch 13\nSpring Break\n\n\nWeek 10\nSession 16\nMarch 18\nPlace Opportunity\n\n\nWeek 10\nSession 17\nMarch 20\nPlace Opportunity\n\n\nWeek 11\n\nMarch 25\nPolicy Memorandum\n\n\nWeek 11\nSession 18\nMarch 25\nTransportation Equity\n\n\nWeek 11\nSession 19\nMarch 27\nTransportation Equity\n\n\nWeek 12\nSession 20\nApril 1\nHealth Equity\n\n\nWeek 12\nSession 21\nApril 3\nHealth Equity\n\n\nWeek 13\nSession 22\nApril 8\nFinal Project Check-In\n\n\nWeek 13\nSession 23\nApril 10\nFinal Project Check-In"
  },
  {
    "objectID": "schedule/index.html#course-wrap-up",
    "href": "schedule/index.html#course-wrap-up",
    "title": "Schedule",
    "section": "Course Wrap-Up",
    "text": "Course Wrap-Up\n\n\n\nWeek\nSession\nDate\nDescription\n\n\n\n\nWeek 14\n\nApril 15\nFinal Assignment First Draft\n\n\nWeek 14\nSession 24\nApril 15\nField Observation\n\n\nWeek 14\nSession 25\nApril 17\nField Observation\n\n\nWeek 15\nSession 26\nApril 22\nFinal Presentations\n\n\nWeek 15\nSession 27\nApril 24\nIndependent Work (No Class)\n\n\nWeek 16\nSession 28\nApril 29\nFinal Presentations\n\n\nWeek 16\nSession 29\nMay 1\nFinal Presentations\n\n\nWeek 17\n\nMay 10\nFinal Assignment Due"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neighborhood Analysis Spring 2024",
    "section": "",
    "text": "Neighborhood Analysis Spring 2024\nLearn to tell stories about neighborhoods for decision-making, public deliberation, and accountability using  and principles of reproducible data analysis.\n  \n\n\n\nInstructor\n\n   Dr. Andrew J. Greenlee\n   M210 Temple Buell Hall\n   agreen4@illinois.edu\n   Github\n   urbprof\n   https://fediscience.org/@urbprof\n   Schedule an appointment\n\n\n\nTeaching Assistant\n\n   Ouafa Benkraouda\n   ouafab2@illinois.edu\n   Github\n   Schedule an appointment\n\n\n\nCourse details\n\n   Mondays and Wednesdays\n   11:00 AM - 12:20 PM\n   Temple Buell Hall 223\n   Discussion"
  }
]