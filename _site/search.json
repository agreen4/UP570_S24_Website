[
  {
    "objectID": "syllabus/index.html",
    "href": "syllabus/index.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Neighborhood Analysis! I am excited to teach and learn with you this semester."
  },
  {
    "objectID": "syllabus/index.html#course-overview-and-objectives",
    "href": "syllabus/index.html#course-overview-and-objectives",
    "title": "Syllabus",
    "section": "Course Overview and Objectives",
    "text": "Course Overview and Objectives\nThis course teaches techniques for analyzing the demographic, economic, physical, and social conditions that exist at the neighborhood and local government scale. While our focus will be on analyzing current conditions, we will also learn how to tell stories about neighborhood change, and will learn advanced strategies for comparing neighborhoods to each other. We will learn how to describe community characteristics with small area census data, work with local administrative data, and will think about how our analysis of quantitative data fit with other forms of data and engagement to fill in gaps in knowledge.\nBy the end of this course, we will:\n\nBecome familiar with common sources of information used to describe neighborhoods and neighborhood characteristics;\nLearn how to use R, RStudio, and Github to create reproducible analysis;\nLearn how to work collaboratively to tell compelling stories for deliberation and decision-making."
  },
  {
    "objectID": "syllabus/index.html#course-format-and-expectations",
    "href": "syllabus/index.html#course-format-and-expectations",
    "title": "Syllabus",
    "section": "Course Format and Expectations",
    "text": "Course Format and Expectations\nOur focus during class sessions will be on learning by doing. Our class will meet twice per week - Tuesdays will typically be devoted to introducing new strategies and information, and Thursdays will typically be devoted to applying these strategies. Outside of class, you’ll use class resources and video tutorials to learn basic concepts and to prepare to apply these concepts in class.\n\nClass Computing Environment\nUnless otherwise noted, please plan on bringing to class a computer that will run R and RStudio (available for PC/Mac/Linux) and for which you have administrative privileges.\n\n\nWhat to Expect from me\n\nEmail: The easiest way to communicate with me outside of class is via email. I try to respond to emails sent during the week within 24 hours. Emails sent over the weekend will receive a response within 48 hours. If you don’t hear from me after that amount of time, it’s okay to nudge me to respond.\nOne-on-one meetings: I am available to speak one on one - you can book an appointment via my Calendly page. Of course, I am happy to chat before or after class if I am available.\n\n\n\n\n\n\nTroubleshooting: Plan on using our class Github Discussion Forum, email, and office hours to get help with troubleshooting problems as they arise in your work. The Resources page provides thoughts and resources for troubleshooting. I also encourage you to work with others in the class to troubleshoot problems - it is highly likely that others in the class have encountered similar problems, and this allows us to build a repository of our problems and responses.\n\n\n\nWhat I Expect From You\n\nBe Present: I expect that you’ll engage fully in our course sessions and in our class community.\nActively Support Each Other: I expect that each of you will take on individual leadership roles within our class, that includes actively supporting our learning community over the course of the semester. This class assumes collaboration and sharing as part of our learning model.\nRead with Care: This course focuses on learning by doing, however, there are important details contained within the documentation on our course website and within reading selections. Details matter in this class - be intentional about reading carefully and completely important course documents (including this syllabus).\nAct with Integrity: I expect that you will act with integrity in all that you do in this class. The class contract grading system places trust in you to not just meet the nominal standards contained within the contract, but to push yourself to produce your best work.\nSeek Balance: I expect that you’ll actively work to find balance between the many demands in your life. This means budgeting adequate time to engage fully in our course but also budgeting time for adequate rest and sleep, exercise, and other actions that support your mental and physical health.\n\n\n\nCourse Prerequisites\nUP 570: Neighborhood Analysis is open to students with graduate-level standing. While students will benefit from having some prior familiarity with R and RStudio or will have taken UP 517: Data Science for Planners, the class this semester will accomodate students with no prior background in coding or statistical analysis.\nOur first few course sessions will focus on ensuring that we are all familiar with basic workflows and methods which we’ll make use of over the course of the semester.\nPlease talk with me if you have any questions regarding whether this course is right for you."
  },
  {
    "objectID": "syllabus/index.html#evaluation-and-course-expectations",
    "href": "syllabus/index.html#evaluation-and-course-expectations",
    "title": "Syllabus",
    "section": "Evaluation and Course Expectations",
    "text": "Evaluation and Course Expectations\nYou will find detailed information on assignments, evaluation, and grading in the Assignments section.\n\nClass Attendance\nYou are expected to attend all of our class sessions in order to meet my standards for adequate performance in this course. Please notify me in advance of any course sessions which you will miss. Your final grades will be reduced by 1% per unexcused absence.\nFor those students who need to miss class due to a religious observance, please complete the Request for Accommodation for Religious Observances form should any instructors require an absence letter in order to manage the absence. In order to best facilitate planning and communication between students and faculty, I request that students make requests for absence letters as early as possible in the semester in which the request applies.\nFor more information on attendance policy as described in the University of Illinois Student Code, please see Sections 1-501 and 1-502.\n\n\nUse of AI Tools\nArtificial intelligence tools like Chat GPT have quickly made waves with their ability to produce text, code, and explanations from natural language prompts. I encourage (and at times will expect) you to integrate such tools into your problem solving strategies and workflow in our class. These tools must, however, be used with care and with understanding around how they synthesize and produce information.\nA few words of guidance:\n\nAssume that any code produced will require additional tweaking or troubleshooting to be implemented effectively in your workflow.\nAssume any facts or figures rendered via AI models are incorrect.\nUse these tools to help you break through coding or analysis challenges, not for writing up your narrative or findings.\nWhile you do not need to provide a citation for Chat GPT or other AI tools in your narrative or references, please indicate in any methods statements that these tools were employed. Please also reflect upon your application of these tools in your assignment submission reflections.\n\nA few related expectations:\n\nLet’s keep a running dialog about how you’re employing these tools, what some of the challenges are, and how you may want to integrate such technology into your workflows.\nYou are ultimately responsible for all content represented in your work. As part of the class pedagogy, you will electronically publish your assignments - this means you are publicly responsible for the content you produce.\nAll writing should be your own and written by you and not generative AI (unless you are quoting or citing someone else).\nDeviations from these expectations will be treated as academic dishonesty and are subject to review and disposition based upon the University’s honor code."
  },
  {
    "objectID": "syllabus/index.html#honor-code",
    "href": "syllabus/index.html#honor-code",
    "title": "Syllabus",
    "section": "Honor Code",
    "text": "Honor Code\nThe Illinois Student Code states: “It is the responsibility of the student to refrain from infractions of academic integrity, from conduct that may lead to suspicion of such infractions, and from conduct that aids others in such infractions.” Note that you are subject to the Honor Code, as well as procedures for addressing violations to the Code, regardless of whether you have read it and understand it. According to the Code, “ignorance is no excuse.”\nTo meet this standard in this course, note the following: in written work, all ideas (as well as data or other information) that are not your own must be cited. Note that ideas that require citation may not have been published or written down anywhere. While you are free—and indeed encouraged—to discuss assignments with your peers, all of your analysis, and writing should be your own. The consequence for violating these expectations may include receiving no credit for the assignment in question, and may include automatic failure of the course.\n\n\n\n\n\n\nA Simple Standard\n\n\n\nPut simply, give credit where credit is due."
  },
  {
    "objectID": "syllabus/index.html#learning-environment-and-support",
    "href": "syllabus/index.html#learning-environment-and-support",
    "title": "Syllabus",
    "section": "Learning Environment and Support",
    "text": "Learning Environment and Support\n\nOur Learning Environment\nThe Department of Urban and Regional Planning is committed to maintaining a learning environment that is rooted in the goals and responsibilities of professional planners. By enrolling in a class offered by the Department of Urban and Regional Planning, students agree to be responsible for maintaining an atmosphere of mutual respect in all activities, including lectures, discussions, labs, projects, and extracurricular opportunities. The University of Illinois Student Code should be considered part of this syllabus. See in particular Student Code Article 1-Student Rights and Responsibilities, Part 1. Student Rights: §1-102.\n\n\nOur Class Environment\nAs part of our classroom and university community, you have an obligation to do the following:\n\nAttend all class sessions if you are feeling well.\nIf you feel ill, do not come to class.\nIf you test positive for covid or have an exposure that requires testing or quarantine, do not come to class.\nPlease be respectful of all members of our learning community and their decisions regarding health and safety precautions.\n\n\n\nAccomodations for Students with Disabilities\nIf you need accommodations for any sort of disability, please make an office hours appointment so we can discuss your needs and ways I can support your learning. To ensure that disability-related concerns are properly addressed, students who require assistance to participate in this class should contact Disability Resources and Educational Services (DRES). DRES provides students with academic accommodations, access, and support services. To contact DRES you may visit 1207 S. Oak St., Champaign, call 333-4603 (V/TDD), or e-mail disability@illinois.edu.\n\n\nSexual Misconduct Reporting Obligation\nThe University of Illinois is committed to combating sexual misconduct. Faculty and staff members are required to report any instances of sexual misconduct to the University’s Title IX and Disability Office. In turn, an individual with the Title IX and Disability Office will provide information about rights and options, including accommodations, support services, the campus disciplinary process, and law enforcement options.\nA list of the designated University employees who, as counselors, confidential advisors, and medical professionals, do not have this reporting responsibility and can maintain confidentiality, can be found here. Other information about resources and reporting is available at wecare.illinois.edu.\n\n\nMental Health\nThe University of Illinois offers a variety of confidential services including individual and group counseling, crisis intervention, psychiatric services, and specialized screenings which are covered through the Student Health Fee. If you or someone you know experiences mental health concerns, please contact or visit any of the University’s resources provided below. Getting help is a smart and courageous thing to do for yourself and for those who care about you.\n\nCounseling Center: (217) 333-3704\nMcKinley Health Center: (217) 333-2700\n988 Suicide and Crisis Lifeline: (800) 273-8255\nRosecrance Crisis Line (217) 359-4141\n\nIf you are in immediate danger, call 911."
  },
  {
    "objectID": "syllabus/index.html#your-wellness",
    "href": "syllabus/index.html#your-wellness",
    "title": "Syllabus",
    "section": "Your Wellness",
    "text": "Your Wellness\nWellness at Illinois: Throughout the semester, you may need assistance coping with emotional, interpersonal, or academic concerns. wellness.illinois.edu is a good resource to identify help for yourself or others who may need assistance. Please do not hesitate to reach out or request assistance.\n\nLearning R and Your Wellness\nProblem solving is a major part of being a coder - you will face challenges related to working with the software this semester. That’s a given, and is an expected part of learning in this class. Part of the goal is to teach you how to understand the intentionality and logic behind the software so that you can anticipate where errors are likely to occur.\n\n\n\n\n\n\nExperiential Learning\n\n\n\nThe only way to do this is to encounter errors - and there will be many!\n\n\nYou are entering an intermediate to advanced stage of learning a new language, its grammar, and its application. While this will be frustrating at times, there is a major payoff in the capacity you will gain in analytic skills and problem-solving. This payoff will come slowly over time - do not expect it to come easily.\n\nYou are not alone in this struggle. In addition to your classmates and others who are going through the same thing, there is a large R user community, and lots of existing documentation and troubleshooting. Any problem you will encounter has likely been encountered and addressed before.\n\n\n\n\n\n\nTip\n\n\n\nWhen I run into an error, after an initial check for simple issues like closing parentheses and spelling errors, I copy and paste error codes directly into a web search to see how others have dealt with similar problems. I encourage you to do the same.\n\n\nYou got this, and there will be a payoff, so long as you use the tools consistently - I promise!\n\n\nWe’re Here For You\nWe’ve been living in particularly abnormal times for the last few years - full stop. It would be irresponsible to expect that teaching and learning would occur “normally” right now. We continue to teach and learn under emergency circumstances.\nAs you face challenges this semester (and beyond) I need you to communicate with me, either during our course sessions or individually. You can schedule an appointment with me at your convenience via my Calendly page. I promise to listen, to be a resource, and to help in any way that I can - if I can’t help you, I will find someone who can."
  },
  {
    "objectID": "schedule/Session_template.html#before-class",
    "href": "schedule/Session_template.html#before-class",
    "title": "NAME",
    "section": "Before Class",
    "text": "Before Class"
  },
  {
    "objectID": "schedule/Session_template.html#reflect",
    "href": "schedule/Session_template.html#reflect",
    "title": "NAME",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/Session_template.html#slides",
    "href": "schedule/Session_template.html#slides",
    "title": "NAME",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/Session_template.html#resources-for-further-exploration",
    "href": "schedule/Session_template.html#resources-for-further-exploration",
    "title": "NAME",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/28_finalpresent.html",
    "href": "schedule/28_finalpresent.html",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/28_finalpresent.html#session-description",
    "href": "schedule/28_finalpresent.html#session-description",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/28_finalpresent.html#todays-presenters",
    "href": "schedule/28_finalpresent.html#todays-presenters",
    "title": "Final Presentations",
    "section": "Today’s Presenters",
    "text": "Today’s Presenters\nReminder - each presenter has ten minutes to share."
  },
  {
    "objectID": "schedule/28_finalpresent.html#providing-feedback",
    "href": "schedule/28_finalpresent.html#providing-feedback",
    "title": "Final Presentations",
    "section": "Providing Feedback",
    "text": "Providing Feedback\nPlease provide feedback to each other in the following form - I will consolidate your written feedback on each presentation and will send it to you."
  },
  {
    "objectID": "schedule/26_finalpresent.html",
    "href": "schedule/26_finalpresent.html",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/26_finalpresent.html#session-description",
    "href": "schedule/26_finalpresent.html#session-description",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/26_finalpresent.html#todays-presenters",
    "href": "schedule/26_finalpresent.html#todays-presenters",
    "title": "Final Presentations",
    "section": "Today’s Presenters",
    "text": "Today’s Presenters\nReminder - each presenter has ten minutes to share."
  },
  {
    "objectID": "schedule/26_finalpresent.html#providing-feedback",
    "href": "schedule/26_finalpresent.html#providing-feedback",
    "title": "Final Presentations",
    "section": "Providing Feedback",
    "text": "Providing Feedback\nPlease provide feedback to each other in the following form - I will consolidate your written feedback on each presentation and will send it to you."
  },
  {
    "objectID": "schedule/24_fieldobs.html",
    "href": "schedule/24_fieldobs.html",
    "title": "Field Observations",
    "section": "",
    "text": "So far, we have started to learn how to tell stories about places using existing indicators. With so many available sources of existing information, it can be easy to experience a disconnect between the values of those indicators and the complexity of what they represent. To think more about what grounds our analysis as planners, this week we will go explore a neighborhood in person and then think through elements of the stories that may help us describe that place through indicators.\nWe are going to spend this class session doing field observation of the West Urbana neighborhood which is designated as an American Planning Association Great Place. We will meet at Carle Park in Urbana at the intersection of Carle and Indiana streets.\nCome prepared to spend the course session exploring the neighborhood. - We will meet at at Carle Park in Urbana on Monday to to focus on field observation. - On Wednesday, we will meet at TBH and will think about our plans for storytelling and description about the West Urbana neighborhood. - Following the completion of our lab, you’ll submit your individual reflection via GitHub. Repository link."
  },
  {
    "objectID": "schedule/24_fieldobs.html#session-description",
    "href": "schedule/24_fieldobs.html#session-description",
    "title": "Field Observations",
    "section": "",
    "text": "So far, we have started to learn how to tell stories about places using existing indicators. With so many available sources of existing information, it can be easy to experience a disconnect between the values of those indicators and the complexity of what they represent. To think more about what grounds our analysis as planners, this week we will go explore a neighborhood in person and then think through elements of the stories that may help us describe that place through indicators.\nWe are going to spend this class session doing field observation of the West Urbana neighborhood which is designated as an American Planning Association Great Place. We will meet at Carle Park in Urbana at the intersection of Carle and Indiana streets.\nCome prepared to spend the course session exploring the neighborhood. - We will meet at at Carle Park in Urbana on Monday to to focus on field observation. - On Wednesday, we will meet at TBH and will think about our plans for storytelling and description about the West Urbana neighborhood. - Following the completion of our lab, you’ll submit your individual reflection via GitHub. Repository link."
  },
  {
    "objectID": "schedule/24_fieldobs.html#before-class",
    "href": "schedule/24_fieldobs.html#before-class",
    "title": "Field Observations",
    "section": "Before Class",
    "text": "Before Class\n\nWrite a short reflection on your pre-existing impressions of the West Urbana neighborhood. Some of you may be very familiar, and others may not at all. Reflect based upon what you know or have heard.\nWear comfortable clothes, and be prepared to be outside for the duration of our course session.\nYou may wish to bring a notebook with you as well as a phone or camera to document your observations."
  },
  {
    "objectID": "schedule/24_fieldobs.html#during-class",
    "href": "schedule/24_fieldobs.html#during-class",
    "title": "Field Observations",
    "section": "During Class",
    "text": "During Class\nWe will split up into groups in order to observe elements of the West Urbana Neighborhood (as a reminder, Monday’s group assignments are below). Working in groups, each group member will be responsible for focusing on one element of observation (you may double up in an area of your choice if you have more than four in your group):\n\nInfrastructure and Environment\nEconomy and Housing\nHealth and Wellbeing\nSense of Place\n\nObservation 1: In your group, spend 20 minutes walking around the neighborhood, taking in and observing your specific element. Then take 5-10 minutes to write down your overall impressions and any questions you have after this brief exploration.\nObservation 2: As a group, pick a block to systematically analyze. Spend 20 minutes observing your specific element as it is reflected on your block. You may want to take pictures, sketch a map, or use other methods to record what you observe.\nObservation 3: Spend the remaining 30 minutes comparing notes with your other group members. Record a list of shared questions you have about the block and portions of the neighborhood that you explored. Reflect as a group upon what you were able to observe.\nAt the end of our class time you can leave."
  },
  {
    "objectID": "schedule/24_fieldobs.html#groups",
    "href": "schedule/24_fieldobs.html#groups",
    "title": "Field Observations",
    "section": "Groups",
    "text": "Groups\n\nGroup 1\nDakshinya B.  Dominic C.  Ar’Mand E.  Dasom H.  Tushar K.  Jenifer M.  Tillie P. \n\n\nGroup 2\nBhagyashree  Natalie C.  Luisa P.  Erin H.  Trinity L.  Anjana N.  Matthew R. \n\n\nGroup 3\nLeela B.  Rithvika D.  Cole F.  Matthew H.  Nadia K.  Erin P.  Aabha S. \n\n\nGroup 4\nGabriela A.  Siti A.  Hyndavi A.  Shinmyeong H.  Joseph J.  Anukriti M.  Anna S.  Alec T.  Zhenpeng Z."
  },
  {
    "objectID": "schedule/24_fieldobs.html#slides",
    "href": "schedule/24_fieldobs.html#slides",
    "title": "Field Observations",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/24_fieldobs.html#resources-for-further-exploration",
    "href": "schedule/24_fieldobs.html#resources-for-further-exploration",
    "title": "Field Observations",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nUrban Institute: Observation\nParticipant Observation and the Development of Urban Neighborhood Policy"
  },
  {
    "objectID": "schedule/22_CheckIn.html",
    "href": "schedule/22_CheckIn.html",
    "title": "Final Project Check-In: Monday",
    "section": "",
    "text": "This week is devoted to working on your your final projects. We will not have a formal class session today - use the time to work on your final project, ask questions of your instructors, or get peer feedback from others in the class."
  },
  {
    "objectID": "schedule/22_CheckIn.html#session-description",
    "href": "schedule/22_CheckIn.html#session-description",
    "title": "Final Project Check-In: Monday",
    "section": "",
    "text": "This week is devoted to working on your your final projects. We will not have a formal class session today - use the time to work on your final project, ask questions of your instructors, or get peer feedback from others in the class."
  },
  {
    "objectID": "schedule/22_CheckIn.html#reflect",
    "href": "schedule/22_CheckIn.html#reflect",
    "title": "Final Project Check-In: Monday",
    "section": "Reflect",
    "text": "Reflect\n\nWhat’s the major story you’re telling in your policy memorandum?\nWho is the audience for the story? What’s the best way to convey your point?\nWhat are the counterarguments or counterfactuals to the evidence that you are sharing. How will you engage with that?\nWhat types of technical issues are you running into that we could help address? Are there any technical concepts or issues that remain unclear that we should engage with or address as a class?"
  },
  {
    "objectID": "schedule/20_healthequity.html",
    "href": "schedule/20_healthequity.html",
    "title": "Health Equity",
    "section": "",
    "text": "A growing public conversation recognizes that individual health and collective health of populations is determined by a range of factors, including many that are social and rooted in social relationships. As this conversation grows and evolves, planners have an important role to play helping to disentangle and describe how places matter and influence the health of populations.\nThis week, we will explore a case study strategy for linking social determinants measures rooted in neighborhood indicators with measures of health outcomes. Rather than focus on building out new methodological or analytical techniques, our focus this week will be on storytelling and in thinking about how to engage communities in further conversations about their health and wellbeing."
  },
  {
    "objectID": "schedule/20_healthequity.html#session-description",
    "href": "schedule/20_healthequity.html#session-description",
    "title": "Health Equity",
    "section": "",
    "text": "A growing public conversation recognizes that individual health and collective health of populations is determined by a range of factors, including many that are social and rooted in social relationships. As this conversation grows and evolves, planners have an important role to play helping to disentangle and describe how places matter and influence the health of populations.\nThis week, we will explore a case study strategy for linking social determinants measures rooted in neighborhood indicators with measures of health outcomes. Rather than focus on building out new methodological or analytical techniques, our focus this week will be on storytelling and in thinking about how to engage communities in further conversations about their health and wellbeing."
  },
  {
    "objectID": "schedule/20_healthequity.html#before-class",
    "href": "schedule/20_healthequity.html#before-class",
    "title": "Health Equity",
    "section": "Before Class",
    "text": "Before Class\nDougherty, Geoff B., Sherita H. Golden, Alden L. Gross, Elizabeth Colantouoni, and Lorraine T. Dean. (2020). Measuring Structural Racism and Its Association with BMI. American Journal of Preventative Medicine .\nPlease also take a look at the appendix section on variables used in their approach. .\nCome to class ready to workshop ways in which we can tell stories about health and wellbeing."
  },
  {
    "objectID": "schedule/20_healthequity.html#reflect",
    "href": "schedule/20_healthequity.html#reflect",
    "title": "Health Equity",
    "section": "Reflect",
    "text": "Reflect\n\nWhat are some tropes or conventions we use to talk about individual health? Collective health?\nHow might health disparities (at a neighborhood or population level) be connected to the many themes we’ve talked about in this class?\nIs population health a planning issue? What role do practitioners and researchers in planning have for influencing how we think and act around health at the neighborhood level?"
  },
  {
    "objectID": "schedule/20_healthequity.html#slides",
    "href": "schedule/20_healthequity.html#slides",
    "title": "Health Equity",
    "section": "Slides",
    "text": "Slides\nAccess Slides Here"
  },
  {
    "objectID": "schedule/20_healthequity.html#resources-for-further-exploration",
    "href": "schedule/20_healthequity.html#resources-for-further-exploration",
    "title": "Health Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/18_transequity.html",
    "href": "schedule/18_transequity.html",
    "title": "Transit Equity",
    "section": "",
    "text": "This week, we’ll be examining principles of transit equity. Our discussions and lab will be facilitated by Ouafa, and inspired by her work."
  },
  {
    "objectID": "schedule/18_transequity.html#session-description",
    "href": "schedule/18_transequity.html#session-description",
    "title": "Transit Equity",
    "section": "",
    "text": "This week, we’ll be examining principles of transit equity. Our discussions and lab will be facilitated by Ouafa, and inspired by her work."
  },
  {
    "objectID": "schedule/18_transequity.html#before-class",
    "href": "schedule/18_transequity.html#before-class",
    "title": "Transit Equity",
    "section": "Before Class",
    "text": "Before Class\nIntroduction to Transportation Equity \nIntroduction to Measuring Transportation Equity \nTransportation Equity \nMobility justice \nMobility justice \nEquiticity"
  },
  {
    "objectID": "schedule/18_transequity.html#reflect",
    "href": "schedule/18_transequity.html#reflect",
    "title": "Transit Equity",
    "section": "Reflect",
    "text": "Reflect\n\nWhat are other urban factors that contribute to transportation and accessibility to jobs and amenities?\nHow can these measures help or put at disadvantage impacted communities and groups?\nHow can planners and policy makers facilitate sustainable (long term) interventions that assure equitable access to mobility?"
  },
  {
    "objectID": "schedule/18_transequity.html#slides",
    "href": "schedule/18_transequity.html#slides",
    "title": "Transit Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/18_transequity.html#resources-for-further-exploration",
    "href": "schedule/18_transequity.html#resources-for-further-exploration",
    "title": "Transit Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/16_opportunity.html",
    "href": "schedule/16_opportunity.html",
    "title": "Place Opportunity",
    "section": "",
    "text": "In this session, we’ll begin to examine opportunity analysis and mapping, which is a strategy used to compare multiple measures of sociodemographic wellbeing to create a single measure of how supportive a place is for economic and social mobility. Opportunity mapping in various forms has been used to craft legal remedies to discrimination and segregation, to design policy interventions, and as a source of information for planning and the allocation of resources.\nYour readings for today provide a) a contemporary overview of opportunity mapping, and b) a conceptual overview of the application of opportunity mapping by one of it’s originators, john powell.\nIn addition to our discussion of opportunity mapping, we’ll also take a few moments to check in on your final assignment progress, your detailed project description, and more generally about how things are going with the class."
  },
  {
    "objectID": "schedule/16_opportunity.html#session-description",
    "href": "schedule/16_opportunity.html#session-description",
    "title": "Place Opportunity",
    "section": "",
    "text": "In this session, we’ll begin to examine opportunity analysis and mapping, which is a strategy used to compare multiple measures of sociodemographic wellbeing to create a single measure of how supportive a place is for economic and social mobility. Opportunity mapping in various forms has been used to craft legal remedies to discrimination and segregation, to design policy interventions, and as a source of information for planning and the allocation of resources.\nYour readings for today provide a) a contemporary overview of opportunity mapping, and b) a conceptual overview of the application of opportunity mapping by one of it’s originators, john powell.\nIn addition to our discussion of opportunity mapping, we’ll also take a few moments to check in on your final assignment progress, your detailed project description, and more generally about how things are going with the class."
  },
  {
    "objectID": "schedule/16_opportunity.html#before-class",
    "href": "schedule/16_opportunity.html#before-class",
    "title": "Place Opportunity",
    "section": "Before Class",
    "text": "Before Class\nStromberg, Brian. (2016). Opportunity Mapping. National Housing Conference. \npowell, john. (2005). Remedial Phase Expert Report of john powell in Thompson v. HUD. \nBalachandran, Sowmya, and Andrew Greenlee. (2022). Examining Spatial Opportunity for Local Action: From Theory to Practice. Journal of Planning Education and Research."
  },
  {
    "objectID": "schedule/16_opportunity.html#reflect",
    "href": "schedule/16_opportunity.html#reflect",
    "title": "Place Opportunity",
    "section": "Reflect",
    "text": "Reflect\n\nWhat does place opportunity mean to you? What makes a place more (or less) opportune?\nAre there some universal dimensions of place opportunity? Some that are more specific to certain population groups?\nWhat types of practices support opportunity mapping? How can we use the outputs from opportunity mapping exercises for deliberation and policy decision-making?"
  },
  {
    "objectID": "schedule/16_opportunity.html#slides",
    "href": "schedule/16_opportunity.html#slides",
    "title": "Place Opportunity",
    "section": "Slides",
    "text": "Slides\nAccess Slides Here"
  },
  {
    "objectID": "schedule/16_opportunity.html#resources-for-further-exploration",
    "href": "schedule/16_opportunity.html#resources-for-further-exploration",
    "title": "Place Opportunity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/14_neighborhood.html",
    "href": "schedule/14_neighborhood.html",
    "title": "Neighborhood Change",
    "section": "",
    "text": "In this session, we’ll begin exploring the measurement of neighborhood change as well as underlying theories of transition."
  },
  {
    "objectID": "schedule/14_neighborhood.html#session-description",
    "href": "schedule/14_neighborhood.html#session-description",
    "title": "Neighborhood Change",
    "section": "",
    "text": "In this session, we’ll begin exploring the measurement of neighborhood change as well as underlying theories of transition."
  },
  {
    "objectID": "schedule/14_neighborhood.html#before-class",
    "href": "schedule/14_neighborhood.html#before-class",
    "title": "Neighborhood Change",
    "section": "Before Class",
    "text": "Before Class\nWBEZ: There Goes the Neighborhood\nWho Can Live in Chicago?: A Tale of Three Cities\nThe Socioeconomic Change of Chicago’s Community Areas (1970-2010)"
  },
  {
    "objectID": "schedule/14_neighborhood.html#reflect",
    "href": "schedule/14_neighborhood.html#reflect",
    "title": "Neighborhood Change",
    "section": "Reflect",
    "text": "Reflect\n\nWhat role should planners take in managing neighborhood change?\nHow do planners balance changes across multiple neighborhoods?\nHow can value judgements placed on change influence the types of solutions we propose (and their feasibility)?"
  },
  {
    "objectID": "schedule/14_neighborhood.html#slides",
    "href": "schedule/14_neighborhood.html#slides",
    "title": "Neighborhood Change",
    "section": "Slides",
    "text": "Slides\nAccess Slides Here"
  },
  {
    "objectID": "schedule/14_neighborhood.html#resources-for-further-exploration",
    "href": "schedule/14_neighborhood.html#resources-for-further-exploration",
    "title": "Neighborhood Change",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nThe Three Cities Within Toronto: Income Polarization Amongst Toronto’s Neighbourhoods, 1970-2005"
  },
  {
    "objectID": "schedule/12_segregation.html",
    "href": "schedule/12_segregation.html",
    "title": "Segregation",
    "section": "",
    "text": "This week, we will begin a conversation about the nature of residential segregation, and the common ways in which it is measured. Our lab this week will build off of the general descriptions which we worked on last week in which we identified and mapped the largest racial groups by census tract."
  },
  {
    "objectID": "schedule/12_segregation.html#session-description",
    "href": "schedule/12_segregation.html#session-description",
    "title": "Segregation",
    "section": "",
    "text": "This week, we will begin a conversation about the nature of residential segregation, and the common ways in which it is measured. Our lab this week will build off of the general descriptions which we worked on last week in which we identified and mapped the largest racial groups by census tract."
  },
  {
    "objectID": "schedule/12_segregation.html#before-class",
    "href": "schedule/12_segregation.html#before-class",
    "title": "Segregation",
    "section": "Before Class",
    "text": "Before Class\nCunningham, Mary K., and Augrey Droesch. Neighborhood Quality and Racial Segregation. The Urban Institute. \nU.S. Bureau of the Census: Measures of Residential Segregation"
  },
  {
    "objectID": "schedule/12_segregation.html#reflect",
    "href": "schedule/12_segregation.html#reflect",
    "title": "Segregation",
    "section": "Reflect",
    "text": "Reflect\n\nWhy, in your opinion, does segregation remain an enduring characteristic for most American cities, despite efforts to address it?\nHow can visualization of segregation (and its consequences) make a difference?\nWhat can segregation measures capture well? What aspects of segregation are more challenging to measure?"
  },
  {
    "objectID": "schedule/12_segregation.html#slides",
    "href": "schedule/12_segregation.html#slides",
    "title": "Segregation",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/12_segregation.html#resources-for-further-exploration",
    "href": "schedule/12_segregation.html#resources-for-further-exploration",
    "title": "Segregation",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/10_census.html",
    "href": "schedule/10_census.html",
    "title": "Population and the Census",
    "section": "",
    "text": "In this session, we will talk about how planners measure basic dimensions of population and change in population at the neighborhood level. We’ll also discuss some of the basic principles and features of the main Census products which planners use to describe places and the people who live and work in them. We’ll introduce this week’s lab, which focuses on working with basic data on population characteristics."
  },
  {
    "objectID": "schedule/10_census.html#session-description",
    "href": "schedule/10_census.html#session-description",
    "title": "Population and the Census",
    "section": "",
    "text": "In this session, we will talk about how planners measure basic dimensions of population and change in population at the neighborhood level. We’ll also discuss some of the basic principles and features of the main Census products which planners use to describe places and the people who live and work in them. We’ll introduce this week’s lab, which focuses on working with basic data on population characteristics."
  },
  {
    "objectID": "schedule/10_census.html#before-class",
    "href": "schedule/10_census.html#before-class",
    "title": "Population and the Census",
    "section": "Before Class",
    "text": "Before Class\nKlosterman 2 .\nRead our lab background and bring any questions to today’s class. We will work on the lab during our class time on Wednesday."
  },
  {
    "objectID": "schedule/10_census.html#reflect",
    "href": "schedule/10_census.html#reflect",
    "title": "Population and the Census",
    "section": "Reflect",
    "text": "Reflect\n\nWhat types of characteristics might be well-represented within Census data? What characteristics are harder to measure or represent?\nAs we’ve discussed in class, the census is a dynamic and evolving survey, and the questions we ask are a valuable window into the social questions and issues at a given time. What questions do you think we should be asking in this current moment? How well are they reflected in the census (as you know it)?"
  },
  {
    "objectID": "schedule/10_census.html#slides",
    "href": "schedule/10_census.html#slides",
    "title": "Population and the Census",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/10_census.html#resources-for-further-exploration",
    "href": "schedule/10_census.html#resources-for-further-exploration",
    "title": "Population and the Census",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\n2020 Census Enumeration Form \n2020 Census Post Enumeration Survey Documentation \nDifferential Privacy and the 2020 US Census \nAmerican Community Survey Sample Size"
  },
  {
    "objectID": "schedule/08_places.html",
    "href": "schedule/08_places.html",
    "title": "Describing Places",
    "section": "",
    "text": "In this session, we’ll spend time talking about how to describe places, as well as some frameworks for making place comparisons. We will think about how the types of stories we tell about places connect with typical arguments and tropes which guide place description and analysis within urban planning and policy settings.\nIn addition to our discussion of places, we’ll debrief your two labs from last week and introduce this week’s lab."
  },
  {
    "objectID": "schedule/08_places.html#session-description",
    "href": "schedule/08_places.html#session-description",
    "title": "Describing Places",
    "section": "",
    "text": "In this session, we’ll spend time talking about how to describe places, as well as some frameworks for making place comparisons. We will think about how the types of stories we tell about places connect with typical arguments and tropes which guide place description and analysis within urban planning and policy settings.\nIn addition to our discussion of places, we’ll debrief your two labs from last week and introduce this week’s lab."
  },
  {
    "objectID": "schedule/08_places.html#before-class",
    "href": "schedule/08_places.html#before-class",
    "title": "Describing Places",
    "section": "Before Class",
    "text": "Before Class\nLynch, Kevin. (1960). The Image of the City. Chapter 3 ."
  },
  {
    "objectID": "schedule/08_places.html#reflect",
    "href": "schedule/08_places.html#reflect",
    "title": "Describing Places",
    "section": "Reflect",
    "text": "Reflect\n\nWe intuitively tell lots of stories about places. What are some of the most evocative tropes you can think of that could apply to places? What makes for a good story about place?\nWhat are some of the benefits and challenges of making place comparisons? How does this feel from the perspective of an analyst? What dangers or challenges does the analyst face when telling comparative stories to others?"
  },
  {
    "objectID": "schedule/08_places.html#slides",
    "href": "schedule/08_places.html#slides",
    "title": "Describing Places",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/08_places.html#resources-for-further-exploration",
    "href": "schedule/08_places.html#resources-for-further-exploration",
    "title": "Describing Places",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nAnti-Eviction Mapping Project \nEviction Lab Methodology \nBloomberg: What’s Really Warming the World \nDesign Justice Project \nGlobal Atlas of Environmental Justice \nFive Thirty Eight: Kidnapping of Girls in Nigeria Is Part of a Worsening Problem \nMemorandum on Transparency and Open Government"
  },
  {
    "objectID": "schedule/06_sharing.html",
    "href": "schedule/06_sharing.html",
    "title": "Sharing Your Work",
    "section": "",
    "text": "This session builds upon the work on your last lab. In that lab, you worked on developing several workflows that will support your work over the course of the semester. Coming into this lab, you should have a formatted Quarto markdown document. In this session, we’ll talk about strategies for sharing that work, will configure your computer to communicate with GitHub, and will create your first publicly facing websites."
  },
  {
    "objectID": "schedule/06_sharing.html#session-description",
    "href": "schedule/06_sharing.html#session-description",
    "title": "Sharing Your Work",
    "section": "",
    "text": "This session builds upon the work on your last lab. In that lab, you worked on developing several workflows that will support your work over the course of the semester. Coming into this lab, you should have a formatted Quarto markdown document. In this session, we’ll talk about strategies for sharing that work, will configure your computer to communicate with GitHub, and will create your first publicly facing websites."
  },
  {
    "objectID": "schedule/06_sharing.html#before-class",
    "href": "schedule/06_sharing.html#before-class",
    "title": "Sharing Your Work",
    "section": "Before Class",
    "text": "Before Class\n\nRead through the entire lab background description before approaching lab tasks.\nBe prepared to access the formatted Quarto notebook you worked on in the last lab that contains your analysis of Chicago community areas.\nBe prepared to access your Lab 1 reflection."
  },
  {
    "objectID": "schedule/06_sharing.html#in-class",
    "href": "schedule/06_sharing.html#in-class",
    "title": "Sharing Your Work",
    "section": "In Class",
    "text": "In Class\nOuafa Zoom Link\nHello everyone - live from class!"
  },
  {
    "objectID": "schedule/06_sharing.html#reflect",
    "href": "schedule/06_sharing.html#reflect",
    "title": "Sharing Your Work",
    "section": "Reflect",
    "text": "Reflect\n\nHow can planners and others engaging directly in public policy discourse and debate leverage emotion in their analysis in ways that generate meaning and connection without manipulating or leading towards particular conclusions?\nIs there such thing a “neutral” data analysis?\nCan you think of classification systems that may have unintended consequences or biases in data that you’ve used for urban analysis in the past?"
  },
  {
    "objectID": "schedule/06_sharing.html#slides",
    "href": "schedule/06_sharing.html#slides",
    "title": "Sharing Your Work",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/06_sharing.html#resources-for-further-exploration",
    "href": "schedule/06_sharing.html#resources-for-further-exploration",
    "title": "Sharing Your Work",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nGitcreds Package"
  },
  {
    "objectID": "schedule/04_tidydata.html",
    "href": "schedule/04_tidydata.html",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "",
    "text": "Our work this semester blends building capacity for data analysis and storytelling with basic data science skills. Throughout the course of the semester, we will frequently work to structure data in a tidy format - one in which we have one variable per column, and for which each row represents a unique observation. Some of you with a prior background or experience working in R will already be familiar with these principles, but some of you are not. This week, we’ll spend time reviewing the basics of what tidy data are, and will review common manipulation strategies."
  },
  {
    "objectID": "schedule/04_tidydata.html#session-description",
    "href": "schedule/04_tidydata.html#session-description",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "",
    "text": "Our work this semester blends building capacity for data analysis and storytelling with basic data science skills. Throughout the course of the semester, we will frequently work to structure data in a tidy format - one in which we have one variable per column, and for which each row represents a unique observation. Some of you with a prior background or experience working in R will already be familiar with these principles, but some of you are not. This week, we’ll spend time reviewing the basics of what tidy data are, and will review common manipulation strategies."
  },
  {
    "objectID": "schedule/04_tidydata.html#before-class",
    "href": "schedule/04_tidydata.html#before-class",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "Before Class",
    "text": "Before Class\nReview the How To Lesson 1 and Lesson 2. If you are comfortable with these principles and strategies, please take a look at Lesson 3"
  },
  {
    "objectID": "schedule/04_tidydata.html#reflect",
    "href": "schedule/04_tidydata.html#reflect",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/04_tidydata.html#slides",
    "href": "schedule/04_tidydata.html#slides",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/04_tidydata.html#resources-for-further-exploration",
    "href": "schedule/04_tidydata.html#resources-for-further-exploration",
    "title": "Introduction to Tidy Data - Session 1",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/02_neighborhood.html",
    "href": "schedule/02_neighborhood.html",
    "title": "What is a Neighborhood?",
    "section": "",
    "text": "In this session, we’ll explore the many ways in which the concept of neighborhoods are used in various areas of urban planning and governance. We will continue to build upon our initial discussion regarding the significance of neighborhoods as a unit of analysis, planning, and policymaking, and will explore frameworks for neighborhood analysis."
  },
  {
    "objectID": "schedule/02_neighborhood.html#session-description",
    "href": "schedule/02_neighborhood.html#session-description",
    "title": "What is a Neighborhood?",
    "section": "",
    "text": "In this session, we’ll explore the many ways in which the concept of neighborhoods are used in various areas of urban planning and governance. We will continue to build upon our initial discussion regarding the significance of neighborhoods as a unit of analysis, planning, and policymaking, and will explore frameworks for neighborhood analysis."
  },
  {
    "objectID": "schedule/02_neighborhood.html#before-class",
    "href": "schedule/02_neighborhood.html#before-class",
    "title": "What is a Neighborhood?",
    "section": "Before Class",
    "text": "Before Class\nYour readings for today provide insight into some of the working definitions for value judgments regarding the qualities of neighborhoods and why planners have found these qualities to be of importance to measure and understand.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou must be logged in to your UIUC Box.com account in order to access these readings. This is to respect the copyright of the authors on a publicly accessible website.\n\n\nTalen, Emily, Sunny Menozzi, and Chloe Schaefer. (2015). What is a “Great Neighborhood”? An Analysis of APA’s Top-Rated Places. Journal of the American Planning Association, 81(2): 121-141. \nRohe, William. (2009). From Local to Global: One Hundred Years of Neighborhood Planning. Journal of the American Planning Association 75(2): 209-230."
  },
  {
    "objectID": "schedule/02_neighborhood.html#reflect",
    "href": "schedule/02_neighborhood.html#reflect",
    "title": "What is a Neighborhood?",
    "section": "Reflect",
    "text": "Reflect\n\nWhat matters about neighborhoods? How have neighborhoods shaped your life?\nWhat types of stories do we tend to tell about neighborhoods? How do these stories contextualize how neighborhoods “fit” within cities and their regions?"
  },
  {
    "objectID": "schedule/02_neighborhood.html#slides",
    "href": "schedule/02_neighborhood.html#slides",
    "title": "What is a Neighborhood?",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/02_neighborhood.html#resources-for-further-exploration",
    "href": "schedule/02_neighborhood.html#resources-for-further-exploration",
    "title": "What is a Neighborhood?",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nDahir, James. (1947). The Neighborhood Unit Plan: Its Spread and Acceptance. New York: The Russell Sage Foundation.\nMumford, Lewis. (1954). The Neighborhood and Neighborhood Unit. The Town Planning Review, 24(4): 256-270.\nSteuteville, Robert. (2019). The Once and Future Neighborhood. CNU Public Square."
  },
  {
    "objectID": "resources/textbook.html",
    "href": "resources/textbook.html",
    "title": "Text Resources",
    "section": "",
    "text": "D’Ignazio, Catherine, and Lauren F. Klein. (2020). Data Feminism. MIT Press.\n\nChapter 1 \nChapter 2 \nChapter 3 \nChapter 4 \nChapter 5 \nChapter 6 \nChapter 7"
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "This page contains resources that will help you over the course of the class. As you encounter more resources that you find useful, please share them so they can be added to this page."
  },
  {
    "objectID": "resources/analysis.html",
    "href": "resources/analysis.html",
    "title": "Neighborhood Analysis",
    "section": "",
    "text": "City of Champaign Neighborhood Wellness Vision Report and Action Plan\nChicago's Million Dollar Blocks\nDePaul Institute for Housing Studies: Mapping Displacement Pressure in Chicago, 2020\nNathalie P. Voorhees Center's Socioeconomic Change of Chicago's Community Areas (1970-2010)\nNational Neighborhood Indicators Partnership\nFind My Landlord, a tool that maps the properties in Chicago owned by a single landlord or management company.\nMetropolitan Planning Council and Urban Institute's The Cost of Segregationreport for Chicago.\nAmerican segregation, mapped at day and night"
  },
  {
    "objectID": "howto/setupr.html",
    "href": "howto/setupr.html",
    "title": "Setting up R and RStudio",
    "section": "",
    "text": "R can be downloaded from the Comprehensive R Archive Network (CRAN), a network of mirrored servers throughout the world that host the R software as well as user-developed packages. Visit https://cloud.r-project.org which will automatically direct you to the best download server.\nOnce here, you can then download and install R using the version that matches your operating system:\n\nGo ahead and download R and install following the installer defaults. After R is downloaded and installed, run R to confirm. You should see a single window that looks something like this:\nThat’s it for R. This should be the first and last time you’ll need to open the R console directly. We typically will not access R directly, but instead will use RStudio as our primary interface. Go ahead and close the R console and proceed by downloading and installing RStudio, which is the IDE we will use to interface and write code with R."
  },
  {
    "objectID": "howto/setupr.html#download-r",
    "href": "howto/setupr.html#download-r",
    "title": "Setting up R and RStudio",
    "section": "",
    "text": "R can be downloaded from the Comprehensive R Archive Network (CRAN), a network of mirrored servers throughout the world that host the R software as well as user-developed packages. Visit https://cloud.r-project.org which will automatically direct you to the best download server.\nOnce here, you can then download and install R using the version that matches your operating system:\n\nGo ahead and download R and install following the installer defaults. After R is downloaded and installed, run R to confirm. You should see a single window that looks something like this:\nThat’s it for R. This should be the first and last time you’ll need to open the R console directly. We typically will not access R directly, but instead will use RStudio as our primary interface. Go ahead and close the R console and proceed by downloading and installing RStudio, which is the IDE we will use to interface and write code with R."
  },
  {
    "objectID": "howto/setupr.html#downlad-rstudio",
    "href": "howto/setupr.html#downlad-rstudio",
    "title": "Setting up R and RStudio",
    "section": "Downlad RStudio",
    "text": "Downlad RStudio\nNext up, let’s download RStudio. Go to https://rstudio.com/products/rstudio/download. Click on “Download” under RStudio Desktop. The website will suggest the most appropriate current version of the software based upon the computer you are loading RStudio on. You may also choose from an alternate installer if you believe it is more appropriate for your operating system.\nDownload and install RStudio, again using the default installation settings.\nOnce you have RStudio installed, open RStudio. You should see something that looks like this (the information in your console window will likely describe a newer version of R than what is displayed here).\n\nYou’ll note that what appears in the portion of the console to the left looks very similar to the R console window which you opened before you started your install of RStudio. This console functions in exactly the same way, however, you’ll also note that there are other areas which you have access to as well."
  },
  {
    "objectID": "howto/setupr.html#rtools-error-pc-users",
    "href": "howto/setupr.html#rtools-error-pc-users",
    "title": "Setting up R and RStudio",
    "section": "Rtools error (PC users)",
    "text": "Rtools error (PC users)\nSome PC users may encounter an error message regarding Rtools not being installed. This would come up with you first try to install a package such as tidyverse either using R’s package manager or using the install.packages() command. To remedy this error if it occurs, PC users need to close R, manually download a patch, install it, and restart your computer. This error should then be remedied.\nIf you are a PC user who encounters this error, please do the following: 1. Save any work and close your RStudio session 2. Download Rtools (download link) + the link above contains instructions for putting Rtools on the PATH - you should not need to do this. 3. Run the downloaded executable file to install Rtools 4. Restart your computer 5. Re-open RStudio - the problem should be resolved"
  },
  {
    "objectID": "howto/setupr.html#run-r-in-the-cloud",
    "href": "howto/setupr.html#run-r-in-the-cloud",
    "title": "Setting up R and RStudio",
    "section": "Run R in the Cloud",
    "text": "Run R in the Cloud\nUIUC AnyWare provides cloud-based options for running software including RStudio."
  },
  {
    "objectID": "howto/setupr.html#the-rstudio-interface",
    "href": "howto/setupr.html#the-rstudio-interface",
    "title": "Setting up R and RStudio",
    "section": "The RStudio Interface",
    "text": "The RStudio Interface\n\nThe R Console is the place where code you write is executed. Typically we’ll write code in a script or R Notebook (more on those later) and active pieces of code will them be executed in the R console. You can also type code directly into the console and execute it by hitting the return or enter key.\nThe Environment Window provides information on variables, data tables, and other objects you create and define as you work.\nThe Auxiliary Window provides a range of information, and includes a file browser, plot visualization window, and access to help documentation.\n\nWith those basics in mind, you can start to explore the basic functionality of R."
  },
  {
    "objectID": "howto/rbasics_02.html#lesson-goals",
    "href": "howto/rbasics_02.html#lesson-goals",
    "title": "Lesson 2: Manipulating Data Frames",
    "section": "Lesson Goals",
    "text": "Lesson Goals\nBy the end of this lesson, you should be familiar with:\n\nBasic tools for selecting and subsetting data\nStrategies for describing data"
  },
  {
    "objectID": "howto/rbasics_02.html#getting-set-up",
    "href": "howto/rbasics_02.html#getting-set-up",
    "title": "Lesson 2: Manipulating Data Frames",
    "section": "Getting Set Up",
    "text": "Getting Set Up\nThe Data\nSo far, we’ve worked with “tiny” bits of data - mainly things that we input manually. It’s time to start working with some real world data!\nWe’re going to work with data on those census tracts that were designated as Opportunity Zones as part of the federal Tax Cuts and Jobs Act of 2017. These incentives are designed to spur investment in low-income and undercapitalized cities, by providing investors with tax incentives to invest capital in these locations. Each state had the opportunity to designate specific census tracts as Opportunity Zones. Practitioners and researchers have many questions about the efficacy of the program and the designations made by governors.\nTake a look here to see a map of where designated opportunity zones are located. The orange geometries reflected on the map are census tracts, which we often use as a proxy for neighborhoods, especially in urban areas. Find a place you know, and take a look at which areas are designated.\nThe specific data we’ll work with comes from the Urban Institute - they have joined the IRS’s list of Opportunity Zones designated by the Tax Cuts and Jobs Act to a series of indicators focused on investment potential. A copy of the Urban Institute’s dataset is available here for download.\n\n\nYou’ll need to authenticate and log in to UIUC Box to access this file. You can also download the data directly from Urban Institute’s Opportunity Zone landing page.\nExtending Base R With Packages\nWe’re been working exclusively in “base” R as we are getting familiar with the R language and RStudio interface. This means we’ve been working with the commands and functions that come with every new installation of R.\nThere’s a whole world of other functions that we can use to extend R’s functionality - these packages can help us do things like load data with specific formats, make visualizations, and run specific types of statistical models. There’s more than 20,000 packages which users have created to extend R’s functionality - you can find a list here. We’ll introduce common packages which help with neighborhood analysis tasks, but your favorite search engine is probably the easiest way to find packages or functions that may help you read specific data or perform certain types of tasks.\nReadxl\nOur first exploration with packages will help us read in our Opportunity Zone data. We will use a package called readxl which is designed to, well, read Microsoft Excel files such as the Opportunity Zone data.\nInstalling Readxl\nWe will use the readxl package to help us read our Opportunity Zone data from an Excel workbook into Rstudio so we can view it as a data frame. The first time you want to make use of a package you have not used before, you’ll need to install it, which essentially means R needs to download it from a repository on the internet and prepare it to be accessed on your computer.\nWe use the install.packages() command to automatically download and prepare the package for use:\n\ninstall.packages(\"readxl\")\n\n\n\nNote that the name of the package is in quotes here. Any guesses as to why?\nNote that we only need to install the package one time. It will remain available on your machine for use in the future.\nLoading Readxl\nIn our previous step, we installed the readxl package which involved downloading it form an internet repository and preparing it for use. There’s an additional step - now we need to load the package, which tells R that we want to make the package’s functions available for use in our current R session. The library() command loads packages for use in our current R session. While we only need to install a package one time, each time you start a new R session, you’ll need to load the packages you want to use for a particular kind of additional functionality.\n\nlibrary(readxl)\n\n\n\nNote that the name of the package is not in quotes here. Any guesses as to why?\nNow that the readxl package is loaded, we can make use of it to read our Opportunity Zone data into our R session.\nTakeaways\n\nIn general, you will only need to use install.packages() once (once a package is installed, you can use it for all R sessions moving forward).\nTo load packages, we use the library() command. This will load an already installed package and make it accessible within our current R session. You will need to load already installed packages at the beginning of each new R session. Typically, it is a good practice to load the packages you’ll use in a script at the beginning of the script.\n\nNote that to install the package, you need to treat the package name as a character vector \"readxl\", but when you load it in your R session, it does not need to be treated as a character vectorreadxl."
  },
  {
    "objectID": "howto/rbasics_02.html#read-in-data",
    "href": "howto/rbasics_02.html#read-in-data",
    "title": "Lesson 2: Manipulating Data Frames",
    "section": "Read in Data",
    "text": "Read in Data\nNow that we’ve loaded the readxl package, we can import the Excel file containing data on tracts designated as opportunity zones in the United States. To learn about the functions in the package, you can either do a Google search for Readxl, or you can use R’s built in documentation by typing ?readxl\n\n?readxl\n\nNote that running this command pops open documentation for readxl in the Help tab of the auxiliary pane. As the documentation states, readxl imports excel files. Looking at the documentation, the read_excel() command will read a single excel sheet, or we can optionally select a sheet by name or number from an excel workbook with multiple sheets. In this case, the Urban Institute data is in a workbook with a single sheet, so we just need to tell R where the file is to load.\nThe dataset we want to load is called “urbaninstitute_tractlevelozanalysis_update01142021.xlsx” (oof! - that’s a descriptive but way too long file name!). We can point R to the correct location. Since our R project file sets the relative path for all of the work within, the path to the data is:\"data/urbaninstitute_tractlevelozanalysis_update1242018.xlsx\". Wrapped into the command to read the excel file, it looks like this:\n\nread_excel(\"data/urbaninstitute_tractlevelozanalysis_update01142021.xlsx\")\n\nR read the data and is displaying it to us. But one problem - it read the data in and we can look at it, but it’s not really available for our continued use. We typically want to read data and store it as an object so that we can refer back to it and even modify it.\nLet’s go ahead and read the Excel data in again, but this time, we’ll assign it to an object called “ozs”:\n\nozs&lt;-read_excel(\"data/urbaninstitute_tractlevelozanalysis_update01142021.xlsx\")\n\nLook at your Environment window (top right quadrant of RStudio) - a data frame containing information on opportunity zones should be loaded in an object called “ozs”.\nThe environment window tells us that the object ozs contains 42,176 observations (rows) and 27 variables (columns).\nIf we type the name of the object, we can view it’s contents:\n\nozs\n\n# A tibble: 42,178 × 27\n   geoid       state   DesignatedOZ county    Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 01001020200 Alabama           NA Autauga … Low-…         4      NA       2196\n 2 01001020300 Alabama           NA Autauga … Non-…         6      NA       3136\n 3 01001020700 Alabama            1 Autauga … Low-…         8      NA       3047\n 4 01001020802 Alabama           NA Autauga … Non-…        10      NA      10743\n 5 01001021000 Alabama           NA Autauga … Non-…         5      NA       2899\n 6 01001021100 Alabama           NA Autauga … Low-…         6      NA       3247\n 7 01003010100 Alabama           NA Baldwin … Non-…         6      NA       4013\n 8 01003010200 Alabama            1 Baldwin … Low-…         9      NA       3067\n 9 01003010300 Alabama           NA Baldwin … Non-…        10      NA       8079\n10 01003010400 Alabama            1 Baldwin … Non-…         9      NA       4578\n# ℹ 42,168 more rows\n# ℹ 19 more variables: medhhincome &lt;dbl&gt;, PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;,\n#   medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;,\n#   vacancyrate &lt;dbl&gt;, pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;,\n#   pctAAPIalone &lt;dbl&gt;, pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;,\n#   BAorhigher &lt;dbl&gt;, Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\nYou could also inspect the dataset using the View() command. This will allow us to look at the data in a tabular format.\n\nView(ozs)\n\nNow, use the str() (structure) command to gain a better understanding of the types of data in each column\n\nstr(ozs)\n\ntibble [42,178 × 27] (S3: tbl_df/tbl/data.frame)\n $ geoid           : chr [1:42178] \"01001020200\" \"01001020300\" \"01001020700\" \"01001020802\" ...\n $ state           : chr [1:42178] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ DesignatedOZ    : num [1:42178] NA NA 1 NA NA NA NA 1 NA 1 ...\n $ county          : chr [1:42178] \"Autauga County\" \"Autauga County\" \"Autauga County\" \"Autauga County\" ...\n $ Type            : chr [1:42178] \"Low-Income Community\" \"Non-LIC Contiguous\" \"Low-Income Community\" \"Non-LIC Contiguous\" ...\n $ dec_score       : num [1:42178] 4 6 8 10 5 6 6 9 10 9 ...\n $ SE_Flag         : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n $ Population      : num [1:42178] 2196 3136 3047 10743 2899 ...\n $ medhhincome     : num [1:42178] 41107 51250 45234 61242 49567 ...\n $ PovertyRate     : num [1:42178] 0.24 0.107 0.19 0.153 0.151 ...\n $ unemprate       : num [1:42178] 0.0775 0.051 0.1407 0.0459 0.0289 ...\n $ medvalue        : num [1:42178] 95300 113800 93500 160400 102900 ...\n $ medrent         : num [1:42178] 743 817 695 1018 546 ...\n $ pctown          : num [1:42178] 0.628 0.703 0.711 0.823 0.83 ...\n $ severerentburden: num [1:42178] 0.3269 0.3223 0.3887 0.1994 0.0994 ...\n $ vacancyrate     : num [1:42178] 0.0584 0.1399 0.0619 0.0609 0.2182 ...\n $ pctwhite        : num [1:42178] 0.439 0.671 0.833 0.814 0.726 ...\n $ pctBlack        : num [1:42178] 0.5187 0.205 0.0922 0.1572 0.2456 ...\n $ pctHispanic     : num [1:42178] 0.01275 0.0727 0.0338 0.01368 0.00448 ...\n $ pctAAPIalone    : num [1:42178] 0.01093 0.01052 0 0.00959 0 ...\n $ pctunder18      : num [1:42178] 0.218 0.224 0.249 0.27 0.245 ...\n $ pctover64       : num [1:42178] 0.124 0.175 0.149 0.122 0.156 ...\n $ HSorlower       : num [1:42178] 0.581 0.464 0.544 0.45 0.621 ...\n $ BAorhigher      : num [1:42178] 0.162 0.219 0.113 0.229 0.136 ...\n $ Metro           : num [1:42178] 1 1 1 1 1 1 1 1 1 1 ...\n $ Micro           : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n $ NoCBSAType      : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n\n\nWe get a list of the columns in the data, along with their types (in this case character or numeric), and then we see the values associated with the first few observations.\nA few things to note after your preliminary inspection:\n\nThese data are at the census tract level and include geographic identifiers including geoid, the combined, state-county-tract FIPS code, state the state name, and county the county name.\nThese data include a field named Designated which is 1 when an eligible tract was designated as an opportunity zone, and NA where the tract was not designated.\nThe dataset also includes some other tract-level demographic measures, as well as additional geographic flags."
  },
  {
    "objectID": "howto/rbasics_02.html#describing-the-data",
    "href": "howto/rbasics_02.html#describing-the-data",
    "title": "Lesson 2: Manipulating Data Frames",
    "section": "Describing the Data",
    "text": "Describing the Data\nWhen we load a new dataset that we’re not familiar with, it’s a good idea to spend a few minutes describing the data. This allows us to understand a bit more about it’s structure and may also help us ask some basic questions about the validity and reliability of the data (at least compared to what we are expecting to see). R has several functions for determining the structure of data frames and tibbles. See below:\nSize\n\n\ndim(ozs): returns a vector with the number of rows in the first element, and the number of columns as the second element (the dimensions of the object)\n\n\ndim(ozs)\n\n[1] 42178    27\n\n\n\n\nnrow(ozs): returns the number of rows\n\n\nnrow(ozs)\n\n[1] 42178\n\n\n\n\nncol(ozs): returns the number of columns\n\n\nncol(ozs)\n\n[1] 27\n\n\nContent\n\n\nhead(ozs): shows the first 6 rows\n\n\nhead(ozs)\n\n# A tibble: 6 × 27\n  geoid state DesignatedOZ county Type  dec_score SE_Flag Population medhhincome\n  &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 0100… Alab…           NA Autau… Low-…         4      NA       2196       41107\n2 0100… Alab…           NA Autau… Non-…         6      NA       3136       51250\n3 0100… Alab…            1 Autau… Low-…         8      NA       3047       45234\n4 0100… Alab…           NA Autau… Non-…        10      NA      10743       61242\n5 0100… Alab…           NA Autau… Non-…         5      NA       2899       49567\n6 0100… Alab…           NA Autau… Low-…         6      NA       3247       40801\n# ℹ 18 more variables: PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;, medvalue &lt;dbl&gt;,\n#   medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;, vacancyrate &lt;dbl&gt;,\n#   pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;, pctAAPIalone &lt;dbl&gt;,\n#   pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;, BAorhigher &lt;dbl&gt;,\n#   Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\n\n\ntail(ozs): shows the last 6 rows\n\n\ntail(ozs)\n\n# A tibble: 6 × 27\n  geoid state DesignatedOZ county Type  dec_score SE_Flag Population medhhincome\n  &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 7803… &lt;NA&gt;            NA &lt;NA&gt;   Non-…        NA      NA         NA          NA\n2 7803… Virg…            1 &lt;NA&gt;   Non-…        NA      NA         NA          NA\n3 7803… Virg…            1 &lt;NA&gt;   Low-…        NA      NA         NA          NA\n4 7803… Virg…            1 &lt;NA&gt;   Low-…        NA      NA         NA          NA\n5 7803… Virg…            1 &lt;NA&gt;   Low-…        NA      NA         NA          NA\n6 7803… Virg…            1 &lt;NA&gt;   Low-…        NA      NA         NA          NA\n# ℹ 18 more variables: PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;, medvalue &lt;dbl&gt;,\n#   medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;, vacancyrate &lt;dbl&gt;,\n#   pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;, pctAAPIalone &lt;dbl&gt;,\n#   pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;, BAorhigher &lt;dbl&gt;,\n#   Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\nNames\n\n\nnames(ozs): returns the column names as a list\n\n\nnames(ozs)\n\n [1] \"geoid\"            \"state\"            \"DesignatedOZ\"     \"county\"          \n [5] \"Type\"             \"dec_score\"        \"SE_Flag\"          \"Population\"      \n [9] \"medhhincome\"      \"PovertyRate\"      \"unemprate\"        \"medvalue\"        \n[13] \"medrent\"          \"pctown\"           \"severerentburden\" \"vacancyrate\"     \n[17] \"pctwhite\"         \"pctBlack\"         \"pctHispanic\"      \"pctAAPIalone\"    \n[21] \"pctunder18\"       \"pctover64\"        \"HSorlower\"        \"BAorhigher\"      \n[25] \"Metro\"            \"Micro\"            \"NoCBSAType\"      \n\n\nSummaries\n\n\nstr(ozs): structure of the object and information about the class, length and content of each column\n\n\nstr(ozs)\n\ntibble [42,178 × 27] (S3: tbl_df/tbl/data.frame)\n $ geoid           : chr [1:42178] \"01001020200\" \"01001020300\" \"01001020700\" \"01001020802\" ...\n $ state           : chr [1:42178] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ DesignatedOZ    : num [1:42178] NA NA 1 NA NA NA NA 1 NA 1 ...\n $ county          : chr [1:42178] \"Autauga County\" \"Autauga County\" \"Autauga County\" \"Autauga County\" ...\n $ Type            : chr [1:42178] \"Low-Income Community\" \"Non-LIC Contiguous\" \"Low-Income Community\" \"Non-LIC Contiguous\" ...\n $ dec_score       : num [1:42178] 4 6 8 10 5 6 6 9 10 9 ...\n $ SE_Flag         : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n $ Population      : num [1:42178] 2196 3136 3047 10743 2899 ...\n $ medhhincome     : num [1:42178] 41107 51250 45234 61242 49567 ...\n $ PovertyRate     : num [1:42178] 0.24 0.107 0.19 0.153 0.151 ...\n $ unemprate       : num [1:42178] 0.0775 0.051 0.1407 0.0459 0.0289 ...\n $ medvalue        : num [1:42178] 95300 113800 93500 160400 102900 ...\n $ medrent         : num [1:42178] 743 817 695 1018 546 ...\n $ pctown          : num [1:42178] 0.628 0.703 0.711 0.823 0.83 ...\n $ severerentburden: num [1:42178] 0.3269 0.3223 0.3887 0.1994 0.0994 ...\n $ vacancyrate     : num [1:42178] 0.0584 0.1399 0.0619 0.0609 0.2182 ...\n $ pctwhite        : num [1:42178] 0.439 0.671 0.833 0.814 0.726 ...\n $ pctBlack        : num [1:42178] 0.5187 0.205 0.0922 0.1572 0.2456 ...\n $ pctHispanic     : num [1:42178] 0.01275 0.0727 0.0338 0.01368 0.00448 ...\n $ pctAAPIalone    : num [1:42178] 0.01093 0.01052 0 0.00959 0 ...\n $ pctunder18      : num [1:42178] 0.218 0.224 0.249 0.27 0.245 ...\n $ pctover64       : num [1:42178] 0.124 0.175 0.149 0.122 0.156 ...\n $ HSorlower       : num [1:42178] 0.581 0.464 0.544 0.45 0.621 ...\n $ BAorhigher      : num [1:42178] 0.162 0.219 0.113 0.229 0.136 ...\n $ Metro           : num [1:42178] 1 1 1 1 1 1 1 1 1 1 ...\n $ Micro           : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n $ NoCBSAType      : num [1:42178] NA NA NA NA NA NA NA NA NA NA ...\n\n\n\n\nsummary(ozs): summary statistics for each column\n\n\nsummary(ozs)\n\n    geoid              state            DesignatedOZ      county         \n Length:42178       Length:42178       Min.   :1       Length:42178      \n Class :character   Class :character   1st Qu.:1       Class :character  \n Mode  :character   Mode  :character   Median :1       Mode  :character  \n                                       Mean   :1                         \n                                       3rd Qu.:1                         \n                                       Max.   :1                         \n                                       NA's   :33414                     \n     Type             dec_score         SE_Flag        Population   \n Length:42178       Min.   : 1.000   Min.   :1       Min.   :    0  \n Class :character   1st Qu.: 3.000   1st Qu.:1       1st Qu.: 2752  \n Mode  :character   Median : 5.000   Median :1       Median : 3897  \n                    Mean   : 5.495   Mean   :1       Mean   : 4147  \n                    3rd Qu.: 8.000   3rd Qu.:1       3rd Qu.: 5224  \n                    Max.   :10.000   Max.   :1       Max.   :40616  \n                    NA's   :1239     NA's   :41113   NA's   :112    \n  medhhincome      PovertyRate       unemprate          medvalue      \n Min.   :  2499   Min.   :0.0000   Min.   :0.00000   Min.   :   9999  \n 1st Qu.: 32014   1st Qu.:0.1381   1st Qu.:0.05900   1st Qu.:  85700  \n Median : 41094   Median :0.2055   Median :0.08735   Median : 122400  \n Mean   : 42153   Mean   :0.2331   Mean   :0.10063   Mean   : 165663  \n 3rd Qu.: 50833   3rd Qu.:0.2996   3rd Qu.:0.12600   3rd Qu.: 191300  \n Max.   :181406   Max.   :1.0000   Max.   :1.00000   Max.   :2000001  \n NA's   :249      NA's   :141      NA's   :141       NA's   :1106     \n    medrent           pctown       severerentburden  vacancyrate     \n Min.   :  99.0   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.: 655.0   1st Qu.:0.3833   1st Qu.:0.1662   1st Qu.:0.07116  \n Median : 800.0   Median :0.5728   Median :0.2403   Median :0.11661  \n Mean   : 860.9   Mean   :0.5436   Mean   :0.2476   Mean   :0.14121  \n 3rd Qu.:1010.0   3rd Qu.:0.7316   3rd Qu.:0.3206   3rd Qu.:0.18011  \n Max.   :3501.0   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n NA's   :395      NA's   :1035     NA's   :189      NA's   :167      \n    pctwhite         pctBlack        pctHispanic       pctAAPIalone    \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.2039   1st Qu.:0.01071   1st Qu.:0.02603   1st Qu.:0.00000  \n Median :0.5614   Median :0.06655   Median :0.09304   Median :0.00883  \n Mean   :0.5210   Mean   :0.18651   Mean   :0.22063   Mean   :0.03806  \n 3rd Qu.:0.8294   3rd Qu.:0.24998   3rd Qu.:0.32018   3rd Qu.:0.03532  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :0.91144  \n NA's   :131      NA's   :131       NA's   :131       NA's   :131      \n   pctunder18       pctover64         HSorlower        BAorhigher    \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.1908   1st Qu.:0.09436   1st Qu.:0.4150   1st Qu.:0.1120  \n Median :0.2300   Median :0.13604   Median :0.5182   Median :0.1680  \n Mean   :0.2295   Mean   :0.14340   Mean   :0.5067   Mean   :0.2035  \n 3rd Qu.:0.2719   3rd Qu.:0.18057   3rd Qu.:0.6113   3rd Qu.:0.2536  \n Max.   :0.6468   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n NA's   :131      NA's   :131       NA's   :132      NA's   :132     \n     Metro          Micro         NoCBSAType   \n Min.   :1      Min.   :1       Min.   :1      \n 1st Qu.:1      1st Qu.:1       1st Qu.:1      \n Median :1      Median :1       Median :1      \n Mean   :1      Mean   :1       Mean   :1      \n 3rd Qu.:1      3rd Qu.:1       3rd Qu.:1      \n Max.   :1      Max.   :1       Max.   :1      \n NA's   :9111   NA's   :37450   NA's   :37795  \n\n\n\n\n\n\n\n\nYour Turn!\n\n\n\nTry your hand at some of these summarization methods to see what they produce.\n\n\nSelective Summaries\nHow would we run summaries just for population, median household income, and poverty rate (think back to how we created subsets using lists)?\n\nsummary(ozs[, c(\"Population\", \"medhhincome\", \"PovertyRate\")])\n\n   Population     medhhincome      PovertyRate    \n Min.   :    0   Min.   :  2499   Min.   :0.0000  \n 1st Qu.: 2752   1st Qu.: 32014   1st Qu.:0.1381  \n Median : 3897   Median : 41094   Median :0.2055  \n Mean   : 4147   Mean   : 42153   Mean   :0.2331  \n 3rd Qu.: 5224   3rd Qu.: 50833   3rd Qu.:0.2996  \n Max.   :40616   Max.   :181406   Max.   :1.0000  \n NA's   :112     NA's   :249      NA's   :141     \n\n\n\n\nYour Turn!\nSolution\n\n\n\nPractice your querying skills - how would we return only those records for census tracts with a median household income above $100,000 per year?\n\n\n\nozs[ozs$medhhincome&gt;=100000,]\n\n# A tibble: 399 × 27\n   geoid       state   DesignatedOZ county    Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 2 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 3 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 4 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 5 04013815600 Arizona           NA Maricopa… Non-…        10      NA       5762\n 6 04013816800 Arizona           NA Maricopa… Non-…        10       1       3727\n 7 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 8 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n 9 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n10 &lt;NA&gt;        &lt;NA&gt;              NA &lt;NA&gt;      &lt;NA&gt;         NA      NA         NA\n# ℹ 389 more rows\n# ℹ 19 more variables: medhhincome &lt;dbl&gt;, PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;,\n#   medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;,\n#   vacancyrate &lt;dbl&gt;, pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;,\n#   pctAAPIalone &lt;dbl&gt;, pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;,\n#   BAorhigher &lt;dbl&gt;, Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\n\n\n\nTracts in Illinois\nOftentimes, we’ll want to query out a subset of observations based upon their geographic location. Let’s try selecting all tracts in Illinois based upon their designation status.\n\n\nYour Turn!\nSolution\n\n\n\nHow would we query out tracts in Illinois? Experiment in your own script.\n\n\n\nozs[ozs$state == \"Illinois\",]\n\n# A tibble: 1,682 × 27\n   geoid       state    DesignatedOZ county   Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 17001000201 Illinois           NA Adams C… Non-…         7      NA       1937\n 2 17001000202 Illinois           NA Adams C… Low-…         1      NA       2563\n 3 17001000400 Illinois           NA Adams C… Low-…         1      NA       3403\n 4 17001000500 Illinois           NA Adams C… Low-…         1      NA       2298\n 5 17001000700 Illinois           NA Adams C… Low-…         1      NA       1259\n 6 17001000800 Illinois            1 Adams C… Low-…         1      NA       2700\n 7 17001000900 Illinois           NA Adams C… Low-…         5      NA       2671\n 8 17001010100 Illinois           NA Adams C… Non-…         2      NA       4323\n 9 17001010200 Illinois           NA Adams C… Low-…         2      NA       3436\n10 17001010300 Illinois           NA Adams C… Non-…         8      NA       6038\n# ℹ 1,672 more rows\n# ℹ 19 more variables: medhhincome &lt;dbl&gt;, PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;,\n#   medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;,\n#   vacancyrate &lt;dbl&gt;, pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;,\n#   pctAAPIalone &lt;dbl&gt;, pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;,\n#   BAorhigher &lt;dbl&gt;, Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\n\n\n\nWe can see in our table output that there are 1,682 eligible or designated tracts in Illinois. We could also use the nrow() command to count the number of rows.\n\n\nYour Turn!\nSolution\n\n\n\nTry crafting code that would count the number of rows for Illinois\n\n\n\nnrow(ozs[ozs$state == \"Illinois\",])\n\n[1] 1682\n\n\n\n\n\nGrouped Means\nWe might also want to calculate statistics like averages for subsets. mean() will calculate the mean of a list or column. What’s the average income for tracts with a vacancy rate above 20 percent?\n\n\nYour Turn\nSolution\n\n\n\nWhat’s the average income for tracts with a vacancy rate below 20 percent?\n\n\n\nmean(ozs$medhhincome[ozs$vacancyrate &gt; .2], na.rm=TRUE)\n\n[1] 35375.68\n\nmean(ozs$medhhincome[ozs$vacancyrate &lt; .2], na.rm=TRUE)\n\n[1] 43849.1\n\n\n\n\n\n\n\nYou might need to check out the documentation for mean() in order to return an answer here. R will not calculate the mean if an NA values are present in the vector for which you’ve requested the mean - a good safety feature if you’re expecting all values to be present.\nIn imperfect data like what we’re dealing with, you can instruct R to remove those NAs and find the mean for remaining values. You may also want to make sure you’ve counted the number of NA values so you know what proportion of your data the mean is actually representing.\nSubsetting\nWe might also be interested in combining query criteria. R can make use of logical statements. & is equivalent to AND and | is equivalent to OR. Now give it a go!\n\n\nYour Turn\nSolution\n\n\n\nWhat is the average income for tracts in Illinois with a poverty rate of greater than 20 percent.\n\n\n\nmean(ozs$medhhincome[ozs$state == \"Illinois\" & ozs$PovertyRate &gt; .2], na.rm=TRUE)\n\n[1] 33526\n\n\n\n\n\nTo confirm we got the query correct, it may be useful to have a look at the returned data without calculating the mean:\n\nozs$medhhincome[ozs$state == \"Illinois\" & ozs$PovertyRate &gt; .2]\n\n  [1] 28819 32313 17850 26012 40475 35387 40714 22326 21500 49590 40599 37679\n [13] 26676  7004    NA  7273  5736 38056 38083 33873 49597 22813 30994 49303\n [25] 48125 46364  7234 22688 44800 35904 21107 37823 36164 44460 30406 30682\n [37] 22945 15020 38922 39000 29861 38861 29432 26750 29870 17569 36464 51458\n [49] 64073 39349 40913 45349 51840 48450 40272 36848 39096 35463 43681 29091\n [61] 27931 34671 35286 27018 36412 22647 23700 42316 38571 31696 43895    NA\n [73] 42077 36159 42280 45417 51250 52074 47652 31364 42762 42232 36522 44750\n [85] 46905 39926 37284 36607 36644 48316 40662 38693 49236 47000 44393 43912\n [97] 55741 34375 43607 34340 45208 47344 52063 28558 52833 43717 56250 30503\n[109] 32317 34933 36338 32232 26773 27788 51542 35698 40515 43650 69348 52096\n[121] 28487 37227 31004 33873 31160 35625 34583 23516 43011 28631 28363 22708\n[133] 34030 28311 20405 20620 22289 23264 33508 31029 24914 12036 25921 29306\n[145] 22097 28867 21607 23214 20912 20563 30817 16989 23929 22150 32717 31181\n[157] 17652 21250 20278 20565 58047 45625 71250 61389 23834 21516 25291 27019\n[169] 36250 38054 40511 29120 29446 25099 26234 26375 31216 30205 37013 35764\n[181] 33088 29914 45096 48083 29980 40719 34922 31875 33173 20990 11964 41645\n[193] 11250 42324 11310 10942 23603 24500 19747 18063 21250 26000 34327    NA\n[205]    NA 32500 31602 18808 24178 20250 31938 25346 18859 21250 51875 35636\n[217] 38260 52817 82667 19034 37610 29960 14500 22353 17731 20573 29688 21658\n[229] 33849 20889 23606 15723 18125 19872 28214 19688 42292 32500 21362 22407\n[241] 21727 22306 22619 33125 28984 22371 33281 31008 27353 29125 20944 43341\n[253] 31518 20118 13536 23684 36938 41917 44514 51723 38750 36631 32430 35469\n[265] 43083 26641 39276 26630 26443 39348 33304 37679 32277 34154 33750 35240\n[277] 38857 24604 27073 42684 39500 41625 35089 25956 14556 14273 52485 44706\n[289] 40199 39722 40743 40254 41081 38452 31688 49358 43698 32682 32377 31982\n[301] 33631 21678 20761 22969 26727 26289 32949 27622 31250 26985 27880 25609\n[313] 25855 23906 45000 32331 33509 40054 52879 40901 22422 32083 39942 27712\n[325] 27156 35338 25846 31329 45313 35647 24408 22381 25786 27589 20119 24911\n[337] 22000 34583 27266 19205 33947 26094 25133 24427 36042 42670 22844 27841\n[349] 15485 15139 16862 22163 22972 35476 18875 26070 29844 34609 20431 28472\n[361] 36818 25461 22250 14622 26180 24806 32961 27864 24045 22250 19561 31330\n[373] 39005 32788 35573 35991 31867 51681 47656 50913 40125 43872 57692 61991\n[385] 50726 49116 49518 48011 65357 55523 56975 41051 51699 61806 49268 30532\n[397] 46988 62132 35500 44678 32027 40268 43021 44778 34835 35810 39139 40308\n[409] 40201 38163 36250 47943 46172 39212 40083 39965 28508 22261 45901 44083\n[421] 37941 40099 37885 39534 39766 46728 41196 49572 46739 27861 37374 32196\n[433] 37334 44048 36693 37500 41449 44653 31683 41405 30769 31474 37281 17425\n[445] 22857 23048 22650 41250 20500 17633 23761 30487 36613 37659 43529 39593\n[457] 27214 14722 22242 44722 22956 31188 35577 33220 44592 46533 99375 21456\n[469] 41902 42786 40286 35179 32500 34231 58500 50521 51442 23750 37179 31238\n[481] 30161 38883 32204 20898 26071 21827 19054 26514 37723 19207 23667    NA\n[493] 55813 18627 52717 25699 52308 30946 28806 43203 26319 13429 26023 27283\n[505] 33250 38750 56908 17377 17370 38382 43516 13281 20294 19228 84780 29831\n[517] 22279 42200 33942 45694 37891 31389 30759 50417 27185 32315 43409 25694\n[529] 19000 22446 18582 32895 35528 39652  9485 35041 21053 17197 28000 42254\n[541] 37353 24886 35875 31788 38060 27012 43780 74038 23302 16844 15242 54444\n[553] 52368 38551 39306 34028 34375 41646 37772 39049 43156 36469 24922 28448\n[565] 31907 38279 30656 35469 54293 43860 31845 42463 34167 46731 24886 29831\n[577] 37917 10127 13101 15671 22108 33514 18411 28791 15000 47733 54483 44872\n[589] 35329 40491 47273 36543 47908 49904 44732 49091 47981 38281 46639 49101\n[601] 36830 26627 30705 26406 24423 21488 42832 23569 36806 41053 17938 22813\n[613] 33579 23011 34970 38380 36824 60630 38188 36127 48339 22500 29408 37883\n[625] 51121 22312 30547 34063 32134 52981 37361 45333 46020 32725 26861 45385\n[637] 38625 35123 33250 41824 21739    NA 37443 33056 47813 55861 11053 37266\n[649] 62966 37356 26224 40188 42537 51354 29479 26250 31250 40556 21613 26103\n[661] 26667 14434 17577 28866 32083 48706 35191 39013 41739 23222 26432 25377\n[673] 32083 55313 16010 45078 34382 22121 21701 41958 37261 44291 36618 40408\n[685] 39514 20000 39792 30735 74554 36917 25781 37875 44094 29304 28125 24932\n[697] 35179 32198 33144 31446 38144 30464 34314 46034 12474 29773 22121 21582\n[709] 26591 12247 16337 18815 27599 27225 29762 44844 67000 31653 35291 41462\n[721] 45929 20942 31635 42583 32396 28884 36902 24671 40298 38750 29677 36979\n[733] 41037 32434 42917 42344 20741 36960 21969 25463  9836 17120 15394 28065\n[745] 21849 24095 29875 41964 37750 28077 37357 34449 19650 27019 29250 33240\n[757] 25134 29779 24486  9967 15036 27005 16353 20439 35398 28750 63750 36333\n[769] 32188 38750 41886 30930 25994 38480 28294 20500 16667 31716 16750 16513\n[781] 17500 24821 41966 31002 23214 32956 54387 32697 32260 52788 52768 25078\n[793] 23371 29978 40506 19145 27165 32034 23514 43007 31639 33981 40532 39006\n[805] 34708 37185 31250 37414 51767 31772 34732 31674 37386 27448 19518 22899\n[817] 50052 40212 37792 40676 45962 35028 56190 38083 36794 30932 46587 27457\n[829] 18983 14731 23594 34420 36053 27566 28004 14853 40737 20000 27397 19167\n[841] 25791 26979 12816 21091 21330 27432 31477 34939 44792 35660    NA    NA\n[853]    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n[865]    NA    NA    NA    NA    NA    NA    NA    NA    NA\n\n\nDealing with Missing Values\nYou’ll note that there are several flag variables in the data for which values are either 1 or NA. We have flags for whether a tract was designated as an Opportunity Zone, Whether it is located in a Metropolitan, Micropolitan, or Non-Core-Based Statistical Area.\nWe can use logical tests in R to identify those values that are NA. We can use is.na() to test whether a value is NA (TRUE) or is not NA (FALSE). We could also use the negation sign ! to determine whether a value is not NA (!is.na()).\nThe following code returns a vector of logical values (TRUE / FALSE) regarding whether the value for the Designated column is NA or not.\n\nis.na(ozs$Designated)\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\nlogical(0)\n\n\nFor logical values, R codes 1 as TRUE and 0 as false, meaning if we wanted to count the number of undesginated tracts, we could ask for the sum of the values for which the logical test is true (the sum of the values that are NA):\n\nsum(is.na(ozs$Designated))\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] 0\n\n\n33,414 tracts were not designated.\n\n\nYour Turn\nSolution\n\n\n\nNow count the number of tracts that were designated (where the value is not NA).\n\n\n\nsum(!is.na(ozs$Designated))\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] 0\n\n\n\n\n\nWe might also want to recode those NA values to something else. We can use assignment and subset notation to replace na values with something else. Let’s replace those NAs in the Designated column with 0.\n\nozs$DesignatedOZ[is.na(ozs$DesignatedOZ)]&lt;-0\nozs\n\n# A tibble: 42,178 × 27\n   geoid       state   DesignatedOZ county    Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 01001020200 Alabama            0 Autauga … Low-…         4      NA       2196\n 2 01001020300 Alabama            0 Autauga … Non-…         6      NA       3136\n 3 01001020700 Alabama            1 Autauga … Low-…         8      NA       3047\n 4 01001020802 Alabama            0 Autauga … Non-…        10      NA      10743\n 5 01001021000 Alabama            0 Autauga … Non-…         5      NA       2899\n 6 01001021100 Alabama            0 Autauga … Low-…         6      NA       3247\n 7 01003010100 Alabama            0 Baldwin … Non-…         6      NA       4013\n 8 01003010200 Alabama            1 Baldwin … Low-…         9      NA       3067\n 9 01003010300 Alabama            0 Baldwin … Non-…        10      NA       8079\n10 01003010400 Alabama            1 Baldwin … Non-…         9      NA       4578\n# ℹ 42,168 more rows\n# ℹ 19 more variables: medhhincome &lt;dbl&gt;, PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;,\n#   medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;,\n#   vacancyrate &lt;dbl&gt;, pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;,\n#   pctAAPIalone &lt;dbl&gt;, pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;,\n#   BAorhigher &lt;dbl&gt;, Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\nCan you inspect the table here to see what happened? In plain language, we told R “for those values of the column named Designated in the ozs data table where the values are NA, assign a new value of 0.”\n\n\nYour Turn\nSolution\n\n\n\nGo ahead and do the same thing for the Metro, Micro, and NoCBSAType columns.\n\n\n\nozs$Metro[is.na(ozs$Metro)]&lt;-0\nozs$Micro[is.na(ozs$Micro)]&lt;-0\nozs$NoCBSAType[is.na(ozs$NoCBSAType)]&lt;-0"
  },
  {
    "objectID": "howto/rbasics_02.html#independent-exploration",
    "href": "howto/rbasics_02.html#independent-exploration",
    "title": "Lesson 2: Manipulating Data Frames",
    "section": "Independent Exploration:",
    "text": "Independent Exploration:\nNow answer the following questions:\n\n\nYour Turn\nSolution\n\n\n\nReport average poverty rates for designated opportunity zones in metropolitan, micropolitan, and non-CBSA areas\n\n\n\n# Your Work Here\nmean(ozs$PovertyRate[ozs$Designated ==1 & ozs$Metro == 1], na.rm=TRUE)\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] NaN\n\nmean(ozs$PovertyRate[ozs$Designated ==1 & ozs$Micro == 1], na.rm=TRUE)\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] NaN\n\nmean(ozs$PovertyRate[ozs$Designated ==1 & ozs$NoCBSAType == 1], na.rm=TRUE)\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] NaN\n\n\n\n\n\n\n\nYour Turn\nSolution\n\n\n\nFor Illinois, how different are the average vacancy rates for designated and undesignated census tracts?\n\n\n\nmean(ozs$vacancyrate[ozs$state == \"Illinois\" & ozs$Designated == 1])\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] NaN\n\nmean(ozs$vacancyrate[ozs$state == \"Illinois\" & ozs$Designated == 0], na.rm=TRUE)\n\nWarning: Unknown or uninitialised column: `Designated`.\n\n\n[1] NaN\n\n\n\n\n\nThink of another question you’d like to ask of these data - write it down and work the problem out."
  },
  {
    "objectID": "howto/index.html",
    "href": "howto/index.html",
    "title": "How To",
    "section": "",
    "text": "R is an open source programming language that is a common tool used for data analysis across a range of disciplines. This means that in addition to being free and available for a range of operating systems and environments, R is also directly supported by a diverse user community who continually develop approaches for specialized applications or data. Need to download U.S. Census Data? There’s an R package for that. Need to perform common data cleaning tasks? There’s an R package for that too. We’ll be exploring a range of these specialized applications over the course of the semester.\nOf course there are alternate languages which we could employ in service of neighborhood analysis. Python, for example, is an even more ubiquitous programming language with its own set of tools for data science. R was originally built as a statistical computing language, and that brings some important benefits for the types of data science we’ll be learning this semester. R is also fairly prevalent among the user community working in public policy analysis and urban data science - this is the user community which you will be joining. Finally, R has a high learning curve, but also a very active user community, meaning that abundant documentation of problems and their solutions is available.\nAs we get started, let’s be clear - you are going to experience some frustration and challenges as you learn the R programming language. This class assumes no prior background in R or any other programming language for that matter, and we’ll work to quickly build your “vocabulary” and the ability to get results. We will spend some time picking up basics, and will then use our exploration of specific analysis approaches to reinforce our use of the grammar and structure of the language and to build more complex scripts over time.\nIt’s fair to equate learning R with learning to drive a manual car. Increasingly, people learn how to drive in automatic cars - essentially allowing the car to handle the function of switching gears - you put the car into drive, press the accelerator pedal and the car moves forward. Your past exposure to computer-based analytic tools has probably followed a similar strategy - you likely learned using software that had graphical user interfaces that allow them to call up and run programs and then spit out results. Most of us learn to point and click in order to accomplish a particular set of analytic tasks, meaning that if we want to generate the same results in the future, we would have to repeat all of those same steps over again.\nSo why learn on a manual? For many car enthusiasts, manuals are both more efficient and more engaging to drive - they offer additional control, and come with a heightened awareness of what the car is doing. Of course, they also come with a steep learning curve.\nSome of the same attributes apply to our use of R as a tool for analysis:\n\nFirst, a manual approach forces you to explicitly understand more of the requirements and assumptions that go into the analysis that you’re doing.\nSecond, you have to know your data (and its strengths and limitations) well in order to get it set up for analysis and to produce useful output.\nThird, this approach emphasizes reproducible analysis, meaning that you will develop workflows that can be repeated over and over again producing the same results - important for sharing your work with others and for accountability, especially within contexts related to public deliberation of the strengths and weaknesses of policy arguments.\n\nKeeping these three benefits in mind, we’ll be using the R software coupled with the RStudio Integrated Development Environment (IDE) to counter some of the downsides of a purely manual approach. RStudio will help us implement R code more effectively and efficiently. Hopefully at this point, this prospect leaves you excited rather than daunted!"
  },
  {
    "objectID": "howto/index.html#introduction",
    "href": "howto/index.html#introduction",
    "title": "How To",
    "section": "",
    "text": "R is an open source programming language that is a common tool used for data analysis across a range of disciplines. This means that in addition to being free and available for a range of operating systems and environments, R is also directly supported by a diverse user community who continually develop approaches for specialized applications or data. Need to download U.S. Census Data? There’s an R package for that. Need to perform common data cleaning tasks? There’s an R package for that too. We’ll be exploring a range of these specialized applications over the course of the semester.\nOf course there are alternate languages which we could employ in service of neighborhood analysis. Python, for example, is an even more ubiquitous programming language with its own set of tools for data science. R was originally built as a statistical computing language, and that brings some important benefits for the types of data science we’ll be learning this semester. R is also fairly prevalent among the user community working in public policy analysis and urban data science - this is the user community which you will be joining. Finally, R has a high learning curve, but also a very active user community, meaning that abundant documentation of problems and their solutions is available.\nAs we get started, let’s be clear - you are going to experience some frustration and challenges as you learn the R programming language. This class assumes no prior background in R or any other programming language for that matter, and we’ll work to quickly build your “vocabulary” and the ability to get results. We will spend some time picking up basics, and will then use our exploration of specific analysis approaches to reinforce our use of the grammar and structure of the language and to build more complex scripts over time.\nIt’s fair to equate learning R with learning to drive a manual car. Increasingly, people learn how to drive in automatic cars - essentially allowing the car to handle the function of switching gears - you put the car into drive, press the accelerator pedal and the car moves forward. Your past exposure to computer-based analytic tools has probably followed a similar strategy - you likely learned using software that had graphical user interfaces that allow them to call up and run programs and then spit out results. Most of us learn to point and click in order to accomplish a particular set of analytic tasks, meaning that if we want to generate the same results in the future, we would have to repeat all of those same steps over again.\nSo why learn on a manual? For many car enthusiasts, manuals are both more efficient and more engaging to drive - they offer additional control, and come with a heightened awareness of what the car is doing. Of course, they also come with a steep learning curve.\nSome of the same attributes apply to our use of R as a tool for analysis:\n\nFirst, a manual approach forces you to explicitly understand more of the requirements and assumptions that go into the analysis that you’re doing.\nSecond, you have to know your data (and its strengths and limitations) well in order to get it set up for analysis and to produce useful output.\nThird, this approach emphasizes reproducible analysis, meaning that you will develop workflows that can be repeated over and over again producing the same results - important for sharing your work with others and for accountability, especially within contexts related to public deliberation of the strengths and weaknesses of policy arguments.\n\nKeeping these three benefits in mind, we’ll be using the R software coupled with the RStudio Integrated Development Environment (IDE) to counter some of the downsides of a purely manual approach. RStudio will help us implement R code more effectively and efficiently. Hopefully at this point, this prospect leaves you excited rather than daunted!"
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html",
    "href": "howto/PWBTN/PWBTN.html",
    "title": "Communicating Quantitative Information",
    "section": "",
    "text": "This document draws heavily from Dr. Ed. Feser’s Professional Writing by the Numbers: For Planners and Policy Analysts, Version 4.0 (2006). Updated content focuses on digital-first communication strategies."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#basic-communication-strategies",
    "href": "howto/PWBTN/PWBTN.html#basic-communication-strategies",
    "title": "Communicating Quantitative Information",
    "section": "Basic Communication Strategies",
    "text": "Basic Communication Strategies\n\nMemo Format\nMemos are often used by urban planners to share information within their agency or with other planners and local government officials. Memos tend to be concise documents that may contain either requested information for staff or recommendations on how to debate or take action around a particular policy issue. Memos are built around clear and concise writing with a logical structure, clear illustrations and visualizations, proper formatting and spelling.\nMemos start with a heading that includes the name of the memo author, name of the intended recipient, date, and a descriptive subject heading:\nMemorandum\n\nTo: Janet Jacobs, Planning Director\nFrom: Mae Q. Plannington, Planner 1\nDate: January 7, 2024\nSubject: Recommendation on Zoning Change for Mumford Parcel\nA memo is typically written in response to something. The first paragraph of the memo should describe what the memo is responding to, and should summarize what the writer of the memo has done to prepare the response. The first paragraph should also summarize any key findings or recommendations.\nInformation within a memo should be organized in a logical fashion. Section headings with descriptive titles help your audience find particular information more easily.\nGeneral information should be presented first followed by more detailed information. As you support information with evidence, you should follow the same progression of general to specific.\nMemos should conclude with a summary statement that encapsulates the key findings which you have come up with. If you are providing recommendations in your memo, this is also an appropriate place to re-state your recommendations, and provide instructions for how your memo recipient should follow up.\nBecause a memo is designed to be a concise document, you may have important additional information which would be useful to share, but which does not fit within the scope of the memorandum. Such items can be included in an appendix or attachment to the memo, and should be referenced as appropriate within the memo (e.g. “See the attached document for the property surveyor’s description of the Mumford parcel”).\n\n\nShort Report Format\nPresent your analysis in the format of a short report or “data brief.”\n\n\nDo Not Include Title Page\nTitle pages have their place in academic papers and larger, formal technical reports. However, they are inappropriate for memos and short data or policy briefs. Keep it simple.\n\n\nAvoid Headers and Footers in Memos\nRunning footers and headers also have their place for certain types of documents, but not for memos.\n\n\nNo Conclusion Needed\nIn other contexts, you have likely been taught to include both introductions and conclusions as ways to help transition in and out of different sections. The memo format is more direct, and does not require the use of opening and closing paragraphs. State you case and let it go at that.\n\n\nNotes and Citations\n\nUse Notes for Technical Explanations\nFootnotes or endnotes are best for stating brief technical explanations for methods and data. Reports may require a longer methodological appendix, but this is typically not appropriate for a memorandum.\n\n\nUse Notes for Citations\nUse an endnote or footnote to cite sources for memos and report briefs. Bibliographies or “Works Cited” lists are appropriate for longer reports or academic papers.\n\n\nCombine Data Source Citations\nIt makes good sense to use endnotes or footnotes to discuss data used in an analysis in a memo or report brief. But you do not necessarily need to use a separate note for each data source or series. Instead, combine them by introducing one note early in the document. Something like: “The data used in this analysis are from the U.S. Census Bureau’s 2020 5-year American Community Survey and 2020 Census Household Pulse Survey”.\n\n\n\nFormatting Considerations for Printing\nIncreasingly, memos and other routine planning communications are communicated digitally, however, it is important to prepare documents that will be legible when printed. In most circumstances, formatting considerations for printing will also apply to documents intended to be communicated digitally.\n\nLeft Justify Text\nUse left justification for text, and not full justification.\n\n\nInclude Page Numbers\nInclude page numbers in the bottom center or bottom right of your page.\n\n\nOne Inch Margins\nUse one inch margins on the left, right, top and bottom, including on all pages with tables and figures.\n\n\nTwelve point font\nUse a twelve point font, preferably something like Times for your text. You can use a ten point font for footnotes and endnotes. No text should be smaller than ten points."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#style",
    "href": "howto/PWBTN/PWBTN.html#style",
    "title": "Communicating Quantitative Information",
    "section": "Style",
    "text": "Style\n\nAwkward Sentence Construction\nA catch-all category for a painful-to-read but not necessarily grammatically incorrect sentence. Sentence may be an affront to good writing style. Admittedly, professors often put AWK in places where they know something is amiss, but not being writing instructors, they’re not sure technically how to explain what is wrong. Bottom line: the sentence doesn’t work. Proofread yourself and then have someone else read to ensure clarity.\n\n\nWrite Directly\nDon’t mince words: come out and say it. Instead of “The BLS and BEA data show that employment in the microchip industry is very small,” write “Employment in the microchip industry is very small.” Or, instead of “It is very important to note that data disclosure rules preclude reporting employment for all sectors,” write “Data disclosure rules preclude. . .”\n\n\nAvoid Excessive Use of Jargon\nAvoid the planner’s pitfall: too much use of all those fancy planning terms and acronyms. It’s not impressive or erudite, just boring.\n\n\nAvoid Passive Voice\nThe extensive use of passive voice may be the single most common reason why a majority of sane individuals would rather walk on hot coals than read a technical document. Doesn’t “The region’s population growth dramatically outpaced the nation’s over the study period” sound better than “Using population data, it was found that the region grew much faster than the US over the study period”?\nAnother example: replace “A three-part analysis will be conducted in this paper” with “This paper will present a three-part analysis” or “In this paper, I will conduct a three-part analysis.” Reject passivity. Be active.\n\n\nUse Active Headings and Subheadings\nUse headings and subheadings to help organize your findings and discussion. In addition, keep ’em active and efficient. Instead of “Location Quotient Analysis” as a sub-heading, try a short title that conveys findings, e.g., “Region Specialized in Manufacturing and Services.” Headings and subheadings should never extend beyond one line of text.\n\n\nAvoid Unnecessary Equivocation\nSometimes it makes sense to offer caveats or otherwise “hedge your bets” when discussing a finding. However, don’t overdo it. Population growth doesn’t “seem fast.” It either is or is not fast, relative to something else (which you should be comparing it to). A location quotient indicates whether a region is or is not specialized in a given industry; it does not indicate that the region “seems specialized” or “may be specialized.” On the other hand, location quotients do not say much beyond specialization (you would not damn the torpedoes and argue, on the basis of a manufacturing LQ above 1.2, that the region is “highly competitive” in manufacturing).\nIf you find yourself making equivocations, it’s worthwhile to revisit the evidence you’ve provided to make a point to ensure you’re making the strongest argument possible. Communicate that argument confidently, and provide appropriate caveats when necessary.\n\n\nIn Memos, Don’t Stay What You’re Going to Say\nIn academic papers or longer reports it often makes sense to provide a roadmap to the document (e.g., “This report begins by summarizing major trends in population. It then. . .”). The adage that you should “say what you are going to say, say it, then say what you said” doesn’t apply for short policy and analysis pieces. Just get on with the analysis and findings. Strong organization and active headings will help your reader infer how you’ll make your points.\n\n\nPlaces Don’t Have Agency\nIn discussing social, economic, and demographic trends for neighborhoods, cities and regions, avoid implying that places have “agency.” Example:\nEl Paso shifted its population mix in response to major changes in Federal immigration policy.\nIn this case, El Paso as a collective, is being treated as an actor when it is actually simply a place with a collection of actors (people, households, businesses, organizations) who are reacting to the federal policy change in various ways.\nIt would be more accurate to say something like this:\nEl Paso’s population mix shifted in response to major changes in Federal immigration policy.\nThere may be cases when implying the place is an actor makes sense, for instance when you are discussing a community-wide strategy or policy.\n\n\nDiscuss Findings, Not Exhibits\nPeople don’t want to read about figures and tables. They want to read about trends that matter from your analysis. Avoid discussing exhibits. Instead, discuss findings, referring to figures and tables as supporting evidence.\nNo: “Figure 1 shows that poverty in center city neighborhoods in Cleveland is increasing.”\nYes: “Poverty in center city Cleveland is increasing (see Table 1).”"
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#usage",
    "href": "howto/PWBTN/PWBTN.html#usage",
    "title": "Communicating Quantitative Information",
    "section": "Usage",
    "text": "Usage\n\nPrint and Proof\nWe increasingly have an imperative to produce, edit, and disseminate our work using a completely electronic workflow. This has its benefits, but you need to develop a workflow that allows you to edit your draft with diligence. Printing and proofing a document by hand can often catch errors that would otherwise become lost in digital copy - such mistakes often reveal themselves in embarrassingly stark relief once disseminated.\n\n\nDefine Acronyms on First Use\nThe first time you use an acronym (e.g., HUD), spell it out, followed by the acronym in parentheses. “Data are from the U.S. Department of Housing and Urban Development (HUD).” Then use the acronym to your heart’s content.\n\n\nWrite out Numbers Less than 11\nAlthough sometimes the convention is that numbers one through ninety-nine should be spelled out. Numerals should be used for rates, percentages and other “data” indicators. Thus we would write “there are eight counties in the MSA,” but “the region grew by 8 percent.”\n\n\nSpell out Numbers Starting Sentences\nAny number that begins a sentence (or a bullet point) should be spelled out.\n\n\nWatch Your Capitalization\nNo need to capitalize industries, occupations, or other sectoral-type categories. Do not over-capitalize. Note that when referring to a single county, write “Tehama County.” But writing about multiple counties, it is “Tehama and Shasta counties.” Also: the “City of New York” but the “cities of Palo Alto and San Jose.”\n\n\nNo Apostrophe on Dates\nWhen referring to decades (e.g., 1990s), do not use an apostrophe.\n\n\nData are Plural\nNo: “The data is hard to find.”\nYes: “The data are hard to find.”\n\n\nUse Proper Note Punctuation\nNote numbers are best placed at the end of a sentence outside the punctuation.\nYes: This is a sentence that requires a citation.1\nNo: This is a sentence that requires a citation2.\n\n\nComplete Sentences in Notes\nFootnotes and endnotes should be complete sentences. Complete sentences have punctuation at the end.\n\n\nPunctuation Inside Quotes\nPunctuation generally goes inside quotes.\nYes: “Run, Tom, run,” said Jane.\nNo: “Down, Spot, down”, said Percival, Dick’s little-known cousin from Topeka.\n\n\nWrite out Percent\nOne of the few times the “less ink is better” rule is violated. Write “8 percent,” not “8%.” It’s less distracting to the eye.\n\n\nUse Arabic Numbers for Notes\nMicrosoft Word often defaults to the use of Roman numerals for notes and endnotes (i, ii, iii. . .). Change this option and use Arabic numerals. More efficient.\n\n\nCiting the URL is Not Enough\nIn the Internet age it has become distressingly common practice to cite only the URL for online documents. But consider this: if you quoted Tolstoy’s War and Peace using a copy you checked out from the New York Public Library, you would not cite the library as the source. The same principle applies to the Internet. For web sites, which are inherently Internet-based, you should list the name of the cite and then the URL. For documents accessed online, you should cite in the usual way (author, date, title, etc.) and then include the URL. Note that you are not obligated to list the URL for freestanding documents if you include the complete citation otherwise.\n\n\nAvoid Ampersand (&) in Text\nThe symbol “&”, known as the ampersand, should not be used in your text write-up. It is ok to use it for labels in tables and figures."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#numbers",
    "href": "howto/PWBTN/PWBTN.html#numbers",
    "title": "Communicating Quantitative Information",
    "section": "Numbers",
    "text": "Numbers\n\nInterpret Quantities by Comparison\nDo not just report growth rates, quantities and other indicators for individual places. They are hard to interpret by themselves. For example, to back up a claim that your region has faced substantial population growth in the last decade, contrast its growth rate with the national average growth rate.\n\n\nExplain Regional Geography\nThe first time you mention your region, explain–either in the text or in a note–what its geographic composition is (e.g., its counties).\n\n\nSignificant Digits\nUse numbers with levels of precision that match the realistic precision in the underlying data. Should a location quotient be expressed as 1.709? No. Round to the nearest tenth (1.7). Percentage growth rates for subnational areas are also usually best expressed with one decimal place. Shares can be converted to percentages and expressed to one decimal place to make them more readable.\n\n\nOnly Include Exhibits You Reference in Text\nNo table, figure, chart or line drawing should be included in the report or report appendix unless it is referenced somewhere in the text discussion. That reference may be very brief (“see Table 4”), but it has to be there (it could also be in an endnote or footnote). Think of it this way: if it wasn’t important enough for you to mention it, why did you include it?\n\n\nReport the Quantity\nSometimes it is easy to forget to report the variable levels when we are analyzing variable trends. For example, it is common in analyses of wage trends (“wages are on the up and up”) to find nary a mention of wage levels. As a reader, you are left wondering “wages are going up, but are they high or low?” It is better to ground an analysis of changes in a given variable with a mention of the levels of the variable. So “The current annual average wage in River City for production workers is $27,500. That is up 12 percent in real terms since 1997. By contrast, the average production worker wage nationwide increased by 16.5 percent. . .”\n\n\nEmphasize Significant Findings\nWhen analyzing data we are looking for the most significant findings and often a “story” that helps explain those findings. In descriptive analysis, significance is often first assessed not in a statistical sense, but by looking for high and low values or major changes. But not all high/low values or large changes are necessarily important from a policy or planning point of view. Be careful to think through the potential implications of a finding before discussing it. Ask yourself: “So what?” If you can’t think of an answer, leave it out. (Example: “Since 1990, River City’s unemployment rate has registered below the U.S. rate in every quarter except IIQ 1996 and IVQ 1998” might be better stated as “River City’s unemployment rate has registered below the U.S. rate in 54 of the last 56 quarters.” A subtle change but the reader is not left asking: “Hmm, I wonder if I’m supposed to know why it was higher in those two periods?”\n\n\nPercent versus Percentage Points\nLet’s say you’re comparing the U.S. unemployment rate of 5.0 percent to the Peoria unemployment rate of 4.0 percent. Is the Peoria rate 1 percent lower than the national rate? No. It is 1 percentage point lower. A Peoria rate 1 percent lower than the national average would be 4.95 percent.\n\n\nRefer to Exhibits in Text\nRefer to your figures and tables directly in your text. Example: “Table 2 reports employment growth figures for the 1995 to 2003 period.” Or: “Employment growth was particularly strong in the retail and construction sectors (see Table 4).” If you are going to send readers to a table in a parenthetical phrase (like the last example), be sure to include the word “see.” So, you would not write: “Employment growth was particularly strong in the retail and construction sectors (Table 4).”"
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#tables-figures-and-graphics",
    "href": "howto/PWBTN/PWBTN.html#tables-figures-and-graphics",
    "title": "Communicating Quantitative Information",
    "section": "Tables, Figures and Graphics",
    "text": "Tables, Figures and Graphics\n\nMake ’Em so they Stand Alone\nFigures, graphs and tables should be constructed so that they can stand alone (as much as feasible). That is, someone could pick up and read the table without the accompanying text and get the gist of what it is trying to say. It goes without saying that sources of all data and calculations should be clearly indicated.\n\n\nAvoid Use of Grids in Tables\nThere is almost never any reason to include gridlines on a data table. Putting a line below the column headings and one below the last row of data, followed by the data source, is usually the best approach.\n\n\nDecimal Justify Data Columns\nLine up columns of numbers in tables on their explicit or implicit decimal points. Do not center justify numeric data.\n\n\nUse Descriptive Column Labels in Tables\nRemember the golden rule in table and figure construction: make it stand alone. That means column headings that someone can understand without reading the report body. Sometimes this is hard to do efficiently (headings can get too long). In such cases an alternative is to use short-hand headings but explain what they mean in a footnote to the table.\n\n\nUse Label Hierarchies in Tables\nA good way to make tables more efficient is to use hierarchical labeling of columns. In the table below, employment and payroll data for Illinois were available in 2003. But national and regional data were available only for 2001. The table efficiently reports the 2003 Illinois numbers along with national and Midwest growth rates for 1990-2001 and location quotients for 2001. A note at the bottom of the table should clarify how the Midwest is defined as well as what the reference area is for the location quotient.\n\n\n\nIndicate Source on Figures and Tables\nData sources should be indicated clearly on all figures and tables.\n\n\nUse Detailed Sources\nFor some series simply listing the data provider agency is not enough. For example, BLS reports several employment series. Therefore, list the series within BLS (or other) that you are using. For example, Regional Economic Information System, US Bureau of Economic Analysis, or Covered Wages and Employment, US Bureau of Labor Statistics.\n\n\nPie Chart? No Thanks\nWhile seductive to the eye, pie charts are far less interpretable than a simple bar chart. Go for conveying your findings clearly, not spicing up your document with spurious graphical devices.\n\n\nShun 3-D\nThree dimensional bar charts and other graphics generally should be avoided. They usually compromise proper interpretation of the findings. Keep it simple: no 3-D.\n\n\nCall Tables “Tables” and Figures “Figures”\nBy convention, tables are called “tables” and graphics such as data charts or line drawings are called “figures.” Sometimes people use the word “exhibit” to label a line drawing. Avoid use of the terms “chart” and “graphic” for labels (e.g., Chart 1, Graphic 1). Also, never call a table a figure or a figure a table. Note that exhibit labels are generally capitalized: “Population trends for River City are summarized in Table 4.”\n\n\nSequence Tables and Figures in Order in the Text\nIf you have three tables, do not discuss Table 1, then Table 3, then Table 2. If the results in Table 3 are mentioned prior to those in Table 2, renumber the tables. Same for figures or any other exhibit.\n\n\nUse Commas for Numbers\nIn data tables, include commas in the number formatting to denote significant digits. So: $47,500 instead of $47500.\n\n\nAvoid Grids on Figures\nMany graphical packages default settings include gridlines on charts. Most charts are simple enough not to need them to be clearly interpreted. Get rid of the clutter and save some ink!\n\n\nDon’t Rely on Color\nRemember that your best intents for people to view your document in color may be foiled - some individuals may print documents in grayscale, and others may be colorblind and unable to perceive all colors. That means your document should not depend on color to convey its findings. An example of depending on color would be to use red text in your tables to indicate negative values or any color to highlight interesting trends.\n\n\nReport Numbers in 000s\nIt is ok to report very large numbers in thousands or even millions in tables if the precision in the original units is unnecessary. It is probably not critical in most cases for people to know that the US population changed by 1,456,789 over a given period (1.46 million will probably suffice). For small areas, such as counties and regions, reporting the original units is usually preferred except for large number variables (e.g., total dollar income).\n\n\nNo Superfluous Material\nAvoid the practice of tacking on government documents or tables to your reports as “general points of information.” The idea in most professional writing is to convey maximum information with minimum material; tossing in nonessential material from secondary sources defeats that aim."
  },
  {
    "objectID": "howto/PWBTN/PWBTN.html#footnotes",
    "href": "howto/PWBTN/PWBTN.html#footnotes",
    "title": "Communicating Quantitative Information",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere’s the citation↩︎\nA citation though the note number is incorrectly placed↩︎"
  },
  {
    "objectID": "assignments/labs/09_transportation_equity/measuring-transportation-equity.html",
    "href": "assignments/labs/09_transportation_equity/measuring-transportation-equity.html",
    "title": "measuring-transportation-equity",
    "section": "",
    "text": "In this lab, we will learn how to use different types of data to explore and visualize access to transportation. This will allow us to identify the spatial disparities due to unequal distribution of transportation infrastructure (e.g. transit stops, transit routes, transit frequency, etc.). We are also able to see the repercussions on commute quality (i.e., trip duration, transportation expenses, etc.)\nIn this lab the main things you will learn are the following:\n\nCalculate and visualize accessibility measure to transportation infrastructure (i.e., bus stops, charging stations, etc)\nCalculate and visualize percentage of public transportation users\nCalculate and visualize average commute times\n\nYou can build on those to see the correlation between income/race/gender and the different transportation equity measures"
  },
  {
    "objectID": "assignments/labs/09_transportation_equity/measuring-transportation-equity.html#transportation-equity",
    "href": "assignments/labs/09_transportation_equity/measuring-transportation-equity.html#transportation-equity",
    "title": "measuring-transportation-equity",
    "section": "",
    "text": "In this lab, we will learn how to use different types of data to explore and visualize access to transportation. This will allow us to identify the spatial disparities due to unequal distribution of transportation infrastructure (e.g. transit stops, transit routes, transit frequency, etc.). We are also able to see the repercussions on commute quality (i.e., trip duration, transportation expenses, etc.)\nIn this lab the main things you will learn are the following:\n\nCalculate and visualize accessibility measure to transportation infrastructure (i.e., bus stops, charging stations, etc)\nCalculate and visualize percentage of public transportation users\nCalculate and visualize average commute times\n\nYou can build on those to see the correlation between income/race/gender and the different transportation equity measures"
  },
  {
    "objectID": "assignments/labs/09_transportation_equity/measuring-transportation-equity.html#access-to-transportation-infrastructure",
    "href": "assignments/labs/09_transportation_equity/measuring-transportation-equity.html#access-to-transportation-infrastructure",
    "title": "measuring-transportation-equity",
    "section": "Access to transportation infrastructure",
    "text": "Access to transportation infrastructure\nIn this example I am using bus stops as a proxy for transportation infrastructure. You can use many other infrastructure types and explore their distribution in relation to socioeconomic and demographics of you neighborhood/region. You can find transit information from GTFS as well as open source portals. For the example I showed during class on electric charging stations, I had to grab this data from a private provider.\nFirst lets start by loading our libraries. By now, you should be familiar with tidycensus, tidyverse, and tigris. I am introducing Simple Futures (sf) for some spatial analysis (i.e., calculating the density of buses per census tract)\n\n# Load required packages\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\n\nLet’s download our shape files for our neighborhood. I am using Cook County as my case study. I am using the census tract level. I have defined a certain type of projection crs=4326 to unify our projection system.\nNow that we have our census tract shapefile, let’s download the bus stops data, read them into a dataset, and project them accordingly.\n\n# Read bus stop data\nbus_stops &lt;- read_csv(\"stops.txt\")\n\n# Convert bus_stops data to an sf object\nbus_stops_sf &lt;- st_as_sf(bus_stops, coords = c(\"stop_lon\", \"stop_lat\"), crs = 4326)\n\nLast data input we need is the socioeconomic of our neighborhood. For the sake of this tutorial I am limiting it to population and household median and income. But please feel free to explore further attribute.\n\n# Specify the ACS variables you want to download\nvariables &lt;- c(\"B01003_001\", # Total population\n               \"B19013_001\") # Median household income\n\n# Download the data for Cook County census tracts\ncook_county_data &lt;- get_acs(geography = \"tract\",\n                            state = \"17\",\n                            county = \"031\",\n                            variables = variables,\n                            year = 2019,\n                            output=\"wide\",\n                            survey = \"acs5\",\n                            geometry = FALSE)\n\nTo make our table more legible, I am renaming our columns and rescaling the median income variable\n\ncook_county_data &lt;- cook_county_data |&gt; \n  rename(name = NAME, pop_tot = B01003_001E, med_income = B19013_001E) |&gt; \n  select(GEOID, name, pop_tot, med_income) \n\n# scale income data so you are able to correlate it with transportation infrastructure\ncook_county_data &lt;- cook_county_data |&gt;  mutate(income_scaled = scale(med_income))\n\nlibrary(gt)\ncook_county_data |&gt; head() |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      GEOID\n      name\n      pop_tot\n      med_income\n      income_scaled\n    \n  \n  \n    17031630200\nCensus Tract 6302, Cook County, Illinois\n1825\n37422\n-0.83877210\n    17031580700\nCensus Tract 5807, Cook County, Illinois\n5908\n47000\n-0.57701469\n    17031590600\nCensus Tract 5906, Cook County, Illinois\n3419\n46033\n-0.60344186\n    17031600700\nCensus Tract 6007, Cook County, Illinois\n2835\n45294\n-0.62363801\n    17031611900\nCensus Tract 6119, Cook County, Illinois\n1639\n24507\n-1.19172646\n    17031804505\nCensus Tract 8045.05, Cook County, Illinois\n3445\n71438\n0.09085204\n  \n  \n  \n\n\n\n\nGreat! Now we have our census demographic data ready. Let’s spend time on preparing our bus stops data. Most counties and cities provide access to bus data through their region’s data portal or official government website. Try typing in google your city’s name along with the abbreviation “gtfs” which stands for General Transit Feed Specification. This file will have the most recent data on public transportation. This is the file that Google Maps as well as other applications rely on to predict our commute times. For example, I typed cook county gtfs in google. I chose the first option and downloaded the stops.text file.\nhttps://www.transitchicago.com/developers/gtfs/\nRemember these are represented in Lat, Long (coordinate system). We have to associate them to the correct census tracts. To do so, we will have to spatially join the bus shapefile to the census tract shapefile. We can then calculate how many bus stops per tract are available.\n\n# buses per census tract\n#sf_use_s2(FALSE)\nbus_stops_per_census_tract &lt;- bus_stops_sf %&gt;%\n  st_join(cook_county_tracts) %&gt;%\n  group_by(TRACTCE) %&gt;%\n  summarise(bus_stop_count = n()) %&gt;%\n  ungroup()\n\nView results\n\nbus_stops_per_census_tract |&gt; head() |&gt; gt()\n\n\n\n\n\n  \n    \n    \n      TRACTCE\n      bus_stop_count\n      geometry\n    \n  \n  \n    010100\n2\nc(-87.6770962, -87.6663015, 42.02289954, 42.01930539)\n    010201\n3\nc(-87.68426462, -87.68054061, -87.6777439, 42.01941115, 42.01936686, 42.01937362)\n    010202\n18\nc(-87.67632528, -87.67580666, -87.67532833, -87.675138, -87.67380984, -87.67305981, -87.67316244, -87.672892, -87.67290599, -87.67281382, -87.67279844, -87.67279597, -87.6724214, -87.67456121, 42.01918517, 42.01798492, 42.01611703, 42.01595, 42.01665359, 42.0180352, 42.01896555, 42.019063, 42.01874262, 42.01850551, 42.01822603, 42.01776729, 42.01744751, 42.01276478)\n    010300\n17\nc(-87.67058707, -87.67103858, -87.6692571266, -87.669092, -87.66932276, -87.6689834, -87.66617692, -87.665802, -87.66551305, -87.665018, -87.66511368, -87.66467448, -87.663375, -87.663994, -87.664145, -87.66469399, 42.01928162, 42.01808861, 42.0160204165, 42.015876, 42.01931247, 42.0190888, 42.01919502, 42.018021, 42.01802283, 42.016268, 42.01580068, 42.01520955, 42.013005, 42.014113, 42.013757, 42.01482404)\n    010400\n9\nc(-87.65693556, -87.65834886, -87.66071682, -87.66060059, -87.660658, -87.660848, -87.661598, -87.661995, -87.662803, 41.99833161, 41.99827386, 42.003871, 42.00038209, 42.001702, 42.00572, 42.008133, 42.009358, 42.01166)\n    010501\n8\nc(-87.66177918, -87.661749, -87.662146, -87.6623874, -87.662952, -87.663526, -87.66563004, -87.66379711, 42.00790263, 42.007777, 42.009002, 42.0092214, 42.011414, 42.012649, 42.0078605, 42.00787643)\n  \n  \n  \n\n\n\n\nNow, lets create the final dataset that includes both the bus data and demographics/socioeconomics of our neighborhood. Use join.\n\n# Merge the count of bus stations per census tract back to the original census tracts shapefile.\ncook_county_tracts_merged &lt;- st_join(cook_county_tracts, bus_stops_per_census_tract, left = FALSE)\n\n# Merge the count of bus stations per census tract back to the original census tracts shapefile.\ncook_county_tracts_merged &lt;- left_join(cook_county_tracts_merged, cook_county_data, by = \"GEOID\")\n\nNow let’s visualize this and see the ratio of population served by buses. Where are they clustered and if there are any inferences we can make.\n\n# Create a map of income distribution.\nggplot() +\n  geom_sf(data = cook_county_tracts_merged, \n          mapping = aes(fill =cut_number((med_income), 5)), \n          color = \"white\", \n          size = 0.1) +\n  scale_fill_brewer(palette = \"PuBu\", \n                     name = \"Median Income\",\n                    na.value = \"gray\")+       \n  labs(title = \"Median Income per Census Tract in Cook County\",\n       caption = \"Source: Census data ACS\") +\n  theme_minimal() +\n  theme(panel.background = element_blank()) +                   \n  coord_sf(datum = NA) \n\n\n\n# Create a map of the bus.\nggplot() +\n  geom_sf(data = cook_county_tracts_merged, \n          mapping = aes(fill =cut_number(((bus_stop_count/pop_tot)*10000), 5)), \n          color = \"white\", \n          size = 0.1) +\n  scale_fill_brewer(palette = \"PuBu\", \n                     name = \"Bus stations normalized per 10,000 people\",\n                    na.value = \"gray\")+       \n  labs(title = \"Distribution of bus stations in Cook County\",\n       caption = \"Source: Census data and Bus Stations data\") +\n  theme_minimal() +\n  theme(panel.background = element_blank()) +                   \n  coord_sf(datum = NA) \n\n\n\n\nNow it’s your turn to explore other demographic attributes and see how they are correlated to various transportation infrastructure."
  },
  {
    "objectID": "assignments/labs/09_transportation_equity/measuring-transportation-equity.html#travel-behavior",
    "href": "assignments/labs/09_transportation_equity/measuring-transportation-equity.html#travel-behavior",
    "title": "measuring-transportation-equity",
    "section": "Travel behavior",
    "text": "Travel behavior\nProviding transportation infrastructure is not sufficient for assuring equitable access to mobility. By investigating travel behavior, one can see the discrepancies in accessibility, as well as the burden that certain communities have to face. In this example, We will look at transportation modes and commute times. To make this more informative, try to explore the relationship between average commute times and race ratios per census tract.\nFor this analysis we will need commute information from ACS and census tract shape files for visualization.\nLet’s first start by identifying the variables we are interested in. I haven’t renamed them, but good practice is to always rename your columns with legible names.\n\nvariables &lt;- c(\"B19013_001\", # Median Household Income\n\"B01003_001\", # Total population\n\"B03002_003\", # White population\n\"B03002_012\", # Hispanic or Latinx population\n\"B03002_004\", # African American population\n\"B08201_001\", # Number of households\n\"B08141_001\", # population using transportation\n\"B08141_006\", # population drive\n\"B08141_011\", # population carpool\n\"B08141_016\", # population transit\n\"B08141_021\", # population walk\n\"B08141_026\", # population other\n\"B08141_031\", # Population working from home \n\"B08303_001\", # Population of commuters to work\n\"B08303_002\", # Population of commuters less than 5 min\n\"B08303_003\", # Population of commuters 5 to 9 minutes\n\"B08303_004\", # Population of commuters 10 to 14 minutes\n\"B08303_005\", # Population of commuters 15 to 19 minutes\n\"B08303_006\", # Population of commuters 20 to 24 minutes \n\"B08303_007\", # Population of commuters 25 to 29 minutes \n\"B08303_008\", # Population of commuters 30 to 34 minutes \n\"B08303_009\", # Population of commuters 35 to 39 minutes \n\"B08303_010\", # Population of commuters 40 to 44 minutes \n\"B08303_011\", # Population of commuters 45 to 59 minutes \n\"B08303_012\", # Population of commuters 60 to 89 minutes\n\"B08303_013\") # Population of commuters 90 or more minutes\n\nLet’s download the data from ACS using tidycensus.\n\ncook_county_data &lt;- get_acs(geography = \"tract\", \n                            variables = variables, \n                            state = \"IL\", \n                            county = \"Cook\",\n                            survey = \"acs5\", \n                            output=\"wide\",\n                            year = 2019)\n\nWe can use the same shapefile from the previous exercise, so there is no need to re download the census tract shapefile. Now let’s join our demographic data to our shapefile using GEOID as our common index.\n\ncook_county_tracts &lt;- tracts(state = \"IL\", county = \"Cook\") |&gt;\n  erase_water(area_threshold = .9, year = 2020) %&gt;% \n  st_transform(crs = 4326)\n\ncook_county_data_sf &lt;- left_join(cook_county_tracts, cook_county_data, by = c(\"GEOID\" = \"GEOID\"))\n\nNow that we have master dataframe with all the data in it. Let’s go ahead and calculate two measures:\n\nPercentage of transit users (pct_public_transit)\nAverage commute time (avg_commute_travel_time). For this measure, I am multiplying each range of commute time by the lowest value in the range (i.e. 5 mins in 5-10mins). I then add all the values and divide them by the total number of commuters.\n\n\ncook_county_data_sf &lt;- cook_county_data_sf %&gt;%\n  mutate(pct_non_white = (B03002_012E+B03002_004E) / B01003_001E * 100,\n         pct_white = (B03002_003E) / B01003_001E * 100,\n         pct_drive = B08141_006E / B08141_001E * 100,\n         pct_public_transit = B08141_016E / B08141_001E * 100,\n         avg_commute_travel_time = ((B08303_002E*1+\n                                      B08303_003E*5+\n                                      B08303_004E*10+\n                                      B08303_005E*15+\n                                      B08303_006E*20+ \n                                      B08303_007E*25+ \n                                      B08303_008E*30+\n                                      B08303_009E*35+ \n                                      B08303_010E*40+ \n                                      B08303_011E*45+\n                                      B08303_012E*60+\n                                      B08303_013E*90)/\n                                      1)/B08303_001E)\n\nSince there are regions with no commuters, we get inf (infinite values). We have to make sure to convert them to NA so we are able to analyse these and visualize them.\n\ncook_county_data_sf$avg_commute_travel_time[sapply(cook_county_data_sf$avg_commute_travel_time, is.infinite)] &lt;- NA\n\n\nModes of transportation\nLets have a look at the public transportation users and where they are located\n\n# Create public trasnportation users map\nggplot() +\n  geom_sf(data = cook_county_data_sf, \n          aes(fill =pct_public_transit),\n          color = \"white\",\n          size = 0.001) +\n  scale_fill_viridis_c(option = \"PuBu\",\n                    na.value = \"gray\")+       \n  labs(title = \"Percentage of Workers Using Public Transportation in Cook County\",\n       caption = \"Source: US Census Bureau ACS 5-Year Estimates, 2019\") +\n  theme_minimal() +\n  theme(panel.background = element_blank()) +                   \n  coord_sf(datum = NA) \n\n\n\n\nLets have a look at the distribution of people using personal vehicles to drive to work and where they are located\n\n# Create drivers map\nggplot() +\n  geom_sf(data = cook_county_data_sf, \n          aes(fill =pct_drive),\n          color = \"white\",\n          size = 0.001) +\n  scale_fill_viridis_c(option = \"PuBu\",\n                    na.value = \"gray\")+      \n  labs(title = \"Percentage of Workers Driving in Cook County\",\n       caption = \"Source: US Census Bureau ACS 5-Year Estimates, 2019\") +\n  theme_minimal() +\n  theme(panel.background = element_blank()) +                   \n  coord_sf(datum = NA) \n\n\n\n\n\n\nCommute Times\nNow let’s have a look at the average commute time in each census tract and how they are spatially distributed.\n\n# Create average commute times map\nggplot() +\n  geom_sf(data = cook_county_data_sf, \n          mapping = aes(fill =cut_number((avg_commute_travel_time), 5)), \n          color = \"white\", \n          size = 0.1) +\n  scale_fill_brewer(palette = \"PuBu\", \n                     name = \"Avergae commute times\",\n                    na.value = \"gray\")+       \n  labs(title = \"Average Commute Times in Cook County\",\n       caption = \"Source: US Census Bureau ACS 5-Year Estimates, 2019\") +\n  theme_minimal() +\n  theme(panel.background = element_blank()) +                   \n  coord_sf(datum = NA) \n\n\n\n\n\n\nRace and travel time\nVisualize the relationship between race and times of commute. We want to see if there is any correlation.\n\nggplot(data=cook_county_data_sf, aes(x=pct_non_white, y =avg_commute_travel_time))+geom_point()+geom_smooth(method = \"lm\")\n\n\n\n\nThe results show that there is a positive relationship between non whites population and increased commute times."
  },
  {
    "objectID": "assignments/labs/09_transportation_equity/measuring-transportation-equity.html#conclusion",
    "href": "assignments/labs/09_transportation_equity/measuring-transportation-equity.html#conclusion",
    "title": "measuring-transportation-equity",
    "section": "Conclusion",
    "text": "Conclusion\nThis labs took us through simple calculations to measure transportation accessibility and the discrepancies in travel times and modes of transportation."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html",
    "title": "Neighborhood Change",
    "section": "",
    "text": "In this lab, we’ll explore in detail the Nathalie P. Voorhees Center’s methodology for measuring neighborhood change. The center has produced multiple analyses and strategies for measuring such change, focused on the Chicago metropolitan area.\nBy the end of this lab, you will replicate elements of the Voorhees Center’s Who Can Live in Chicago? report including maps and other visualizations of neighborhood change. We will develop a code base that could be applied to any location in the United States."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#introduction",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#introduction",
    "title": "Neighborhood Change",
    "section": "",
    "text": "In this lab, we’ll explore in detail the Nathalie P. Voorhees Center’s methodology for measuring neighborhood change. The center has produced multiple analyses and strategies for measuring such change, focused on the Chicago metropolitan area.\nBy the end of this lab, you will replicate elements of the Voorhees Center’s Who Can Live in Chicago? report including maps and other visualizations of neighborhood change. We will develop a code base that could be applied to any location in the United States."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#goals",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#goals",
    "title": "Neighborhood Change",
    "section": "Goals",
    "text": "Goals\nIn this lab, we will use a combination of decennial census data and American Community Survey data to examine patterns of neighborhood income change. We will overlay on these patterns of income change information about the demographics of who lives in these neighborhoods. Our goals for this lab are as follows:\n\nContinue to work with common sources of sociodemographic information, including those that have been pre-processed by sources other than the Census Bureau.\nThink critically about data processing pipelines and data requirements.\nExplore strategies for spatial longitudinal data description and analysis.\n\nTaking a close look at the visuals from the Voorhees Center website, they calculate for each census tract in each year the average per capita income for the time periods 1970, 1980, 1990, 2000, 2010, and 2017 for each census tract and then compare it to the regional weighted average income.\n\nThey classify tracts based upon whether they are:\n\nVery High Income: Greater than 40% above the regional average\nHigh Income: 20% - 40% above the reg ional average\nMiddle Income: 20% below to 20% above the regional average\nLow Income: 20% below to 40% below the regional average\nVery Low Income: Greater than 40% below the regional average\n\nAfter classifying tracts, the study examines change over time, and then examines recent demographic characteristics based upon this change.\n This gives us a sense of what we need to do to prepare our data."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#github-lab-repository",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#github-lab-repository",
    "title": "Neighborhood Change",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#introduction-to-ltdb-data",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#introduction-to-ltdb-data",
    "title": "Neighborhood Change",
    "section": "Introduction to LTDB Data",
    "text": "Introduction to LTDB Data\nAt this point, you have spent time working with census data and common census geographies. We have used the tidycensus package to download census data from the Census Bureau’s servers using API calls.\nIn this lab, we are going to look at another source of historical census data - the Longitudinal Tract Database. Census tracts are a common geography used by planners and other analysts because tracts roughly approximate neighborhoods, especially in highly urbanized areas where population density means that tracts are relatively small. One challenge faced by analysts looking to examine neighborhood level characteristics over time is that the geographic definitions for census tracts change every 10 years following the decennial census. These changes are performed as part of the Census’ Participant Statistical Areas Program (PSAP).\nWhile some tracts may stay the same, some may be split in cases where the population increases, and others may be merged in cases where the population decreases. See this quick reference guide for more information on thresholds for redefinition:\n\nThe Census Bureau produces geographic relationship files which can help show the relationships between geographies over time.\n\n\n\n\n\n\n  \n    \n    \n      GEOID_TRACT_10\n      GEOID_TRACT_20\n      NAMELSAD_TRACT_10\n      NAMELSAD_TRACT_20\n      AREALAND_TRACT_10\n      AREALAND_TRACT_20\n    \n  \n  \n    01001020100\n01001020100\nCensus Tract 201\nCensus Tract 201\n9,827,271\n9,825,304\n    01001020200\n01001020100\nCensus Tract 202\nCensus Tract 201\n3,325,674\n9,825,304\n    01001020200\n01001020200\nCensus Tract 202\nCensus Tract 202\n3,325,674\n3,320,818\n    01001020300\n01001020300\nCensus Tract 203\nCensus Tract 203\n5,349,271\n5,349,271\n    01001020400\n01001020400\nCensus Tract 204\nCensus Tract 204\n6,384,282\n6,384,282\n    01001020500\n01001020501\nCensus Tract 205\nCensus Tract 205.01\n11,408,867\n6,203,654\n  \n  \n  \n\n\n\n\nNote from these example records shifts in tract definitions and areas. These relationship files can help you identify which census tracts have changed, however, creating consistent estimates over time from this information is challenging. Researchers tend to use strategies that either attribute characteristics using either area or share of population to weight how portions of one tract might translate into a new boundary in a different year.\nBrown University’s Spatial Structures in the Social Sciences group has produced a Longitudinal Tract Database which provides demographic estimates for prior decennial census years 1970, 1980, 1990, and 2000 adjusted to 2010 boundaries. The files also include selected demographic indicators that are comparable across each of these decennial census enumerations (the questions are asked in the same or similar enough way to be able to reliably compare these qualities over time).\nLTDB data come from two sources:\n\nDecennial “short form” census data from the 100 percent count of the population. This includes race and ethnicity, age, household composition, and household tenure information.\nDecennial “long form” census data administered to a sample of 1 in 6 households. These surveys contain far more detailed information on the characteristics of the population.\n\nDecennial data continues to be collected, but the 1 in 6 count sample data has now been replaced by the American Community Survey which annually samples the population creating valuable intercensal estimates used by analysts today.\nYou can find more information about the LTDB in the following technical documentation.\nIn order to examine income and demographic change for census tracts, we’re going to use LTDB data, and ultimately will supplement what’s in these data with additional ACS data. We do this so that we can compare estimates over time using the same census tract geographies."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#setup",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#setup",
    "title": "Neighborhood Change",
    "section": "Setup",
    "text": "Setup\nLet’s get set up by loading a few packages:\n\n\nCode\nlibrary(gt)\nlibrary(tidyverse)\n\n\n\nLTDB Data Download\nLet’s start by downloading LTDB data for analysis.\nGo to the data download page. You will need to enter your email address and certify that you will follow terms of use for the data.\nMake note of what datasets are aggregated in the LTDB Standard Datasets. \nDownload both the full and sample standard data set files for all years. Also download the codebook.\nDecompress the downloaded folder containing the data files and place the raw data csv files in your raw data folder. You should have 12 files - six “fullcount” files and six “sample” files (you can ignore those files in the fullcount download with “Global Neighborhood” in their file name).\nPlace the codebook in your data documentation folder.\nNow that we have data and documentation manually downloaded, we can import the data for analysis.\n\n\nData Import\nLet’s import our data. These are csv files - in the past we’ve used read_csv() from the readr package to do this (readr is part of the tidyverse so you don’t need to load it separately).\nFor clarity, use the prefix “F_” for full count data and “S_” for sample data when you read the LTDB into table objects in R. Go ahead and import the ten data files:\n\n\nCode\nF_1970&lt;-read_csv(\"data_raw/LTDB_Std_1970_fullcount.csv\")\nS_1970&lt;-read_csv(\"data_raw/ltdb_std_1970_sample.csv\")\n\nF_1980&lt;-read_csv(\"data_raw/LTDB_Std_1980_fullcount.csv\")\nS_1980&lt;-read_csv(\"data_raw/ltdb_std_1980_sample.csv\")\n\nF_1990&lt;-read_csv(\"data_raw/LTDB_Std_1990_fullcount.csv\")\nS_1990&lt;-read_csv(\"data_raw/ltdb_std_1990_sample.csv\")\n\nF_2000&lt;-read_csv(\"data_raw/LTDB_Std_2000_fullcount.csv\")\nS_2000&lt;-read_csv(\"data_raw/ltdb_std_2000_sample.csv\")\n\nF_2010&lt;-read_csv(\"data_raw/LTDB_Std_2010_fullcount.csv\")\nS_2010&lt;-read_csv(\"data_raw/LTDB_std_200812_Sample.csv\")\n\nF_2020&lt;-read_csv(\"data_raw/LTDB_Std_2020_fullcount.csv\")\nS_2020&lt;-read_csv(\"data_raw/LTDB_std_201519_Sample.csv\")\n\n\nNow that we have all of the data loaded, we can start working on preparing it for analysis."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#data-preparation",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#data-preparation",
    "title": "Neighborhood Change",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nIsolate Income Data\nWe need to start by identifying the variable(s) containing income information. Go into the data codebook and search for income. We have information on median household income, some breakdowns by race and ethnicity, and per-capita income.\nLet’s move forward with the per-capita income information, since this is what is listed in the Voorhees study. We will need to pull that information from each of the data years 1970, 1980, 1990, 2000, 2010 (as proxied by 2008-2012 ACS data, and 2020 (as proxied by 2014 - 2019 ACS data)).\nFor each of the five decades, create a new dataset inc_[year] that contains the 2010 tract identifier (TRTID10), and the income per capita variable (INCPC[year]). For the 2010 data, also include the state identified (statea), the county identified (countya), and the tract identifier (tracta):\n\n\nCode\ninc_70&lt;-S_1970 |&gt; select(TRTID10, INCPC70)\ninc_80&lt;-S_1980 |&gt; select(trtid10, incpc80)\ninc_90&lt;-S_1990 |&gt; select(TRTID10, INCPC90)\ninc_00&lt;-S_2000 |&gt; select(TRTID10, INCPC00)\ninc_10&lt;-S_2010 |&gt; select(tractid, incpc12)\ninc_20&lt;-S_2020 |&gt; select(tractid, statea, countya, tracta, incpc19)\n\n\nNow that we’ve isolated the income per capita data, we can start to think about how we’d join it all together for the purposes of mapping and further analysis.\nOne thing we should immediately note here is that there are different numbers of observations for each year. For instance, inc_20 (observations for 2020) contains 73,056 observations:\n\n\nCode\nnrow(inc_20)\n\n\n[1] 73056\n\n\ninc_70 contains 52,759 observations:\n\n\nCode\nnrow(inc_70)\n\n\n[1] 52759\n\n\nThis should guide our strategy for how we join the data together. Our join strategy should try to keep all records corresponding to the observation years that have more observations (this is due to differences in geographic coverage between each decade).\nAnother thing we may want to handle before we join the data together are to standardize column names. You’ll note from selecting columns and constructing the inc_[year] datasets that there’s different capitalization and naming for some years. We can fix this when we’re selecting our data by specifying [new name] = [column name] in our select calls:\n\n\nCode\ninc_70&lt;-S_1970 |&gt; select(TRTID10, INCPC70)\ninc_80&lt;-S_1980 |&gt; select(TRTID10 = trtid10, incpc80)\ninc_90&lt;-S_1990 |&gt; select(TRTID10, INCPC90)\ninc_00&lt;-S_2000 |&gt; select(TRTID10, INCPC00)\ninc_10&lt;-S_2010 |&gt; select(TRTID10 = tractid, INCPC12 = incpc12)\ninc_20&lt;-S_2020 |&gt; select(TRTID10 = tractid, STATE = statea, COUNTY = countya, TRACT = tracta, INCPC19 = incpc19)\n\n\nWe have another major problem to deal with before we can join these data together. Take a look at the column types using the str() command:\n\n\nCode\nstr(inc_70)\n\n\ntibble [52,759 × 2] (S3: tbl_df/tbl/data.frame)\n $ TRTID10: num [1:52759] 1e+09 1e+09 1e+09 1e+09 1e+09 ...\n $ INCPC70: num [1:52759] 2850 2597 2876 2567 2847 ...\n\n\nCode\nstr(inc_80)\n\n\ntibble [59,187 × 2] (S3: tbl_df/tbl/data.frame)\n $ TRTID10: num [1:59187] 1e+09 1e+09 1e+09 1e+09 1e+09 ...\n $ incpc80: num [1:59187] 0 4648 5804 7668 6339 ...\n\n\nCode\nstr(inc_90)\n\n\ntibble [72,693 × 2] (S3: tbl_df/tbl/data.frame)\n $ TRTID10: num [1:72693] 1e+09 1e+09 1e+09 1e+09 1e+09 ...\n $ INCPC90: num [1:72693] 11663 8555 11782 15323 14492 ...\n\n\nCode\nstr(inc_00)\n\n\ntibble [72,693 × 2] (S3: tbl_df/tbl/data.frame)\n $ TRTID10: num [1:72693] 1e+09 1e+09 1e+09 1e+09 1e+09 ...\n $ INCPC00: num [1:72693] 17771 14217 18346 19741 24492 ...\n\n\nCode\nstr(inc_10)\n\n\ntibble [73,056 × 2] (S3: tbl_df/tbl/data.frame)\n $ TRTID10: chr [1:73056] \"01101005302\" \"01101005402\" \"01101001800\" \"01095030600\" ...\n $ INCPC12: num [1:73056] 18084 18006 24799 21889 16953 ...\n\n\nCode\nstr(inc_20)\n\n\ntibble [73,056 × 5] (S3: tbl_df/tbl/data.frame)\n $ TRTID10: chr [1:73056] \"01073001100\" \"01073001400\" \"01073002000\" \"01073003802\" ...\n $ STATE  : chr [1:73056] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ COUNTY : chr [1:73056] \"Jefferson County\" \"Jefferson County\" \"Jefferson County\" \"Jefferson County\" ...\n $ TRACT  : chr [1:73056] \"Census Tract 11\" \"Census Tract 14\" \"Census Tract 20\" \"Census Tract 38.02\" ...\n $ INCPC19: num [1:73056] 21518 20209 15153 17416 16570 ...\n\n\nSome of our TRTID10 colums are defined as character and others as numeric. Although these FIPS codes look like numbers, they are unique IDs and we want to make sure that they are consistently stored as character vectors. This is also important, because some of the states that come early in the alphabet start with “0” (for instance, Alabama is state “01”). If these import as numeric, that leading zero may be lost. This means we need to go back to our import steps and explicitly specify how read_csv() should import these columns.\nWhen read_csv() initiates the process of reading data, it reads the first several rows of data that then makes an educated guess as to the format to use for each column of data. We can override this and explicitly tell read_csv() what format to use for particular columns using the col_types option. We specify column types by providing an equivalency statement. For instance col_types = cols(TRTID10 = col_character()) would say please let me explicitly define a column type. Where a column is named “TRTID10”, read in this column and assign it’s contents as character. We could explicitly set other types as needed (e.g. col_numeric(), col_boolean(), etc.).\n\n\nCode\nF_1970&lt;-read_csv(\"data_raw/LTDB_Std_1970_fullcount.csv\", col_types = cols(TRTID10=col_character()))\nS_1970&lt;-read_csv(\"data_raw/ltdb_std_1970_sample.csv\", col_types = cols(TRTID10=col_character()))\n\nF_1980&lt;-read_csv(\"data_raw/LTDB_Std_1980_fullcount.csv\", col_types = cols(TRTID10=col_character()))\nS_1980&lt;-read_csv(\"data_raw/ltdb_std_1980_sample.csv\", col_types = cols(trtid10=col_character()))\n\nF_1990&lt;-read_csv(\"data_raw/LTDB_Std_1990_fullcount.csv\", col_types = cols(TRTID10=col_character()))\nS_1990&lt;-read_csv(\"data_raw/ltdb_std_1990_sample.csv\", col_types = cols(TRTID10=col_character()))\n\nF_2000&lt;-read_csv(\"data_raw/LTDB_Std_2000_fullcount.csv\", col_types = cols(TRTID10=col_character()))\nS_2000&lt;-read_csv(\"data_raw/ltdb_std_2000_sample.csv\", col_types = cols(TRTID10=col_character()))\n\nF_2010&lt;-read_csv(\"data_raw/LTDB_Std_2010_fullcount.csv\", col_types = cols(tractid=col_character()))\nS_2010&lt;-read_csv(\"data_raw/LTDB_std_200812_Sample.csv\", col_types = cols(tractid=col_character()))\n\nF_2020&lt;-read_csv(\"data_raw/LTDB_Std_2020_fullcount.csv\", col_types = cols(tractid=col_character()))\nS_2020&lt;-read_csv(\"data_raw/LTDB_std_201519_Sample.csv\", col_types = cols(tractid=col_character()))\n\n\nThen we can go ahead and pull out our income columns as we did before:\n\n\nCode\ninc_70&lt;-S_1970 |&gt; select(TRTID10, INCPC70)\ninc_80&lt;-S_1980 |&gt; select(TRTID10 = trtid10, incpc80)\ninc_90&lt;-S_1990 |&gt; select(TRTID10, INCPC90)\ninc_00&lt;-S_2000 |&gt; select(TRTID10, INCPC00)\ninc_10&lt;-S_2010 |&gt; select(TRTID10 = tractid, INCPC12 = incpc12)\ninc_20&lt;-S_2020 |&gt; select(TRTID10 = tractid, STATE = statea, COUNTY = countya, TRACT = tracta, INCPC19 = incpc19)\n\n\nLet’s run one more check on the TRTID10 columns. Lets chech the number of characters in out TRTID columns. Every combined state-county-tract FIPS code should be 11 characters long:\n\n2 state characters\n3 county characters\n6 tract characters\n\nLets take our inc_70 dataset, group it by the count of the number of characters in each observation in the TRTID10 column (nchar()), and then ciybt the number of observations:\n\n\nCode\ninc_70 |&gt; \n  group_by(nchar(TRTID10)) |&gt; \n  summarise(n())\n\n\n# A tibble: 2 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               10 11375\n2               11 41384\n\n\nHmm - in the 1970 income data, we have 11,375 observations with less than the expected number of characters. Why might this be? This could be an artifact of the data we downloaded or could be a function of how read_csv() is importing these data.\nPast experience tells me that we are missing the leading zero from states with state FIPS codes under 10. We can easily add this back in using the str_pad() command from the stringr package - this package has tools for modifying strings or character vectors.\n\n\nCode\ninc_70&lt;-inc_70 |&gt; \n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\n\n\nThe first part should be straightforward - we’re overwriting the existing inc_70 data with an updated version. We’re using mutate() to alter the TRTID10 column. We’re using str_pad() to “pad” the TRTID10 column. The desired width (number of characters) is 11, we’re going to add our pad to the left side of the existing string, and the pad we’ll add is the character “0”.\nstr_pad() will add however many pad characters we need to get to the desired length. For instance, if we were padding a string “123” with “A” and specified a width of 5, we would get “AA123”. If we applied the same to the string “1536” we would get “A1536”. If we applied it to “15683” we would get “15683”.\nNow we can check to confirm that this worked:\n\n\nCode\ninc_70 |&gt; \n  group_by(nchar(TRTID10)) |&gt;  \n  summarise(n())\n\n\n# A tibble: 1 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               11 52759\n\n\nAll the observations of TRTID10 are now 11 characters wide.\nCheck the 1980 to 2010 data, and pad where needed.\n\n\nCode\ninc_80 |&gt; \n  group_by(nchar(TRTID10)) |&gt;  \n  summarise(n()) # Needs padding\n\n\n# A tibble: 2 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               10 12353\n2               11 46834\n\n\nCode\ninc_90 |&gt;  \n  group_by(nchar(TRTID10)) |&gt;  \n  summarise(n()) # Needs padding\n\n\n# A tibble: 2 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               10 13663\n2               11 59030\n\n\nCode\ninc_00 |&gt;  \n  group_by(nchar(TRTID10)) |&gt;  \n  summarise(n()) # Needs padding\n\n\n# A tibble: 2 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               10 13663\n2               11 59030\n\n\nCode\ninc_10 |&gt;  \n  group_by(nchar(TRTID10)) |&gt; \n  summarise(n()) # No padding needed\n\n\n# A tibble: 1 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               11 73056\n\n\nCode\ninc_20 |&gt;  \n  group_by(nchar(TRTID10)) |&gt;  \n  summarise(n()) # No padding needed\n\n\n# A tibble: 1 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               11 73056\n\n\nCode\ninc_80&lt;-inc_80 |&gt; \n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\ninc_90&lt;-inc_90 |&gt;  \n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\ninc_00&lt;-inc_00 |&gt;  \n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\n\n\nNow confirm that your padding has worked:\n\n\nCode\ninc_70 |&gt; \n  group_by(nchar(TRTID10)) |&gt; \n  summarise(n())\n\n\n# A tibble: 1 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               11 52759\n\n\nCode\ninc_80 |&gt;  \n  group_by(nchar(TRTID10)) |&gt; \n  summarise(n())\n\n\n# A tibble: 1 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               11 59187\n\n\nCode\ninc_90 |&gt; \n  group_by(nchar(TRTID10)) |&gt; \n  summarise(n())\n\n\n# A tibble: 1 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               11 72693\n\n\nCode\ninc_00 |&gt;  \n  group_by(nchar(TRTID10)) |&gt;  \n  summarise(n())\n\n\n# A tibble: 1 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               11 72693\n\n\nCode\ninc_10 |&gt; \n  group_by(nchar(TRTID10)) |&gt; \n  summarise(n())\n\n\n# A tibble: 1 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               11 73056\n\n\nCode\ninc_20 |&gt; \n  group_by(nchar(TRTID10)) |&gt; \n  summarise(n())\n\n\n# A tibble: 1 × 2\n  `nchar(TRTID10)` `n()`\n             &lt;int&gt; &lt;int&gt;\n1               11 73056\n\n\nNote that this padding issue would not effect Illinois (state FIPS code 17), but it’s important to know how to diagnose and fix this problem, as it could come up for other states or places you are working with.\nRemember to take your time, and build code that you (or others) can use universally in the future.\n\n\nRe-doing Data Import\nBefore we move forward, let’s update our data import code so that we incorporate addressing the data issues we’ve covered into our data importation workflow. As a reminder, we need to rename our combined state-county-tract FIPS column and pad the column to 11 characters on the left size with “0”.\n\n\nCode\nF_1970&lt;-read_csv(\"data_raw/LTDB_Std_1970_fullcount.csv\", col_types = cols(TRTID10=col_character())) |&gt;\n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\n\nS_1970&lt;-read_csv(\"data_raw/ltdb_std_1970_sample.csv\", col_types = cols(TRTID10=col_character())) |&gt;\n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\n\nF_1980&lt;-read_csv(\"data_raw/LTDB_Std_1980_fullcount.csv\", col_types = cols(TRTID10=col_character())) |&gt;\n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\nS_1980&lt;-read_csv(\"data_raw/ltdb_std_1980_sample.csv\", col_types = cols(trtid10=col_character())) |&gt; \n  rename(TRTID10 = trtid10) |&gt;\n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\n\nF_1990&lt;-read_csv(\"data_raw/LTDB_Std_1990_fullcount.csv\", col_types = cols(TRTID10=col_character())) |&gt;\n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\nS_1990&lt;-read_csv(\"data_raw/ltdb_std_1990_sample.csv\", col_types = cols(TRTID10=col_character())) |&gt;\n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\n\nF_2000&lt;-read_csv(\"data_raw/LTDB_Std_2000_fullcount.csv\", col_types = cols(TRTID10=col_character())) |&gt;\n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\nS_2000&lt;-read_csv(\"data_raw/ltdb_std_2000_sample.csv\", col_types = cols(TRTID10=col_character())) |&gt;\n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\n\nF_2010&lt;-read_csv(\"data_raw/LTDB_Std_2010_fullcount.csv\", col_types = cols(tractid=col_character())) |&gt;\n    rename(TRTID10 = tractid) |&gt;\n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\nS_2010&lt;-read_csv(\"data_raw/ltdb_std_2010_sample.csv\", col_types = cols(tractid=col_character())) |&gt;\n      rename(TRTID10 = tractid) |&gt;\n  mutate(TRTID10 = str_pad(TRTID10, width = 11, side=\"left\", pad= \"0\"))\n\n\nAnd from here we can re-create the inc_[year] datasets:\n\n\nCode\ninc_70&lt;-S_1970 |&gt;  \n  select(TRTID10, INCPC70)\ninc_80&lt;-S_1980 |&gt;  \n  select(TRTID10, INCPC80 = incpc80)\ninc_90&lt;-S_1990 |&gt;  \n  select(TRTID10, INCPC90)\ninc_00&lt;-S_2000 |&gt;  \n  select(TRTID10, INCPC00)\ninc_10&lt;-S_2010 |&gt;  \n  select(TRTID10, INCPC12 = incpc12)\ninc_20 &lt;- S_2020 |&gt; \n  select(TRTID10, STATE = statea, COUNTY = countya, TRACT = tracta, INCPC19 = incpc19)\n\n\n\n\nJoin Together\nFinally we have our data in a form where we can join our income cross-sections together. Using left joins, join together your six income data cross-sections into a new dataset called income.\n\n\nCode\nincome&lt;-left_join(inc_20, inc_10, by=\"TRTID10\")\nincome&lt;-left_join(income, inc_00, by=\"TRTID10\")\nincome&lt;-left_join(income, inc_90, by=\"TRTID10\")\nincome&lt;-left_join(income, inc_80, by=\"TRTID10\")\nincome&lt;-left_join(income, inc_70, by=\"TRTID10\")\n\n\nRe-organize the income per capita fields so they are in the order 1970, 1980, 1990, 2000, 2010, 2010.\n\n\nCode\nincome&lt;-income  |&gt;  \n  select(TRTID10, STATE, COUNTY, TRACT, INCPC70, INCPC80, INCPC90, INCPC00, INCPC12, INCPC19)\n\n\n\n\nAdjust for Inflation\nWe have one more step to accomplish before we can move forward with analysis. Each of our income observations is reported out in the monetary value associated with the time period in which it was collected. Due to inflation, money has a different value over time - $5 in 1970 could buy you a lot more than it can in 2020. Using the CPI Inflation Calculator, we can calculate that the value of 5 dollars in 1970 would have the equivalent value of 39 dollars and 57 cents in 2023 dollars.\nBy standardizing all of the dollar values to their equivalent value in a common year, it is much easier to compare changes in income over time as they are in the same dollar equivalent unit.\nThe CPI Inflation Calculator can help us here. If we find out how much $1 in our data collection year is worth in a common current year, we can figure out what we’d have to multiply our observations by to convert them to current year dollars.\nWe will eventually add some additional data and demographic characteristics from the 2015-2019 ACS. Looking in the 2015-2019 ACS per capital income table B19301: Per Capita Income in the Past 12 Months we see that these data are reported in inflation-adjusted 2019 dollars.\nSince 2019 will be our most recent point of reference, let’s adjust our dollar values from 1970 - 2010 to 2019 dollars. Using the CPI Inflation Calculator, find the value of 1 dollar in January of each of the decennial years in January 2019.\n\n\n\nYear\nValue\n\n\n\n\n1970\n6.66\n\n\n1980\n3.24\n\n\n1990\n1.98\n\n\n2000\n1.49\n\n\n2010 (2012)\n1.11\n\n\n\nNow multiply each of the income per capita columns by their inflation adjustment factor to get them into 2019 dollars (assume 1970, 1980, 1990, and 2000 data are from January of that year, and our 2010 data which comes from the ACS is actually in 2012 dollars).\n\n\nCode\nincome&lt;-income |&gt; \n  mutate(\n  INCPC70 = INCPC70*6.66,\n  INCPC80 = INCPC80*3.24,\n  INCPC90 = INCPC90*1.98,\n  INCPC00 = INCPC00*1.49,\n  INCPC12 = INCPC12*1.11\n)\n\n\nWe now have a relatively clean income dataset that we could use to analyze any tract, city, or region within the United States."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#select-regional-data",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#select-regional-data",
    "title": "Neighborhood Change",
    "section": "Select Regional Data",
    "text": "Select Regional Data\nAs we have for our last few analyses, we’re going to examine neighborhood change for New York City. As in the past, we’ll start by pulling out data for the counties that represent boroughs of New York City. Our LTDB data contains a combined State, County, Tract FIPS code - how could we pull out the state and county FIPS codes we would need to identify counties?\nWe have casually been introduced to stringr’s substr() function in the past, that allows us to pull out substrings based upon the character positions. In this case, we want to pull out the first five characters from the TRTID10 field, which contains the combined state and county FIPS code. Use substr() inside a mutate()` to create a new column called COFIPS in our combined income dataset that contains the first five characters from TRTID10.\n\n\nCode\nincome&lt;-income |&gt; \n  mutate(COFIPS = substr(TRTID10, 0, 5))\n\n\nAs a reminder, here’s the names and FIPS codes for NYC Boroughs:\n\n\n\nFIPS Code\nCounty Name\nBorough Name\n\n\n\n\n36047\nKings County\nBrooklyn\n\n\n36005\nBronx County\nBronx\n\n\n36081\nQueens County\nQueens\n\n\n36085\nRichmond County\nStaten Island\n\n\n36061\nNew York County\nManhattan\n\n\n\nLet’s create a new income dataset called _income that contains income observations for New York City’s Boroughs (in your filter() statement consider using %in% with a list of combined state-county FIPS codes):\n\n\nCode\nnyc_income &lt;- income |&gt; \n  filter(COFIPS %in% c(\"36047\", \"36005\", \"36081\", \"36085\", \"36061\"))\n\n\nNow’s a good time to check descriptive statistics for our income data. Let’s use summary() on the chi_income object:\n\n\nCode\nsummary(nyc_income)\n\n\n   TRTID10             STATE              COUNTY             TRACT          \n Length:2167        Length:2167        Length:2167        Length:2167       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n    INCPC70          INCPC80          INCPC90          INCPC00      \n Min.   :     0   Min.   :     0   Min.   :     0   Min.   :     0  \n 1st Qu.: 24115   1st Qu.: 15030   1st Qu.: 18831   1st Qu.: 18996  \n Median : 29101   Median : 21062   Median : 27186   Median : 25768  \n Mean   : 31049   Mean   : 22503   Mean   : 30743   Mean   : 32044  \n 3rd Qu.: 34249   3rd Qu.: 25867   3rd Qu.: 34583   3rd Qu.: 34538  \n Max.   :278792   Max.   :140026   Max.   :315848   Max.   :441085  \n NA's   :30       NA's   :15       NA's   :15       NA's   :15      \n    INCPC12            INCPC19          COFIPS         \n Min.   :   387.4   Min.   :  3341   Length:2167       \n 1st Qu.: 19782.1   1st Qu.: 22879   Class :character  \n Median : 26797.6   Median : 30120   Mode  :character  \n Mean   : 33951.3   Mean   : 38454                     \n 3rd Qu.: 36367.8   3rd Qu.: 41411                     \n Max.   :280727.9   Max.   :258909                     \n NA's   :47         NA's   :50                         \n\n\nWe have 2,167 observations for New York City."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#moving-forward-with-our-three-cities-analysis",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#moving-forward-with-our-three-cities-analysis",
    "title": "Neighborhood Change",
    "section": "Moving Forward with our Three Cities Analysis",
    "text": "Moving Forward with our Three Cities Analysis\n\nJoin ACS Data to our Income Data\nLet’s next join our “new” ACS data to our prior income data. Let’s join to our chi_income (as opposed to chi_income_long data):\n\n\nCode\nnyc_income &lt;- left_join(nyc_income, acs_19, by=c(\"TRTID10\" = \"GEOID\"))\n\n\n\n\nRe-Create Income Categories in Wide-Format data\nPreviously, we created income category labels in our long-format version of the nyc_income data. This time, let’s do the same thing in the wide format data.\n\n\nCode\nnyc_income&lt;-nyc_income |&gt; \n  mutate(\n    pincome_70 = case_when(\n    between(pincome_70, -100, -.4) ~ \"Very Low Income\",\n    between(pincome_70, -.4, -.2) ~ \"Low Income\",\n    between(pincome_70, -.2, .2) ~ \"Middle Income\",\n    between(pincome_70, .2, .4) ~ \"High Income\",\n    between(pincome_70, .4, 100) ~ \"Very High Income\"\n  ),\n  pincome_80 = case_when(\n    between(pincome_80, -100, -.4) ~ \"Very Low Income\",\n    between(pincome_80, -.4, -.2) ~ \"Low Income\",\n    between(pincome_80, -.2, .2) ~ \"Middle Income\",\n    between(pincome_80, .2, .4) ~ \"High Income\",\n    between(pincome_80, .4, 100) ~ \"Very High Income\"\n  ),\n    pincome_90 = case_when(\n    between(pincome_90, -100, -.4) ~ \"Very Low Income\",\n    between(pincome_90, -.4, -.2) ~ \"Low Income\",\n    between(pincome_90, -.2, .2) ~ \"Middle Income\",\n    between(pincome_90, .2, .4) ~ \"High Income\",\n    between(pincome_90, .4, 100) ~ \"Very High Income\"\n  ),\n    pincome_00 = case_when(\n    between(pincome_00, -100, -.4) ~ \"Very Low Income\",\n    between(pincome_00, -.4, -.2) ~ \"Low Income\",\n    between(pincome_00, -.2, .2) ~ \"Middle Income\",\n    between(pincome_00, .2, .4) ~ \"High Income\",\n    between(pincome_00, .4, 100) ~ \"Very High Income\"\n  ),\n    pincome_10 = case_when(\n    between(pincome_10, -100, -.4) ~ \"Very Low Income\",\n    between(pincome_10, -.4, -.2) ~ \"Low Income\",\n    between(pincome_10, -.2, .2) ~ \"Middle Income\",\n    between(pincome_10, .2, .4) ~ \"High Income\",\n    between(pincome_10, .4, 100) ~ \"Very High Income\"\n  ),\n    pincome_20 = case_when(\n    between(pincome_20, -100, -.4) ~ \"Very Low Income\",\n    between(pincome_20, -.4, -.2) ~ \"Low Income\",\n    between(pincome_20, -.2, .2) ~ \"Middle Income\",\n    between(pincome_20, .2, .4) ~ \"High Income\",\n    between(pincome_20, .4, 100) ~ \"Very High Income\"\n  )\n)\n\n\n\n\nCalculate Three Cities Categories\nIf you look on the “Three Cities” tab on the Voorhees website, you’ll note that they separate tracts based upon the change in income between 1970 and the most recent time period (they use 2016 but we’ll use 2019). Recall that…\n\nCity One includes all Census Tracts that increased their proportion of regional income by 20 percent;\nCity Two includes all Census Tracts that increased or decreased their proportion of regional income by less than 20 percent;\nCity Three includes all Census Tracts that decreased their proportion of regional income by 20 percent.\n\nLet’s go ahead and calculate that change as well as the categorization of each census tract:\n\n\nCode\nnyc_income&lt;-nyc_income |&gt; \n  mutate(pchange_70.19 = (INCPC19 - INCPC70)/INCPC70)\n\n\nAnd once we calculate this, we can assign each tract to which “City” it belongs to based upon the change in income between 1970 and 2019:\n\n\nCode\nnyc_income&lt;-nyc_income |&gt; \n  mutate(change_cat_70.19 = case_when(\n    between(pchange_70.19, .2, 100) ~ \"City 1 - Growing\",\n    between(pchange_70.19, -.2, .2) ~ \"City 2 - Middle\",\n    between(pchange_70.19,  -100, -.2) ~ \"City 3 - Shrinking\",\n    is.na(pchange_70.19) == \"TRUE\" ~ \"Uncategorized\",\n    pchange_70.19 == Inf ~ \"Uncategorized\")\n  )\n\n\nWe have a small proportion of tracts which are ultimately not characterized because they are missing data for either 1970 or 2020. We can label these as uncategorized so that we can compare them and understand how they may be different from tracts whoch area assigned to a change type.\nWith this done, we can now develop some descriptives to help us understand differences between our three cities."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#describing-out-three-cities",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#describing-out-three-cities",
    "title": "Neighborhood Change",
    "section": "Describing Out Three Cities",
    "text": "Describing Out Three Cities\nFollowing the descriptions provided on the Voorhees Center’s three cities study site, let’s systematically describe New York’s neighborhoods based upon change in income.\n\nHow Many Tracts? How Many People?\nLooking at the first tab in the Voorhees report, we need to describe how many tracts and how many people fall within each city.\n Start by counting how many tracts there are in each city and how many people lived in each city in 2019. Use group_by() and summarise() to accomplish this.\n\n\nCode\nnyc_income |&gt; \n  group_by(change_cat_70.19) |&gt; \n  summarise(Tracts = n(), Population = sum(POP19, na.rm=TRUE)) |&gt; \n  gt() |&gt; \n  fmt_number(3, decimals = 0)\n\n\n\n\n\n\n  \n    \n    \n      change_cat_70.19\n      Tracts\n      Population\n    \n  \n  \n    City 1 - Growing\n678\n2,824,154\n    City 2 - Middle\n971\n3,543,062\n    City 3 - Shrinking\n453\n2,009,535\n    Uncategorized\n65\n42,565\n  \n  \n  \n\n\n\n\nWe can see the split in 2019 population between growing, middle and shrinking places. We have 65 tracts which are unclassified with a population of around 42,565.\n\n\nRace\nThe next pane in the Voorhees analysis summarizes each city based upon it’s racial composition:\n\nLet’s develop a table that does the same:\n\n\nCode\nnyc_income |&gt; \n  group_by(change_cat_70.19) |&gt; \n  summarise(\nTracts = n(), \nPWhite = mean(P_White, na.rm=TRUE),\nPBlack = mean(P_Black, na.rm=TRUE),\nPAsian = mean(P_Asian, na.rm=TRUE),\nPLatinx = mean(P_Latinx, na.rm=TRUE),\nPOther = mean(P_Other, na.rm=TRUE)\n) |&gt; \n  gt() |&gt; \n  fmt_percent(3:7, decimals = 1)\n\n\n\n\n\n\n  \n    \n    \n      change_cat_70.19\n      Tracts\n      PWhite\n      PBlack\n      PAsian\n      PLatinx\n      POther\n    \n  \n  \n    City 1 - Growing\n678\n49.3%\n17.4%\n10.7%\n19.5%\n3.1%\n    City 2 - Middle\n971\n27.9%\n28.0%\n15.1%\n25.5%\n3.5%\n    City 3 - Shrinking\n453\n16.1%\n20.6%\n18.4%\n42.0%\n2.8%\n    Uncategorized\n65\n30.9%\n38.2%\n4.2%\n25.9%\n0.8%\n  \n  \n  \n\n\n\n\nWe might also want to revisit our based LTDB data so that we can describe change in racial composition for each city over time.\n\n\nAge\nWe will next summarize each of the three cities by their age composition. Consider calculating percentages in addition to reporting the population counts.\n\n\n\nCode\nnyc_income |&gt; \n  group_by(change_cat_70.19) |&gt; \n  summarise(\nTracts = n(), \nPopTot = sum(POP19, na.rm=TRUE),\nAge_00_19 = sum(Age_00_19, na.rm=TRUE)/sum(POP19, na.rm=TRUE),\nAge_20_34 = sum(Age_20_34, na.rm=TRUE)/sum(POP19, na.rm=TRUE),\nAge_35_64 = sum(Age_35_64, na.rm=TRUE)/sum(POP19, na.rm=TRUE),\nAge_65up = sum(Age_65up, na.rm=TRUE)/sum(POP19, na.rm=TRUE)\n) |&gt; \n  gt() |&gt; \n  fmt_number(2:3, decimals = 0) |&gt; \n  fmt_percent(4:7, decimals = 1)\n\n\n\n\n\n\n  \n    \n    \n      change_cat_70.19\n      Tracts\n      PopTot\n      Age_00_19\n      Age_20_34\n      Age_35_64\n      Age_65up\n    \n  \n  \n    City 1 - Growing\n678\n2,824,154\n17.8%\n28.4%\n38.7%\n15.0%\n    City 2 - Middle\n971\n3,543,062\n23.7%\n21.8%\n39.0%\n15.5%\n    City 3 - Shrinking\n453\n2,009,535\n28.9%\n23.2%\n35.8%\n12.1%\n    Uncategorized\n65\n42,565\n18.8%\n33.2%\n34.1%\n14.0%\n  \n  \n  \n\n\n\n\n\n\nDisability\nNext up, we’ll examine disability, primarily be calculating the average share of each city that is disabled.\n\n\n\nCode\nnyc_income |&gt; \n  group_by(change_cat_70.19) |&gt; \n  summarise(\nTracts = n(), \nP_Disabled = mean(P_Disabled, na.rm=TRUE)\n) |&gt; \n  gt() |&gt; \n  fmt_percent(3, decimals = 1)\n\n\n\n\n\n\n  \n    \n    \n      change_cat_70.19\n      Tracts\n      P_Disabled\n    \n  \n  \n    City 1 - Growing\n678\n9.7%\n    City 2 - Middle\n971\n10.8%\n    City 3 - Shrinking\n453\n12.0%\n    Uncategorized\n65\n17.4%\n  \n  \n  \n\n\n\n\n\n\nHousing\nNext up, we’ll look at housing tenure (the balance between owners and renters), as well as cost burden (the share of households paying more than 30 percent of their income towards housing costs.\n[]images/08_Housing.png)\n\n\nCode\nnyc_income |&gt; \n  group_by(change_cat_70.19) |&gt; \n  summarise(\nTracts = n(), \nP_Own = mean(P_Own, na.rm=TRUE),\nP_Rent = mean(P_Rent, na.rm=TRUE),\nP_CostBurden = mean(P_CostBurden, na.rm=TRUE)\n) |&gt; \n  gt() |&gt; \n  fmt_percent(3:5, decimals = 1)\n\n\n\n\n\n\n  \n    \n    \n      change_cat_70.19\n      Tracts\n      P_Own\n      P_Rent\n      P_CostBurden\n    \n  \n  \n    City 1 - Growing\n678\n36.1%\n63.9%\n39.2%\n    City 2 - Middle\n971\n45.4%\n54.6%\n46.6%\n    City 3 - Shrinking\n453\n22.7%\n77.3%\n55.6%\n    Uncategorized\n65\n37.5%\n62.5%\n51.2%\n  \n  \n  \n\n\n\n\n\n\nEmployment\nLet’s finally calculate labor force participation and unemployment by city.\n\n\n\nCode\nnyc_income |&gt; \n  group_by(change_cat_70.19) |&gt; \n  summarise(\nTracts = n(), \nP_Unemp = mean(P_Unemp, na.rm=TRUE),\nP_LabForce = mean(P_LabForce, na.rm=TRUE)\n) |&gt; \n  gt() |&gt; \n  fmt_percent(3:4, decimals = 1)\n\n\n\n\n\n\n  \n    \n    \n      change_cat_70.19\n      Tracts\n      P_Unemp\n      P_LabForce\n    \n  \n  \n    City 1 - Growing\n678\n5.0%\n68.2%\n    City 2 - Middle\n971\n6.5%\n61.8%\n    City 3 - Shrinking\n453\n8.5%\n59.0%\n    Uncategorized\n65\n13.0%\n58.6%\n  \n  \n  \n\n\n\n\nAlso calculate average employment by industry for the three cities.\n\n\nCode\nnyc_income |&gt; \n  group_by(change_cat_70.19) |&gt; \n  summarise(\nTracts = n(), \nP_Emp_Ag = mean(P_Emp_Ag, na.rm=TRUE),\nP_Emp_Const = mean(P_Emp_Const, na.rm=TRUE),\nP_Emp_Manuf = mean(P_Emp_Manuf, na.rm=TRUE),\nP_Emp_Wholesale = mean(P_Emp_Wholesale, na.rm=TRUE),\nP_Emp_Retail = mean(P_Emp_Retail, na.rm=TRUE),\nP_Emp_Transportation = mean(P_Emp_Transportation, na.rm=TRUE),\nP_Emp_FIRE = mean(P_Emp_FIRE, na.rm=TRUE),\nP_Emp_Management = mean(P_Emp_Management, na.rm=TRUE),\nP_Emp_Education = mean(P_Emp_Education, na.rm=TRUE),\nP_Emp_Arts = mean(P_Emp_Arts, na.rm=TRUE),\nP_Emp_Other = mean(P_Emp_Other, na.rm=TRUE),\nP_Emp_PA = mean(P_Emp_PA, na.rm=TRUE)\n) |&gt; \n  gt() |&gt; \n  fmt_percent(3:14, decimals = 1)\n\n\n\n\n\n\n  \n    \n    \n      change_cat_70.19\n      Tracts\n      P_Emp_Ag\n      P_Emp_Const\n      P_Emp_Manuf\n      P_Emp_Wholesale\n      P_Emp_Retail\n      P_Emp_Transportation\n      P_Emp_FIRE\n      P_Emp_Management\n      P_Emp_Education\n      P_Emp_Arts\n      P_Emp_Other\n      P_Emp_PA\n    \n  \n  \n    City 1 - Growing\n678\n0.1%\n3.8%\n3.0%\n2.0%\n7.8%\n4.6%\n11.8%\n18.0%\n24.4%\n9.9%\n4.5%\n4.1%\n    City 2 - Middle\n971\n0.1%\n6.3%\n3.2%\n2.3%\n9.6%\n8.5%\n7.4%\n10.5%\n30.2%\n9.6%\n5.4%\n4.5%\n    City 3 - Shrinking\n453\n0.2%\n6.4%\n3.6%\n2.2%\n11.2%\n8.4%\n6.2%\n9.2%\n28.5%\n13.0%\n6.6%\n3.0%\n    Uncategorized\n65\n0.0%\n2.4%\n6.3%\n0.8%\n7.1%\n3.1%\n11.7%\n19.7%\n26.5%\n7.5%\n7.6%\n4.1%"
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#describing-our-three-cities",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#describing-our-three-cities",
    "title": "Neighborhood Change",
    "section": "Describing our Three Cities",
    "text": "Describing our Three Cities"
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#lab-evaluation",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#lab-evaluation",
    "title": "Neighborhood Change",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nProper import, cleaning, and formatting of LTDB data, paying particular attention to building reproducible data pipelines that you could use for any geographic location.\nThoughtful reflection on data process and outputs.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#references",
    "href": "assignments/labs/07_neighborhood_change/07_neighborhood_change.html#references",
    "title": "Neighborhood Change",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/05_population_census/06_census2.html",
    "href": "assignments/labs/05_population_census/06_census2.html",
    "title": "Population and the Census (Week 2)",
    "section": "",
    "text": "In our previous lab, you learned some of the basics of how to download tables and variables from the census API using the tidycensus package. This week, we will continue to add depth to our exploration of census data, focused on refining our workflows, particularly workflows involving the visualization of census data.\nPlease refer to our interactive coding session focused on data refinement and visualization, which references workflows which you will use in your lab analysis.\nPortions of this lab are inspired by Kyle Walker’s Analyzing US Census Data: Methods, Maps and Models in R Chapter 3."
  },
  {
    "objectID": "assignments/labs/05_population_census/06_census2.html#introduction",
    "href": "assignments/labs/05_population_census/06_census2.html#introduction",
    "title": "Population and the Census (Week 2)",
    "section": "",
    "text": "In our previous lab, you learned some of the basics of how to download tables and variables from the census API using the tidycensus package. This week, we will continue to add depth to our exploration of census data, focused on refining our workflows, particularly workflows involving the visualization of census data.\nPlease refer to our interactive coding session focused on data refinement and visualization, which references workflows which you will use in your lab analysis.\nPortions of this lab are inspired by Kyle Walker’s Analyzing US Census Data: Methods, Maps and Models in R Chapter 3."
  },
  {
    "objectID": "assignments/labs/05_population_census/06_census2.html#goals",
    "href": "assignments/labs/05_population_census/06_census2.html#goals",
    "title": "Population and the Census (Week 2)",
    "section": "Goals",
    "text": "Goals\n\nPractice using some of the strategies for geographic refinement and data analysis we explored in our Monday class session.\nBuild familiarity with manipulating simple feature objects.\nBuild familiarity with using tools that allow us to characterize, compare, and analyze many areal units at once."
  },
  {
    "objectID": "assignments/labs/05_population_census/06_census2.html#core-concepts",
    "href": "assignments/labs/05_population_census/06_census2.html#core-concepts",
    "title": "Population and the Census (Week 2)",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\ndplyr::tally()\nggplot::geom_sf()\ntidycensus::get_acs()\ntidyr::separate()\ntigris::tracts()\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/05_population_census/06_census2.html#github-lab-repository",
    "href": "assignments/labs/05_population_census/06_census2.html#github-lab-repository",
    "title": "Population and the Census (Week 2)",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/05_population_census/06_census2.html#a-few-functions",
    "href": "assignments/labs/05_population_census/06_census2.html#a-few-functions",
    "title": "Population and the Census (Week 2)",
    "section": "A Few Functions",
    "text": "A Few Functions\nMuch of the content covered in this lab was introduced in Monday’s course description. Let’s take a look at a few functions that may be useful to you in completing today’s lab.\n\n\nCode\nlibrary(gt)\nlibrary(tidyverse)"
  },
  {
    "objectID": "assignments/labs/05_population_census/06_census2.html#separate",
    "href": "assignments/labs/05_population_census/06_census2.html#separate",
    "title": "Population and the Census (Week 2)",
    "section": "Separate",
    "text": "Separate\nSometimes we may have a variable that we need to separate into based upon a known delimiter. separate allows us to do so. Let’s explore an example. Here’s a table containing information on the name and location of Big 10 schools.\n\n\nCode\nInstitution &lt;- c(\n  \"University of Illinois\", \n  \"Indiana University\", \n  \"University of Iowa\", \n  \"University of Maryland\", \n  \"University of Michigan\", \n  \"Michigan State University\", \n  \"University of Minnesota\", \n  \"University of Nebraska-Lincoln\", \n  \"Northwestern University\", \n  \"Ohio State University\", \n  \"Pennsylvania State University\", \n  \"Purdue University\", \n  \"Rutgers University\", \n  \"University of Wisconsin-Madison\")\n\nLocation &lt;- c(\n  \"Champaign, Champaign County, Illinois\",\n  \"Bloomington, Monroe County, Indiana\",\n  \"Iowa City, Johnson County, Iowa\",\n  \"College Park, Baltimore County, Maryland\",\n  \"Ann Arbor, Washtenaw County, Michigan\",\n  \"East Lansing, Ingham County, Michigan\",\n  \"Minneapolis, Hennepin County, Minnesota\",\n  \"Lincoln, Lancaster County, Nebraska\",\n  \"Evanston, Cook County, Illinois\",\n  \"Columbus, Franklin County, Ohio\",\n  \"State College, Centre County, Pennsylvania\",\n  \"West Lafayette, Tippecanoe County, Indiana\",\n  \"Newark, Middlesex County, New Jersey\",\n  \"Madison, Dane County, Wisconsin\"\n  )\n\nbig_10 &lt;- tibble(Institution, Location)\n\nbig_10 |&gt; gt()\n\n\n\n\n\n\n  \n    \n    \n      Institution\n      Location\n    \n  \n  \n    University of Illinois\nChampaign, Champaign County, Illinois\n    Indiana University\nBloomington, Monroe County, Indiana\n    University of Iowa\nIowa City, Johnson County, Iowa\n    University of Maryland\nCollege Park, Baltimore County, Maryland\n    University of Michigan\nAnn Arbor, Washtenaw County, Michigan\n    Michigan State University\nEast Lansing, Ingham County, Michigan\n    University of Minnesota\nMinneapolis, Hennepin County, Minnesota\n    University of Nebraska-Lincoln\nLincoln, Lancaster County, Nebraska\n    Northwestern University\nEvanston, Cook County, Illinois\n    Ohio State University\nColumbus, Franklin County, Ohio\n    Pennsylvania State University\nState College, Centre County, Pennsylvania\n    Purdue University\nWest Lafayette, Tippecanoe County, Indiana\n    Rutgers University\nNewark, Middlesex County, New Jersey\n    University of Wisconsin-Madison\nMadison, Dane County, Wisconsin\n  \n  \n  \n\n\n\n\nThe Location field has quite a bit of information present that we might use to learn more about the locations of Big 10 schools. Let’s use separate() to split the city, county, and state into their own fields.\nTo use separate(), we specify the name of the field we want to separate, we specify the new column names we want to assign to each of the components we’re separating, and we indicate what character is the separator (in this case a comma “,”).\n\n\nCode\nbig_10 |&gt; separate(Location, into = c(\"City\", \"County\", \"State\"), sep = \",\") |&gt; gt()\n\n\n\n\n\n\n  \n    \n    \n      Institution\n      City\n      County\n      State\n    \n  \n  \n    University of Illinois\nChampaign\n Champaign County\n Illinois\n    Indiana University\nBloomington\n Monroe County\n Indiana\n    University of Iowa\nIowa City\n Johnson County\n Iowa\n    University of Maryland\nCollege Park\n Baltimore County\n Maryland\n    University of Michigan\nAnn Arbor\n Washtenaw County\n Michigan\n    Michigan State University\nEast Lansing\n Ingham County\n Michigan\n    University of Minnesota\nMinneapolis\n Hennepin County\n Minnesota\n    University of Nebraska-Lincoln\nLincoln\n Lancaster County\n Nebraska\n    Northwestern University\nEvanston\n Cook County\n Illinois\n    Ohio State University\nColumbus\n Franklin County\n Ohio\n    Pennsylvania State University\nState College\n Centre County\n Pennsylvania\n    Purdue University\nWest Lafayette\n Tippecanoe County\n Indiana\n    Rutgers University\nNewark\n Middlesex County\n New Jersey\n    University of Wisconsin-Madison\nMadison\n Dane County\n Wisconsin\n  \n  \n  \n\n\n\n\nWe get back three new fields that replace the existing location field."
  },
  {
    "objectID": "assignments/labs/05_population_census/06_census2.html#tally",
    "href": "assignments/labs/05_population_census/06_census2.html#tally",
    "title": "Population and the Census (Week 2)",
    "section": "Tally",
    "text": "Tally\nWe have used the combination of group_by() and summarise() on many occasions in order to aggregate characteristics of data by groups. For simple operations, there are some helper functions that we can use to simplify our aggregation of groups. tally() for instance is the equivalent of summarise(n = n()) which creates a count of the observations in each group.\nLet’s explore an example using our Big 10 data. Let’s say, for instance, that we want to count the number of Big 10 schools in each state. How would we do this using group_by() and summarise()?\n\n\nCode\nbig_10 |&gt; \n  separate(Location, into = c(\"City\", \"County\", \"State\"), sep = \",\") |&gt; \n  group_by(State) |&gt; \n  summarise(Institutions = n()) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      State\n      Institutions\n    \n  \n  \n     Illinois\n2\n     Indiana\n2\n     Iowa\n1\n     Maryland\n1\n     Michigan\n2\n     Minnesota\n1\n     Nebraska\n1\n     New Jersey\n1\n     Ohio\n1\n     Pennsylvania\n1\n     Wisconsin\n1\n  \n  \n  \n\n\n\n\nIn this case, we’re taking our raw Big 10 data, separating the location into three columns, and then building a summary based upon the state field.\nHere’s how we might do the same thing using group_by() and tally().\n\n\nCode\nbig_10 |&gt; \n  separate(Location, into = c(\"City\", \"County\", \"State\"), sep = \",\") |&gt; \n  group_by(State) |&gt; \n  tally() |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      State\n      n\n    \n  \n  \n     Illinois\n2\n     Indiana\n2\n     Iowa\n1\n     Maryland\n1\n     Michigan\n2\n     Minnesota\n1\n     Nebraska\n1\n     New Jersey\n1\n     Ohio\n1\n     Pennsylvania\n1\n     Wisconsin\n1\n  \n  \n  \n\n\n\n\nWe get output that is basically the same, just with a count column labelled “n”. We can specify the name of the count column so that we have a label that’s more descriptive:\n\n\nCode\nbig_10 |&gt; \n  separate(Location, into = c(\"City\", \"County\", \"State\"), sep = \",\") |&gt; \n  group_by(State) |&gt; \n  tally(name = \"Institutions\") |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      State\n      Institutions\n    \n  \n  \n     Illinois\n2\n     Indiana\n2\n     Iowa\n1\n     Maryland\n1\n     Michigan\n2\n     Minnesota\n1\n     Nebraska\n1\n     New Jersey\n1\n     Ohio\n1\n     Pennsylvania\n1\n     Wisconsin\n1"
  },
  {
    "objectID": "assignments/labs/05_population_census/06_census2.html#lab-evaluation",
    "href": "assignments/labs/05_population_census/06_census2.html#lab-evaluation",
    "title": "Population and the Census (Week 2)",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nUse of dplyr and tidyverse style formatting in your coding.\nProper download calls to tidycensus and tigris.\nUse of ggplot to visualize spatial relationships.\nRefined table output formatting using tools such as gt.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/05_population_census/06_census2.html#references",
    "href": "assignments/labs/05_population_census/06_census2.html#references",
    "title": "Population and the Census (Week 2)",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html",
    "href": "assignments/labs/04_describing_places/04_describing.html",
    "title": "Describing Places",
    "section": "",
    "text": "In this lab, we’ll learn some techniques for creating publication-quality summary tables while working to tell policy-relevant stories about places.\nIn addition to thinking about the basics of how we describe places, we will perform a basic policy analysis of the location of federal Opportunity Zones. This analysis will help illustrate how we can strategically build layers of stories. We’ll add some basic information about all census tracts so that we can describe the differences between ineligible, eligible but not designated, and eligible and designated census tracts."
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#introduction",
    "href": "assignments/labs/04_describing_places/04_describing.html#introduction",
    "title": "Describing Places",
    "section": "",
    "text": "In this lab, we’ll learn some techniques for creating publication-quality summary tables while working to tell policy-relevant stories about places.\nIn addition to thinking about the basics of how we describe places, we will perform a basic policy analysis of the location of federal Opportunity Zones. This analysis will help illustrate how we can strategically build layers of stories. We’ll add some basic information about all census tracts so that we can describe the differences between ineligible, eligible but not designated, and eligible and designated census tracts."
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#goals",
    "href": "assignments/labs/04_describing_places/04_describing.html#goals",
    "title": "Describing Places",
    "section": "Goals",
    "text": "Goals\n\nSet up your computer so that RStudio can communicate with your Github account."
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#core-concepts",
    "href": "assignments/labs/04_describing_places/04_describing.html#core-concepts",
    "title": "Describing Places",
    "section": "Core Concepts",
    "text": "Core Concepts\nThis lab asks you to practice some basic data manipulation and management skills using the dplyr package.\n\nIntroduce several commonly used demographic indicators from the census\nIntroduce how to join datasets together based upon a common field\nIntroduce how to recode and classify data based upon one or more characteristics\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#github-lab-repository",
    "href": "assignments/labs/04_describing_places/04_describing.html#github-lab-repository",
    "title": "Describing Places",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#principles-of-tidy-data",
    "href": "assignments/labs/04_describing_places/04_describing.html#principles-of-tidy-data",
    "title": "Describing Places",
    "section": "Principles of Tidy Data",
    "text": "Principles of Tidy Data\nIn the book R for Data Science, Hadley Wickam describes three principles for tidy data:\n\nEach variable must have its own column\nEach observation must have its own row\nEach value must have its own cell\n\n Much of the data we work with in the context of basic planning and policy analysis applications already conforms to this format (or is easily transformed into this format). This makes packages like tidyverse particularly useful for the common types of data manipulation that we perform.\nWhile we’ll occasionally use base r coding over the course of the semester, for the most part, we’ll rely upon the tidyverse suite to help us. Let’s explore some basic command syntax.\n\nLoad Example Data\nWe’re going to work with a dataset that describes those census tracts that were designated as Opportunity Zones as part of the federal Tax Cuts and Jobs Act. These incentives are designed to spur investment in low-income and undercapitalized cities, by providing investors with tax incentives to invest capital in these locations.\nThe specific dataset which we’ll work with was developed by the Urban Institute, and adds to basic identification of designated census tracts some additional analysis of the characteristics of those places.\n\n\nLoading Required Packages\nWe’re already learned how to use install.packages() and library() to (respectively) install and load packages that extend R and RStudio’s functionality. As a reminder, install.packages() downloads the package from a central server and installs it on your computer. You only have to install a package once. Using library() loads that package for use in your current RStudio session. If you plan to use that package in a given analysis, you’ll need to load it. To stay organized, you should load packages at the beginning of your script or markdown document.\n\n\nNote that to install the package, you need to treat the package name as a character vector \"tidyverse\", but when you load it in your R session, it does not need to be treated as a character vector tidyverse because it is an object that R recognizes after it is installed.\nWe are going to load the following packages:\n\ntidyverse contains tools which we’ll use to subset, filter, group, and summarize our data\nreadxl contains tools which will help us to read Excel files into R\ngt contains tools for making nicely formatted tables.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(gt)\n\n\nThe read_xlsx() command from the readxl package will read Microsoft Excel files into data tables. Let’s start by loading the Urban Institute Opportunity Zone dataset:\nLet’s read the Excel data and place it in an object called “ozs”:\n\n\nCode\nozs &lt;- read_xlsx(\"data/urbaninstitute_tractlevelozanalysis_update1242018.xlsx\")\n\n\nYou can either do a Google search for Readxl to find documentation, or you can use R’s built in documentation by typing ?readxl\nAs the documentation states, readxl imports excel files. Looking at the documentation, the read_excel() command will read a single excel sheet, or we can optionally select a sheet by name or number from an excel workbook with multiple sheets. In this case, the Urban Institute data is in a workbook with a single sheet, so we just need to tell R where the file is to load.\n\n\nDescribing Data\nOne of the first steps that we should do when we load an unfamiliar dataset is to get to know it using some basic description commands.\nLet’s use the str() command to analyze the dataset’s structure:\n\n\nCode\nstr(ozs)\n\n\ntibble [42,176 × 27] (S3: tbl_df/tbl/data.frame)\n $ geoid                : chr [1:42176] \"01001020200\" \"01001020300\" \"01001020700\" \"01001020802\" ...\n $ state                : chr [1:42176] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ Designated           : num [1:42176] NA NA 1 NA NA NA NA 1 NA 1 ...\n $ county               : chr [1:42176] \"Autauga County\" \"Autauga County\" \"Autauga County\" \"Autauga County\" ...\n $ Type                 : chr [1:42176] \"Low-Income Community\" \"Non-LIC Contiguous\" \"Low-Income Community\" \"Non-LIC Contiguous\" ...\n $ dec_score            : num [1:42176] 4 6 9 10 5 6 6 9 10 9 ...\n $ SE_Flag              : num [1:42176] NA NA NA NA NA NA NA NA NA NA ...\n $ Population           : num [1:42176] 2196 3136 3047 10743 2899 ...\n $ medhhincome2014_tract: num [1:42176] 41107 51250 45234 61242 49567 ...\n $ PovertyRate          : num [1:42176] 0.24 0.107 0.19 0.153 0.151 ...\n $ unemprate            : num [1:42176] 0.0775 0.051 0.1407 0.0459 0.0289 ...\n $ medvalue             : num [1:42176] 95300 113800 93500 160400 102900 ...\n $ medrent              : num [1:42176] 743 817 695 1018 546 ...\n $ pctown               : num [1:42176] 0.628 0.703 0.711 0.823 0.83 ...\n $ severerentburden     : num [1:42176] 0.3269 0.3223 0.3887 0.1994 0.0994 ...\n $ vacancyrate          : num [1:42176] 0.0584 0.1399 0.0619 0.0609 0.2182 ...\n $ pctwhitealone        : num [1:42176] 0.439 0.671 0.833 0.814 0.726 ...\n $ pctblackalone        : num [1:42176] 0.5187 0.205 0.0922 0.1572 0.2456 ...\n $ pctHispanic          : num [1:42176] 0.01275 0.0727 0.0338 0.01368 0.00448 ...\n $ pctAAPIalone         : num [1:42176] 0.01093 0.01052 0 0.00959 0 ...\n $ pctunder18           : num [1:42176] 0.218 0.224 0.249 0.27 0.245 ...\n $ pctover64            : num [1:42176] 0.124 0.175 0.149 0.122 0.156 ...\n $ HSorlower            : num [1:42176] 0.581 0.464 0.544 0.45 0.621 ...\n $ BAorhigher           : num [1:42176] 0.162 0.219 0.113 0.229 0.136 ...\n $ Metro                : num [1:42176] 1 1 1 1 1 1 1 1 1 1 ...\n $ Micro                : num [1:42176] NA NA NA NA NA NA NA NA NA NA ...\n $ NoCBSAType           : num [1:42176] NA NA NA NA NA NA NA NA NA NA ...\n\n\nWe get a list where each row is a variable in the dataset. We also learn more about the format of the variable (e.g. character, numeric), the number of observations, and we see examples of the first few observations.\nLet’s next use summary() to get a statistical summary of each variable:\n\n\nCode\nsummary(ozs)\n\n\n    geoid              state             Designated       county         \n Length:42176       Length:42176       Min.   :1       Length:42176      \n Class :character   Class :character   1st Qu.:1       Class :character  \n Mode  :character   Mode  :character   Median :1       Mode  :character  \n                                       Mean   :1                         \n                                       3rd Qu.:1                         \n                                       Max.   :1                         \n                                       NA's   :33414                     \n     Type             dec_score         SE_Flag        Population   \n Length:42176       Min.   : 1.000   Min.   :1       Min.   :    0  \n Class :character   1st Qu.: 3.000   1st Qu.:1       1st Qu.: 2752  \n Mode  :character   Median : 5.000   Median :1       Median : 3897  \n                    Mean   : 5.495   Mean   :1       Mean   : 4147  \n                    3rd Qu.: 8.000   3rd Qu.:1       3rd Qu.: 5224  \n                    Max.   :10.000   Max.   :1       Max.   :40616  \n                    NA's   :1253     NA's   :41111   NA's   :112    \n medhhincome2014_tract  PovertyRate       unemprate          medvalue      \n Min.   :  2499        Min.   :0.0000   Min.   :0.00000   Min.   :   9999  \n 1st Qu.: 32014        1st Qu.:0.1380   1st Qu.:0.05900   1st Qu.:  85700  \n Median : 41094        Median :0.2055   Median :0.08734   Median : 122400  \n Mean   : 42153        Mean   :0.2331   Mean   :0.10063   Mean   : 165663  \n 3rd Qu.: 50833        3rd Qu.:0.2996   3rd Qu.:0.12600   3rd Qu.: 191300  \n Max.   :181406        Max.   :1.0000   Max.   :1.00000   Max.   :2000001  \n NA's   :249           NA's   :141      NA's   :141       NA's   :1106     \n    medrent           pctown       severerentburden  vacancyrate     \n Min.   :  99.0   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.: 655.0   1st Qu.:0.3833   1st Qu.:0.1662   1st Qu.:0.07115  \n Median : 800.0   Median :0.5728   Median :0.2403   Median :0.11658  \n Mean   : 860.9   Mean   :0.5436   Mean   :0.2476   Mean   :0.14120  \n 3rd Qu.:1010.0   3rd Qu.:0.7316   3rd Qu.:0.3206   3rd Qu.:0.18011  \n Max.   :3501.0   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000  \n NA's   :395      NA's   :1033     NA's   :189      NA's   :167      \n pctwhitealone    pctblackalone      pctHispanic       pctAAPIalone    \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.2040   1st Qu.:0.01072   1st Qu.:0.02602   1st Qu.:0.00000  \n Median :0.5614   Median :0.06656   Median :0.09304   Median :0.00883  \n Mean   :0.5211   Mean   :0.18652   Mean   :0.22060   Mean   :0.03806  \n 3rd Qu.:0.8294   3rd Qu.:0.25000   3rd Qu.:0.32014   3rd Qu.:0.03533  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :0.91144  \n NA's   :131      NA's   :131       NA's   :131       NA's   :131      \n   pctunder18       pctover64         HSorlower        BAorhigher    \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.1908   1st Qu.:0.09436   1st Qu.:0.4150   1st Qu.:0.1120  \n Median :0.2300   Median :0.13604   Median :0.5182   Median :0.1679  \n Mean   :0.2295   Mean   :0.14340   Mean   :0.5067   Mean   :0.2034  \n 3rd Qu.:0.2719   3rd Qu.:0.18057   3rd Qu.:0.6113   3rd Qu.:0.2536  \n Max.   :0.6468   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n NA's   :131      NA's   :131       NA's   :132      NA's   :132     \n     Metro          Micro         NoCBSAType   \n Min.   :1      Min.   :1       Min.   :1      \n 1st Qu.:1      1st Qu.:1       1st Qu.:1      \n Median :1      Median :1       Median :1      \n Mean   :1      Mean   :1       Mean   :1      \n 3rd Qu.:1      3rd Qu.:1       3rd Qu.:1      \n Max.   :1      Max.   :1       Max.   :1      \n NA's   :9111   NA's   :37448   NA's   :37793  \n\n\nThis gives us a statistical summary including distribution and central tendency statistics, as well as information on the number of values that are NA.\nA few things to note after your preliminary inspection:\n\nThese data are at the census tract level and include geographic identifiers including geoid, the combined, state-county-tract FIPS code, state the state name, and county the county name.\nThese data include a field named Designated which is 1 when an eligible tract was designated as an opportunity zone, and NA where the tract was not designated.\nThe dataset also includes some other tract-level demographic measures, as well as additional geographic flags (variables that take the value 0 or 1)."
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#query-and-describe-the-data",
    "href": "assignments/labs/04_describing_places/04_describing.html#query-and-describe-the-data",
    "title": "Describing Places",
    "section": "Query and Describe the Data",
    "text": "Query and Describe the Data\nThe dataset we’re looking at is for the entire United States. We can easily summarize characteristics of the entire dataset."
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#recoding-values",
    "href": "assignments/labs/04_describing_places/04_describing.html#recoding-values",
    "title": "Describing Places",
    "section": "Recoding Values",
    "text": "Recoding Values\nOne of the characteristics tracked in the Urban Institute data is the median household income for each designated census tract. We might question whether there’s a difference in the median household income for designated and not-designated but eligible census tracts. This may help us understand something about whether the most needy tracts were selected from those that are eligible.\nHow would we do this? Conceptually…\n\nWe need to split our data into designated and not designated census tracts, and then calculate the average of the median income separately for these tracts.\nBefore we do this, let’s take care of one bit of housekeeping. The Urban Institute has coded the designated variable as either taking a value of 1 when designated or NA when not. Let’s recode those NA values to equal 0 instead.\nTo recode, we need to select those values from the Designated column in the ozs data frame where the value is NA and overwrite them with a new value of 0.\n\nThere’s lots of ways we could do this:\n\nStrategy 1 - Conditional Statement\nWe could use a conditional statement ifelse() to specify that if a value is NA in the Designated column we change it to 0.\n\n\nCode\nozs |&gt; \n  mutate(Designated = ifelse(is.na(Designated), 0, Designated))\n\n\n# A tibble: 42,176 × 27\n   geoid       state   Designated county      Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 01001020200 Alabama          0 Autauga Co… Low-…         4      NA       2196\n 2 01001020300 Alabama          0 Autauga Co… Non-…         6      NA       3136\n 3 01001020700 Alabama          1 Autauga Co… Low-…         9      NA       3047\n 4 01001020802 Alabama          0 Autauga Co… Non-…        10      NA      10743\n 5 01001021000 Alabama          0 Autauga Co… Non-…         5      NA       2899\n 6 01001021100 Alabama          0 Autauga Co… Low-…         6      NA       3247\n 7 01003010100 Alabama          0 Baldwin Co… Non-…         6      NA       4013\n 8 01003010200 Alabama          1 Baldwin Co… Low-…         9      NA       3067\n 9 01003010300 Alabama          0 Baldwin Co… Non-…        10      NA       8079\n10 01003010400 Alabama          1 Baldwin Co… Non-…         9      NA       4578\n# ℹ 42,166 more rows\n# ℹ 19 more variables: medhhincome2014_tract &lt;dbl&gt;, PovertyRate &lt;dbl&gt;,\n#   unemprate &lt;dbl&gt;, medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;,\n#   severerentburden &lt;dbl&gt;, vacancyrate &lt;dbl&gt;, pctwhitealone &lt;dbl&gt;,\n#   pctblackalone &lt;dbl&gt;, pctHispanic &lt;dbl&gt;, pctAAPIalone &lt;dbl&gt;,\n#   pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;, BAorhigher &lt;dbl&gt;,\n#   Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\nIn dplyr syntax, what we said here was with reference to the ozs dataset ozs |&gt; let’s alter the dataset mutate(). Let’s alter the column named Designated mutate(Designated = ). Let’s alter the column named Designated conditionally mutate(Designated = ifelse()). If the value of Designated is equal to NA, replace it with 0, otherwise keep the value present in the Designated observation mutate(Designated = ifelse(is.na(Designated), 0, Designated)).\n\n\nLooking at this ifelse() statement, you might have been tempted to write something like Designated ==NA`which will not work.is.na()is the proper logical test to return whether a value is or is notNA`.\n\n\nStrategy 2: Use a Specialized Command\nWe could use a specialized command such as replace_na() from the tidyr package to replace our NA values:\n\n\nCode\nozs |&gt; \n  mutate(Designated = replace_na(Designated, 0))\n\n\n# A tibble: 42,176 × 27\n   geoid       state   Designated county      Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 01001020200 Alabama          0 Autauga Co… Low-…         4      NA       2196\n 2 01001020300 Alabama          0 Autauga Co… Non-…         6      NA       3136\n 3 01001020700 Alabama          1 Autauga Co… Low-…         9      NA       3047\n 4 01001020802 Alabama          0 Autauga Co… Non-…        10      NA      10743\n 5 01001021000 Alabama          0 Autauga Co… Non-…         5      NA       2899\n 6 01001021100 Alabama          0 Autauga Co… Low-…         6      NA       3247\n 7 01003010100 Alabama          0 Baldwin Co… Non-…         6      NA       4013\n 8 01003010200 Alabama          1 Baldwin Co… Low-…         9      NA       3067\n 9 01003010300 Alabama          0 Baldwin Co… Non-…        10      NA       8079\n10 01003010400 Alabama          1 Baldwin Co… Non-…         9      NA       4578\n# ℹ 42,166 more rows\n# ℹ 19 more variables: medhhincome2014_tract &lt;dbl&gt;, PovertyRate &lt;dbl&gt;,\n#   unemprate &lt;dbl&gt;, medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;,\n#   severerentburden &lt;dbl&gt;, vacancyrate &lt;dbl&gt;, pctwhitealone &lt;dbl&gt;,\n#   pctblackalone &lt;dbl&gt;, pctHispanic &lt;dbl&gt;, pctAAPIalone &lt;dbl&gt;,\n#   pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;, BAorhigher &lt;dbl&gt;,\n#   Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\nNote that in replace_na() we are specifying the column we want to replace the NA value in as well as the value we want to replace NA with.\n\n\nStrategy 3: Recode and Change Format\nDepending upon what we wanted to do with our Designated labels, we could simultaneously deal with recoding our NA values and relabeling the values for legibility. case_when() is useful for these more complex operations:\n\n\nCode\nozs |&gt; mutate(\n  Designated = case_when(\n    Designated == 1 ~\"Designated\",\n    is.na(Designated) ~\"Not Designated\"\n))\n\n\n# A tibble: 42,176 × 27\n   geoid       state   Designated     county  Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 01001020200 Alabama Not Designated Autaug… Low-…         4      NA       2196\n 2 01001020300 Alabama Not Designated Autaug… Non-…         6      NA       3136\n 3 01001020700 Alabama Designated     Autaug… Low-…         9      NA       3047\n 4 01001020802 Alabama Not Designated Autaug… Non-…        10      NA      10743\n 5 01001021000 Alabama Not Designated Autaug… Non-…         5      NA       2899\n 6 01001021100 Alabama Not Designated Autaug… Low-…         6      NA       3247\n 7 01003010100 Alabama Not Designated Baldwi… Non-…         6      NA       4013\n 8 01003010200 Alabama Designated     Baldwi… Low-…         9      NA       3067\n 9 01003010300 Alabama Not Designated Baldwi… Non-…        10      NA       8079\n10 01003010400 Alabama Designated     Baldwi… Non-…         9      NA       4578\n# ℹ 42,166 more rows\n# ℹ 19 more variables: medhhincome2014_tract &lt;dbl&gt;, PovertyRate &lt;dbl&gt;,\n#   unemprate &lt;dbl&gt;, medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;,\n#   severerentburden &lt;dbl&gt;, vacancyrate &lt;dbl&gt;, pctwhitealone &lt;dbl&gt;,\n#   pctblackalone &lt;dbl&gt;, pctHispanic &lt;dbl&gt;, pctAAPIalone &lt;dbl&gt;,\n#   pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;, BAorhigher &lt;dbl&gt;,\n#   Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\nWhat’s going on here? case_when() allows us to conditionally recode values. We specify the condition and then what to do when that condition is met. For instance, we specify the condition Designated == 1 and then say when this condition is met, we want you to change that observation to Designated ~\"Designated\". We then say what to do if the value is NA - label it as “Not Designated”.\nFor the sake of legibility, let’s use the third strategy on our dataset:\n\n\nCode\nozs &lt;- ozs |&gt; mutate(\n  Designated = case_when(\n    Designated == 1 ~\"Designated\",\n    is.na(Designated) ~\"Not Designated\"\n))\n\n\nAnd here’s what our Designated column now looks like:\n\n\n\n\n\n\n  \n    \n    \n      geoid\n      state\n      county\n      Designated\n    \n  \n  \n    01001020200\nAlabama\nAutauga County\nNot Designated\n    01001020300\nAlabama\nAutauga County\nNot Designated\n    01001020700\nAlabama\nAutauga County\nDesignated\n    01001020802\nAlabama\nAutauga County\nNot Designated\n    01001021000\nAlabama\nAutauga County\nNot Designated\n    01001021100\nAlabama\nAutauga County\nNot Designated"
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#summarizing-data",
    "href": "assignments/labs/04_describing_places/04_describing.html#summarizing-data",
    "title": "Describing Places",
    "section": "Summarizing Data",
    "text": "Summarizing Data\nNow that we’ve recoded our designated column, let’s do some description of the characteristics of designated and not designated places.\nLet’s use a combination of group_by() and summarise() to produce a summary table showing the mean value for designated and not designated census tracts.\n\n\nCode\nozs |&gt; \n  group_by(Designated) |&gt; \n  summarise(Income = mean(medhhincome2014_tract))\n\n\n# A tibble: 2 × 2\n  Designated     Income\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Designated         NA\n2 Not Designated     NA\n\n\nWe getting a table back, but why did we get NA insted of numbers here? If you’ve ever used the average mean() command in R, you probably understand what’s going on here. As a safety measure, when you average values, R will return NA if any value in that series is NA. If you’re not expecting any NA values, this is good, becuase you’ll quickly discover that there are unexpected NA values in your dataset. We might expect a few census tracts with missing income values coded as NA, so we will want to indicate na.rm = TRUE here so that R removes those NAs when calculating the mean.\n\n\nCode\nozs |&gt; \n  group_by(Designated) |&gt; \n  summarise(Income = mean(medhhincome2014_tract, na.rm=TRUE))\n\n\n# A tibble: 2 × 2\n  Designated     Income\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Designated     33345.\n2 Not Designated 44446.\n\n\nMuch better. We can see that that on average, the median household income for eligible designated census tracts is lower than that for eligible not designated census tracts. Since the Opportunity Zone legislation is designed to target distressed neighborhoods, this is a good sign that program targeting is focused on neighborhoods with greater need.\nWe might want to add some additional information to our summary table. One useful piece of information would be the number of census tracts that are designated or not designated.\n\n\nCode\nozs |&gt; \n  group_by(Designated) |&gt; \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome2014_tract, na.rm=TRUE))\n\n\n# A tibble: 2 × 3\n  Designated     Tracts Income\n  &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;\n1 Designated       8762 33345.\n2 Not Designated  33414 44446.\n\n\nWithin a summarise() statement, n() gives us a count of observations (rows) for each grouping. In this case, there are 8,762 census tracts designated as opportunity zones, and an additional 33,414 that were eligible based upon program criteria but not designated.\nWe could easily add other summaries to our summary table for this dataset, or further modify."
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#filtering-data",
    "href": "assignments/labs/04_describing_places/04_describing.html#filtering-data",
    "title": "Describing Places",
    "section": "Filtering Data",
    "text": "Filtering Data\nNow that we have some sense for how we might produce basic summaries of our data, how can we query out (filter) observations by row? How, for instance, would you modify the above code to produce the same table for counties in Illinois?\nWe can use a filter() statement to easily accomplish this. filter() allows us to specify one (or more) criteria for which we want to select rows from a larger dataset.\nLet’s take a step back and filter our base dataset to focus on observations in Illinois.\n\n\nCode\nozs |&gt; \n  filter(state == \"Illinois\")\n\n\n# A tibble: 1,659 × 27\n   geoid       state    Designated     county Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 17001000201 Illinois Not Designated Adams… Non-…         7      NA       1937\n 2 17001000202 Illinois Not Designated Adams… Low-…         1      NA       2563\n 3 17001000400 Illinois Not Designated Adams… Low-…         1      NA       3403\n 4 17001000500 Illinois Not Designated Adams… Low-…         1      NA       2298\n 5 17001000700 Illinois Not Designated Adams… Low-…         1      NA       1259\n 6 17001000800 Illinois Designated     Adams… Low-…         1      NA       2700\n 7 17001000900 Illinois Not Designated Adams… Low-…         5      NA       2671\n 8 17001010100 Illinois Not Designated Adams… Non-…         2      NA       4323\n 9 17001010200 Illinois Not Designated Adams… Low-…         2      NA       3436\n10 17001010300 Illinois Not Designated Adams… Non-…         8      NA       6038\n# ℹ 1,649 more rows\n# ℹ 19 more variables: medhhincome2014_tract &lt;dbl&gt;, PovertyRate &lt;dbl&gt;,\n#   unemprate &lt;dbl&gt;, medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;,\n#   severerentburden &lt;dbl&gt;, vacancyrate &lt;dbl&gt;, pctwhitealone &lt;dbl&gt;,\n#   pctblackalone &lt;dbl&gt;, pctHispanic &lt;dbl&gt;, pctAAPIalone &lt;dbl&gt;,\n#   pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;, BAorhigher &lt;dbl&gt;,\n#   Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\nRecall that the ozs dataset has 42,176 observations (rows). We filtered the data using the criteria that the value of state is equal to “Illinois”, resulting in 1,659 observations (eligible census tracts in Illinois).\nFrom here, we can re-use our prior code to produce a summary table that is focused on Illinois.\n\n\nCode\nozs |&gt; \n  filter(state == \"Illinois\") |&gt; \n  group_by(Designated) |&gt; \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome2014_tract, na.rm=TRUE))\n\n\n# A tibble: 2 × 3\n  Designated     Tracts Income\n  &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;\n1 Designated        327 30504.\n2 Not Designated   1332 45281.\n\n\nOk - but how do we summarise by county? We just need to add that as an additional grouping criteria in our group_by() statement:\n\n\nCode\nozs |&gt; \n  filter(state == \"Illinois\") |&gt; \n  group_by(county, Designated) |&gt; \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome2014_tract, na.rm=TRUE))\n\n\n# A tibble: 181 × 4\n# Groups:   county [95]\n   county           Designated     Tracts Income\n   &lt;chr&gt;            &lt;chr&gt;           &lt;int&gt;  &lt;dbl&gt;\n 1 Adams County     Designated          1 26012 \n 2 Adams County     Not Designated      9 39614.\n 3 Alexander County Designated          1 21500 \n 4 Alexander County Not Designated      3 32809 \n 5 Bond County      Designated          1 49590 \n 6 Bond County      Not Designated      1 52310 \n 7 Boone County     Designated          1 40599 \n 8 Boone County     Not Designated      2 45742 \n 9 Bureau County    Designated          1 48083 \n10 Bureau County    Not Designated      3 60339.\n# ℹ 171 more rows\n\n\nWe are basically saying, group by both county and designated and then summarize for each.\nWith a few lines of code, we can produce very powerful and specific kinds of summaries for our data."
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#pivoting-data",
    "href": "assignments/labs/04_describing_places/04_describing.html#pivoting-data",
    "title": "Describing Places",
    "section": "Pivoting Data",
    "text": "Pivoting Data\nOur summary is getting more nuanced. We’ve used group_by() and summarise() to sumamrise data based upon certain characteristics. We’ve summarized in such a way where for our Illinois counties, we have two observations for each county - one that summarises values for designated tracts in that county, and one that summarises values for not designated tracts.\nIt might be useful for us to reshape our summary table so that there is one row for each county, with each row containing the summary value for both designated and not designated tracts.\nThe two commands pivot_wider() and pivot_longer() are useful for reshaping our data. pivot_wider() essentially adds columns to a dataset by transitioning content from rows to columns. pivot_longer() does the opposite - it makes a dataset longer by transitioning columns to rows.\nIn our case, let’s use pivot_wider() to transition our Designated and Not Designated rows into columns.\n\n\nCode\nozs |&gt; \n  filter(state == \"Illinois\") |&gt; \n  group_by(county, Designated) |&gt; \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |&gt; pivot_wider(names_from = Designated, values_from = Income)\n\n\n# A tibble: 172 × 4\n# Groups:   county [95]\n   county           Tracts Designated `Not Designated`\n   &lt;chr&gt;             &lt;int&gt;      &lt;dbl&gt;            &lt;dbl&gt;\n 1 Adams County          1      26012              NA \n 2 Adams County          9         NA           39614.\n 3 Alexander County      1      21500              NA \n 4 Alexander County      3         NA           32809 \n 5 Bond County           1      49590           52310 \n 6 Boone County          1      40599              NA \n 7 Boone County          2         NA           45742 \n 8 Bureau County         1      48083              NA \n 9 Bureau County         3         NA           60339.\n10 Calhoun County        2         NA           55290 \n# ℹ 162 more rows\n\n\nWe start with our previous summary and pass two arguments to pivot_wider().\nWe use names_from to specify the column in our dataset contining row values that we want to become new columns. In this case we’d expect that our Desginated column would result in the creation of two new columns - one where values are Designated and one where values are Not Designated.\nWe use values_from to specify the column containing the values we want in our new columns, in this case, the average of tract income.\nOne problem though - our tract count column is still present and these values are not reshaped. To simplify things, let’s just get rid of this count so we can see what things look like:\n\n\nCode\nozs |&gt; \n  filter(state == \"Illinois\") |&gt; \n  group_by(county, Designated) |&gt; \n  summarise(\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |&gt; pivot_wider(names_from = Designated, values_from = Income)\n\n\n# A tibble: 95 × 3\n# Groups:   county [95]\n   county           Designated `Not Designated`\n   &lt;chr&gt;                 &lt;dbl&gt;            &lt;dbl&gt;\n 1 Adams County         26012            39614.\n 2 Alexander County     21500            32809 \n 3 Bond County          49590            52310 \n 4 Boone County         40599            45742 \n 5 Bureau County        48083            60339.\n 6 Calhoun County          NA            55290 \n 7 Carroll County       35184            58942 \n 8 Cass County          37679            46840.\n 9 Champaign County     13989.           45604.\n10 Christian County     36164            45945.\n# ℹ 85 more rows\n\n\nLooking good! To make things a bit more informative, let’s also show the difference in income between designated and not designated tracts:\n\n\nCode\nozs |&gt; \n  filter(state == \"Illinois\") |&gt; \n  group_by(county, Designated) |&gt; \n  summarise(\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |&gt; pivot_wider(names_from = Designated, values_from = Income) |&gt; \n  mutate(Difference = Designated - `Not Designated`)\n\n\n# A tibble: 95 × 4\n# Groups:   county [95]\n   county           Designated `Not Designated` Difference\n   &lt;chr&gt;                 &lt;dbl&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n 1 Adams County         26012            39614.    -13602.\n 2 Alexander County     21500            32809     -11309 \n 3 Bond County          49590            52310      -2720 \n 4 Boone County         40599            45742      -5143 \n 5 Bureau County        48083            60339.    -12256.\n 6 Calhoun County          NA            55290         NA \n 7 Carroll County       35184            58942     -23758 \n 8 Cass County          37679            46840.     -9162.\n 9 Champaign County     13989.           45604.    -31615.\n10 Christian County     36164            45945.     -9781.\n# ℹ 85 more rows\n\n\nOne note here - in the last mutate() statement, you see that Not Designated has backticks around it. This is because there’s a space between “Not” and “Designated” which will be treated as separate variable names. The backticks allow this to be referenced as a column. We could change the name to something like Not_Designated, but backticks will allow us to appropriately reference it as well."
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#joining-tables",
    "href": "assignments/labs/04_describing_places/04_describing.html#joining-tables",
    "title": "Describing Places",
    "section": "Joining Tables",
    "text": "Joining Tables\nLinking together the place data to the ozs data might would give us some additional context regarding opportunity zones. Remember that the opportunity zones data itemizes those census tracts that were eligible for designation with the Designated column denoting which eligible census tracts actually became opportunity zones. If we link together information for census tracts which were not eligible for designation, we could learn something about the differences between undesignated, eligible not designated, and eligible designated census tracts.\nIn order to link together these two datasets, we need to learn about and apply relational joins to bring these two datasets together.\n\nJoins Overview\nJoins are methods to merge two datasets based on common attributes. It’s rare that one single dataset contains all of the information you wish to tell a story about, so it’s important to understand how joins work.\nA Venn diagram of join types.\n The tidyverse package which we’ve installed and loaded in the past can perform seven different types of relational joins. We’ll discuss six of them briefly, but focus on four key types. Joins require us to have two tables with some type of common identifier column present in both that we can match records based on.\n\n\nJoin Types\nLet’s assume we have two data frames named x and y, and we’re trying to match a column called key in both datasets.\n\nleft_join(): A left join returns every row in x and all the columns from x and y. If a value in the key column in x doesn’t exist in y, the row will contain NA values for all the y columns. If there are multiple key matches, all the possible combinations will be returned.\nright_join(): This is similar to a left join, but returns every row in y instead.\ninner_join(): An inner join returns all the rows in x where there is an key match in y, and all the columns in x and y.\nfull_join(): A full join returns all rows and all columns from x and y. If there isn’t a match in the x key column, all x columns will return NA. (The same is true for y.)\nsemi_join(): A semi-join returns the rows in x where there is an key match in y. It is different than an inner join in that it only returns the columns in x and doesn’t duplicate rows if there are multiple matches in y.\nanti_join(): An anti-join returns the rows in x where there is not a matching key in y. It only returns the columns in x.\n\nYou’ll notice that only the first four joins—left, right, inner, and full—merge two datasets. Those are going to be the most valuable ones to learn. Here are a couple of additional illustrations to illustrate how joins work.\n  The basic general syntax for the joins is the same:\n*_join(x, y, by = \"key name\")\nx and y are self-explanatory. The by attribute is used to name the key, or the variable that the two datasets have in common. If you’re lucky, they’ll have the same name. If you’re unlucky, you’ll have to type a bit more: by = c(\"name1\" = \"name2\"), assuming “name1” is the name of the key column in x and “name2” is the name of the key column in y.\n\n\nExample\nLet’s assume we have two data frames: fruit_1 that contains some characteristics about fruit, and fruit_2 that has some others. Here’s how they’re defined:\nNote that the code above is just another syntax for creating tables as we did in the past.\n\n\nCode\nprint(fruit_1)\n\n\n# A tibble: 4 × 2\n  fruit  color \n  &lt;chr&gt;  &lt;chr&gt; \n1 apple  red   \n2 orange orange\n3 banana yellow\n4 lime   green \n\n\nCode\nprint(fruit_2)\n\n\n# A tibble: 4 × 3\n  fruit  shape price\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 orange round  0.4 \n2 banana long   0.3 \n3 lime   oval   0.25\n4 durian spiky  8   \n\n\nWhat would be the result of a left join, right join, and inner join of fruit_1 and fruit 2?\nNote the following:\n\nThe left join includes all records from fruit_1, but excludes those measures from fruit_2 where there isn’t a match in the fruit column (durian). Note that even though there’s not shape and price information for apples in fruit_2, the rows are still included, but with NA where the data would be were it present.\nThe right join includes all records from fruit_2 but excludes those columns from fruit_1 where there isn’t a match in the fruit column. In this case, we’re missing color information for durian fruit.\nThe inner join includes only those records from fruit_1 and fruit_2 where there were matches in both datasets.\n\nThe powerful thing about these joins is that they allow us to bring together data with different shapes and we can control which elements of the data are joined. Joins will become far more intuitive as you use them more.\n\n\nJoining Together OZs and Place datasets\nNow that we have a sense for how joins work, let’s combine our oz and place into one larger table.\n\n\nCode\nplace &lt;- read_csv(\"data/place_name.csv\")\n\n\nRows: 73057 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): STATEFP, COUNTYFP, TRACTCE, GEOID, place_name\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nplace\n\n\n# A tibble: 73,057 × 5\n   STATEFP COUNTYFP TRACTCE GEOID       place_name\n   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;     \n 1 01      001      020100  01001020100 Prattville\n 2 01      001      020200  01001020200 Prattville\n 3 01      001      020300  01001020300 Prattville\n 4 01      001      020400  01001020400 Prattville\n 5 01      001      020500  01001020500 Prattville\n 6 01      001      020600  01001020600 Prattville\n 7 01      001      020700  01001020700 Prattville\n 8 01      001      020801  01001020801 &lt;NA&gt;      \n 9 01      001      020802  01001020802 &lt;NA&gt;      \n10 01      001      020900  01001020900 &lt;NA&gt;      \n# ℹ 73,047 more rows\n\n\nIf you take a look at both tables, you’ll note that they have a field in common called geoid. This represents a unique code that is assigned to each census tract geography. Technically, this is a FIPS (Federal Information Processing Standards) code. FIPS codes for tracts are hierarchical - the first two digits are unique to each state, the next three digits correspond to each county, and the remaining six digits are unique to each census tract in that county.\nBecause each tract is labelled with corresponding FIPS codes, we can join the two datasets together based upon this common field. This will become a fairly common action for you that you will repeat over the course of this class.\nNext, we should think carefully about what kind of join we want. We know we have ozs data for a subset of census tracts in the U.S. and we have the place data for a more expansive set of tracts. If we want to preserve the more extensive data (including those rows that do not match up with oz- eligible tracts), what type of join should we use and how would we construct it?\nJust to make sure we get this correct, I’m going to provide you with the way to complete your first join on real data. We have one more issue to deal with here to successfully join our data together. Recall that join takes three arguments - two table objects to join together and at least one common field to complete the join based on. These columns are both labelled geoid, but one is capitalized and one is not. We’ll need to tell our join function that these two columns with different names (different in that one is capitalized and one is not) should be joined to each other. We use the modified by=c(\"GEOID\" = \"geoid\") to denote that GEOID in the place data should be joined to geoid in the ozs data. If the names were the same (say, both were GEOID), we could simply say by=\"GEOID\" and this would work.\nOkay, with that out of the way, let’s join our data together:\n\n\nCode\ndataset&lt;-left_join(place, ozs, by=c(\"GEOID\" = \"geoid\"))\n\n\nInto a new object called dataset, we joined all rows from place and those records from ozs that matched. Records from place without a match in ozs will have NA where there could be data.\nTake a look at the data:\n\n\nCode\nView(dataset)\n\n\nStart by looking at the number of rows in the data - 73,057 - the same number as in the place data - we have brought in all rows from the place data and have joined to in matching rows in the ozs data. It would be useful for us to start off by knowing how many rows fall into each of our three categories - ineligible for designated, eligible and undesignated, and eligible and designated. At this point, the NA values in the Designated column reflect ineligible, the 0’s in that column reflect eligible but not designated, and the 1’s represent eligible and designated.\nUse your new knowledge of dplyr’s group_by() and summarise() to create a summary table based upon the three values that we expect the Designated column to take. You’ve learned that you could define what type of summary you’d like to produce in your summarise() statement. Let’s use n() which counts the number of rows that meet each category specified in group_by():\n\n\nCode\ndataset |&gt; \n  group_by(Designated) |&gt; \n  summarise(n())\n\n\n# A tibble: 3 × 2\n  Designated     `n()`\n  &lt;chr&gt;          &lt;int&gt;\n1 Designated      7825\n2 Not Designated 33391\n3 &lt;NA&gt;           31841\n\n\nYou should see that we have 31,841 rows (census tracts) that were ineligible for designation, an additional 33,391 that were eligible but not designated, and 7,825 that were eligible and designated. Excellent!\nWhat might you want to do next to be able to properly label the three categories that now exist for designated?"
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#making-nice-tables",
    "href": "assignments/labs/04_describing_places/04_describing.html#making-nice-tables",
    "title": "Describing Places",
    "section": "Making Nice Tables",
    "text": "Making Nice Tables\nAs many of you have remarked in class, outputting “nice” tables is not R’s default. There are several packages that can help to clean up tables and make them presentable. Let’s learn how to use one such package, the gt package. Similar to how GGPlot describes a grammar of graphics for visualizations, gt similarly provides methods to shape elements of a table.\n\nTable Components in GT\nIn GT, there are numerous table components which you can format as you wish:\n\ngt’s documentation can help you become more familiar with these different components.\n\n\nMaking a First GT Table\nLet’s start off by taking the Illinois data we were previously working on and styling the table using gt:\n\n\nCode\nozs |&gt; \n  filter(state == \"Illinois\") |&gt; \n  group_by(county, Designated) |&gt; \n  summarise(\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |&gt; pivot_wider(names_from = Designated, values_from = Income) |&gt; \n  mutate(Difference = Designated - `Not Designated`) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      Designated\n      Not Designated\n      Difference\n    \n  \n  \n    \n      Adams County\n    \n    26012.00\n39614.22\n-13602.2222\n    \n      Alexander County\n    \n    21500.00\n32809.00\n-11309.0000\n    \n      Bond County\n    \n    49590.00\n52310.00\n-2720.0000\n    \n      Boone County\n    \n    40599.00\n45742.00\n-5143.0000\n    \n      Bureau County\n    \n    48083.00\n60338.67\n-12255.6667\n    \n      Calhoun County\n    \n    NA\n55290.00\nNA\n    \n      Carroll County\n    \n    35184.00\n58942.00\n-23758.0000\n    \n      Cass County\n    \n    37679.00\n46840.50\n-9161.5000\n    \n      Champaign County\n    \n    13988.83\n45603.57\n-31614.7319\n    \n      Christian County\n    \n    36164.00\n45945.43\n-9781.4286\n    \n      Clark County\n    \n    44460.00\n50004.67\n-5544.6667\n    \n      Clay County\n    \n    39261.00\n50853.00\n-11592.0000\n    \n      Clinton County\n    \n    38047.00\n51269.00\n-13222.0000\n    \n      Coles County\n    \n    30682.00\n37615.00\n-6933.0000\n    \n      Cook County\n    \n    29154.50\n45208.69\n-16054.1904\n    \n      Crawford County\n    \n    41071.00\n47089.67\n-6018.6667\n    \n      Cumberland County\n    \n    41761.00\n43780.00\n-2019.0000\n    \n      De Witt County\n    \n    44181.00\n44048.00\n133.0000\n    \n      DeKalb County\n    \n    18462.67\n55294.50\n-36831.8333\n    \n      Douglas County\n    \n    54444.00\n51593.50\n2850.5000\n    \n      DuPage County\n    \n    47140.00\n60237.04\n-13097.0435\n    \n      Edgar County\n    \n    39306.00\n47314.00\n-8008.0000\n    \n      Edwards County\n    \n    39844.00\n51919.00\n-12075.0000\n    \n      Effingham County\n    \n    34028.00\n47879.67\n-13851.6667\n    \n      Fayette County\n    \n    34375.00\n44675.67\n-10300.6667\n    \n      Franklin County\n    \n    36469.00\n40014.18\n-3545.1818\n    \n      Fulton County\n    \n    38279.00\n45053.11\n-6774.1111\n    \n      Gallatin County\n    \n    38750.00\n45074.00\n-6324.0000\n    \n      Greene County\n    \n    35469.00\n43493.25\n-8024.2500\n    \n      Hamilton County\n    \n    54293.00\n47627.50\n6665.5000\n    \n      Hancock County\n    \n    50924.00\n49058.00\n1866.0000\n    \n      Hardin County\n    \n    34250.00\n43860.00\n-9610.0000\n    \n      Henderson County\n    \n    49453.00\n45447.50\n4005.5000\n    \n      Henry County\n    \n    31845.00\n45385.20\n-13540.2000\n    \n      Iroquois County\n    \n    34167.00\n47590.80\n-13423.8000\n    \n      Jackson County\n    \n    16960.00\n34503.50\n-17543.5000\n    \n      Jasper County\n    \n    39909.00\nNA\nNA\n    \n      Jefferson County\n    \n    18411.00\n45013.62\n-26602.6250\n    \n      Jersey County\n    \n    54435.00\n49565.25\n4869.7500\n    \n      Jo Daviess County\n    \n    46731.00\n49293.50\n-2562.5000\n    \n      Johnson County\n    \n    40578.00\n38164.67\n2413.3333\n    \n      Kane County\n    \n    47129.00\n50278.10\n-3149.1000\n    \n      Kankakee County\n    \n    23971.50\n48993.08\n-25021.5769\n    \n      Knox County\n    \n    23011.00\n36096.67\n-13085.6667\n    \n      LaSalle County\n    \n    38625.00\n46211.08\n-7586.0769\n    \n      Lake County\n    \n    29174.60\n48117.03\n-18942.4303\n    \n      Lawrence County\n    \n    37883.00\n42074.33\n-4191.3333\n    \n      Lee County\n    \n    43750.00\n53669.33\n-9919.3333\n    \n      Livingston County\n    \n    33250.00\nNA\nNA\n    \n      Logan County\n    \n    41824.00\n54024.25\n-12200.2500\n    \n      Macon County\n    \n    21405.20\n40130.44\n-18725.2375\n    \n      Macoupin County\n    \n    48000.00\n53369.14\n-5369.1429\n    \n      Madison County\n    \n    28699.80\n49991.43\n-21291.6286\n    \n      Marion County\n    \n    24932.00\n39650.29\n-14718.2857\n    \n      Marshall County\n    \n    49625.00\n56748.00\n-7123.0000\n    \n      Mason County\n    \n    35179.00\n46672.20\n-11493.2000\n    \n      Massac County\n    \n    32198.00\n44130.00\n-11932.0000\n    \n      McDonough County\n    \n    NaN\n44538.33\nNaN\n    \n      McHenry County\n    \n    42972.00\n49938.00\n-6966.0000\n    \n      McLean County\n    \n    11053.00\n43965.63\n-32912.6316\n    \n      Mercer County\n    \n    NA\n46705.00\nNA\n    \n      Montgomery County\n    \n    44438.00\n43921.67\n516.3333\n    \n      Morgan County\n    \n    34314.00\n38475.33\n-4161.3333\n    \n      Moultrie County\n    \n    40761.00\n59008.00\n-18247.0000\n    \n      Ogle County\n    \n    43393.00\n47580.00\n-4187.0000\n    \n      Peoria County\n    \n    20660.00\n39899.95\n-19239.9545\n    \n      Perry County\n    \n    31635.00\n47934.00\n-16299.0000\n    \n      Piatt County\n    \n    NA\n50185.00\nNA\n    \n      Pike County\n    \n    37837.00\n41462.75\n-3625.7500\n    \n      Pope County\n    \n    32396.00\n44375.00\n-11979.0000\n    \n      Pulaski County\n    \n    28884.00\n35643.00\n-6759.0000\n    \n      Randolph County\n    \n    36902.00\n48542.14\n-11640.1429\n    \n      Richland County\n    \n    24671.00\n48985.75\n-24314.7500\n    \n      Rock Island County\n    \n    31542.50\n40817.41\n-9274.9091\n    \n      Saline County\n    \n    32188.00\n43553.62\n-11365.6250\n    \n      Sangamon County\n    \n    22488.80\n40009.96\n-17521.1600\n    \n      Schuyler County\n    \n    52768.00\n48594.00\n4174.0000\n    \n      Scott County\n    \n    NA\n47024.00\nNA\n    \n      Shelby County\n    \n    37689.00\n49664.25\n-11975.2500\n    \n      St. Clair County\n    \n    20159.71\n32147.79\n-11988.0774\n    \n      Stark County\n    \n    NA\n52614.00\nNA\n    \n      Stephenson County\n    \n    26674.50\n46470.00\n-19795.5000\n    \n      Tazewell County\n    \n    47367.00\n47820.73\n-453.7273\n    \n      Union County\n    \n    40506.00\n48029.75\n-7523.7500\n    \n      Vermilion County\n    \n    28462.33\n39596.79\n-11134.4524\n    \n      Wabash County\n    \n    NA\n54207.00\nNA\n    \n      Warren County\n    \n    37185.00\n48814.67\n-11629.6667\n    \n      Washington County\n    \n    46406.00\n57554.00\n-11148.0000\n    \n      Wayne County\n    \n    47645.00\n45951.25\n1693.7500\n    \n      White County\n    \n    31250.00\n48981.00\n-17731.0000\n    \n      Whiteside County\n    \n    45106.00\n45458.10\n-352.1000\n    \n      Will County\n    \n    30419.00\n50754.27\n-20335.2667\n    \n      Williamson County\n    \n    38083.00\n42193.00\n-4110.0000\n    \n      Winnebago County\n    \n    20346.00\n37655.07\n-17309.0714\n    \n      Woodford County\n    \n    NA\n61875.00\nNA\n  \n  \n  \n\n\n\n\nWe simply added gt() as a final command after producing a summary table.\n\n\nFormatting Columns\nThere’s a range of formatting options we can take advantage of to style our table. most formatting options begin with fmt_ which makes it fairly easy to search for the format type you’re looking for. Let’s format the “Designated”, “Not Designated”, and “Difference” columns as currency with no decimal places. While we’re at it, let’s rename the “county” column so “county” is capitalized:\n\n\nCode\nozs |&gt; \n  filter(state == \"Illinois\") |&gt; \n  group_by(county, Designated) |&gt; \n  summarise(\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |&gt; pivot_wider(names_from = Designated, values_from = Income) |&gt; \n  mutate(Difference = Designated - `Not Designated`) |&gt; \n  ungroup() |&gt; \n  gt() |&gt; \n  fmt_currency(2:4, decimals = 0) |&gt; \n  cols_label(county = \"County\")\n\n\n\n\n\n\n  \n    \n    \n      County\n      Designated\n      Not Designated\n      Difference\n    \n  \n  \n    Adams County\n$26,012\n$39,614\n−$13,602\n    Alexander County\n$21,500\n$32,809\n−$11,309\n    Bond County\n$49,590\n$52,310\n−$2,720\n    Boone County\n$40,599\n$45,742\n−$5,143\n    Bureau County\n$48,083\n$60,339\n−$12,256\n    Calhoun County\nNA\n$55,290\nNA\n    Carroll County\n$35,184\n$58,942\n−$23,758\n    Cass County\n$37,679\n$46,840\n−$9,162\n    Champaign County\n$13,989\n$45,604\n−$31,615\n    Christian County\n$36,164\n$45,945\n−$9,781\n    Clark County\n$44,460\n$50,005\n−$5,545\n    Clay County\n$39,261\n$50,853\n−$11,592\n    Clinton County\n$38,047\n$51,269\n−$13,222\n    Coles County\n$30,682\n$37,615\n−$6,933\n    Cook County\n$29,155\n$45,209\n−$16,054\n    Crawford County\n$41,071\n$47,090\n−$6,019\n    Cumberland County\n$41,761\n$43,780\n−$2,019\n    De Witt County\n$44,181\n$44,048\n$133\n    DeKalb County\n$18,463\n$55,294\n−$36,832\n    Douglas County\n$54,444\n$51,594\n$2,850\n    DuPage County\n$47,140\n$60,237\n−$13,097\n    Edgar County\n$39,306\n$47,314\n−$8,008\n    Edwards County\n$39,844\n$51,919\n−$12,075\n    Effingham County\n$34,028\n$47,880\n−$13,852\n    Fayette County\n$34,375\n$44,676\n−$10,301\n    Franklin County\n$36,469\n$40,014\n−$3,545\n    Fulton County\n$38,279\n$45,053\n−$6,774\n    Gallatin County\n$38,750\n$45,074\n−$6,324\n    Greene County\n$35,469\n$43,493\n−$8,024\n    Hamilton County\n$54,293\n$47,628\n$6,666\n    Hancock County\n$50,924\n$49,058\n$1,866\n    Hardin County\n$34,250\n$43,860\n−$9,610\n    Henderson County\n$49,453\n$45,448\n$4,006\n    Henry County\n$31,845\n$45,385\n−$13,540\n    Iroquois County\n$34,167\n$47,591\n−$13,424\n    Jackson County\n$16,960\n$34,504\n−$17,544\n    Jasper County\n$39,909\nNA\nNA\n    Jefferson County\n$18,411\n$45,014\n−$26,603\n    Jersey County\n$54,435\n$49,565\n$4,870\n    Jo Daviess County\n$46,731\n$49,294\n−$2,562\n    Johnson County\n$40,578\n$38,165\n$2,413\n    Kane County\n$47,129\n$50,278\n−$3,149\n    Kankakee County\n$23,972\n$48,993\n−$25,022\n    Knox County\n$23,011\n$36,097\n−$13,086\n    LaSalle County\n$38,625\n$46,211\n−$7,586\n    Lake County\n$29,175\n$48,117\n−$18,942\n    Lawrence County\n$37,883\n$42,074\n−$4,191\n    Lee County\n$43,750\n$53,669\n−$9,919\n    Livingston County\n$33,250\nNA\nNA\n    Logan County\n$41,824\n$54,024\n−$12,200\n    Macon County\n$21,405\n$40,130\n−$18,725\n    Macoupin County\n$48,000\n$53,369\n−$5,369\n    Madison County\n$28,700\n$49,991\n−$21,292\n    Marion County\n$24,932\n$39,650\n−$14,718\n    Marshall County\n$49,625\n$56,748\n−$7,123\n    Mason County\n$35,179\n$46,672\n−$11,493\n    Massac County\n$32,198\n$44,130\n−$11,932\n    McDonough County\nNaN\n$44,538\nNaN\n    McHenry County\n$42,972\n$49,938\n−$6,966\n    McLean County\n$11,053\n$43,966\n−$32,913\n    Mercer County\nNA\n$46,705\nNA\n    Montgomery County\n$44,438\n$43,922\n$516\n    Morgan County\n$34,314\n$38,475\n−$4,161\n    Moultrie County\n$40,761\n$59,008\n−$18,247\n    Ogle County\n$43,393\n$47,580\n−$4,187\n    Peoria County\n$20,660\n$39,900\n−$19,240\n    Perry County\n$31,635\n$47,934\n−$16,299\n    Piatt County\nNA\n$50,185\nNA\n    Pike County\n$37,837\n$41,463\n−$3,626\n    Pope County\n$32,396\n$44,375\n−$11,979\n    Pulaski County\n$28,884\n$35,643\n−$6,759\n    Randolph County\n$36,902\n$48,542\n−$11,640\n    Richland County\n$24,671\n$48,986\n−$24,315\n    Rock Island County\n$31,542\n$40,817\n−$9,275\n    Saline County\n$32,188\n$43,554\n−$11,366\n    Sangamon County\n$22,489\n$40,010\n−$17,521\n    Schuyler County\n$52,768\n$48,594\n$4,174\n    Scott County\nNA\n$47,024\nNA\n    Shelby County\n$37,689\n$49,664\n−$11,975\n    St. Clair County\n$20,160\n$32,148\n−$11,988\n    Stark County\nNA\n$52,614\nNA\n    Stephenson County\n$26,674\n$46,470\n−$19,796\n    Tazewell County\n$47,367\n$47,821\n−$454\n    Union County\n$40,506\n$48,030\n−$7,524\n    Vermilion County\n$28,462\n$39,597\n−$11,134\n    Wabash County\nNA\n$54,207\nNA\n    Warren County\n$37,185\n$48,815\n−$11,630\n    Washington County\n$46,406\n$57,554\n−$11,148\n    Wayne County\n$47,645\n$45,951\n$1,694\n    White County\n$31,250\n$48,981\n−$17,731\n    Whiteside County\n$45,106\n$45,458\n−$352\n    Will County\n$30,419\n$50,754\n−$20,335\n    Williamson County\n$38,083\n$42,193\n−$4,110\n    Winnebago County\n$20,346\n$37,655\n−$17,309\n    Woodford County\nNA\n$61,875\nNA\n  \n  \n  \n\n\n\n\nNote that cols_label allows us to adjust column labels by supplying the variable name and then the desired name of the column.\ngt provides a nice workflow example to show you step by step how you might apply formatting options to style a table. Also see the reference guide for specific formats.\n\n\nSaving a Table\nWe can easily use code to insert a table into our document, but what if you want to save it out as a separate file? The gtsave() command allows you to save your formatted table in a variety of formats.\n\n\nCode\nozs |&gt; \n  filter(state == \"Illinois\") |&gt; \n  group_by(county, Designated) |&gt; \n  summarise(\n    Income = mean(medhhincome2014_tract, na.rm=TRUE)) |&gt; pivot_wider(names_from = Designated, values_from = Income) |&gt; \n  mutate(Difference = Designated - `Not Designated`) |&gt; \n  ungroup() |&gt; \n  gt() |&gt; \n  fmt_currency(2:4, decimals = 0) |&gt; \n  cols_label(county = \"County\") |&gt; \n  gtsave(\"il_difference.png\", \"/output\")\n\n\nAnd here’s the actual table that was produced:"
  },
  {
    "objectID": "assignments/labs/04_describing_places/04_describing.html#lab-evaluation",
    "href": "assignments/labs/04_describing_places/04_describing.html#lab-evaluation",
    "title": "Describing Places",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html",
    "href": "assignments/labs/02_sharing/02_sharing.html",
    "title": "Sharing Your Work",
    "section": "",
    "text": "This lab focuses on getting your computer set up to communicate with Github and to introduce several workflows for publicly sharing your work. We will use these workflows throughout the course to share work and record feedback. Github represents a standard industry tool for code version control, collaborative development, debugging, and documentation.\nWhile code version control isn’t directly a focal point within practices of neighborhood analysis, we leverage it as a strategy to achieve one of our course goals. As we have discussed in our introductory framing to the course, transparency is an important principle of accountability within data analysis. Version control systems allow us to track changes in our code over time. These systems can help to facilitate accountability by making code accessible and easy to share. Version control systems also opens up the opportunities for collaboration, especially around the debugging of code.\n\n\n\n\n\nFor those people who have not used these systems before, there’s some core concepts to master. Once you do, integrating these systems into your workflow will become much easier. By the end of the class, you will have a lot of experience leveraging these tools as part of your workflows.\nVersion control systems like Github are also well integrated with several other tools that make it really easy to quickly and effectively share analysis via the internet. Again, this form of sharing is not integral to neighborhood analysis, but it does help us achieve a course goal focused on accessibility of analysis. While print and paper documents still rightfully have their place in our workflows, increasingly, our work is shared and consumed digitally. Being able to produce documents that are publicly accessible via the internet, therefore, becomes an important workflow for us to master.\n\n\nPlease read this lab background in its entirety before you proceed to engage the lab prompts."
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#introduction",
    "href": "assignments/labs/02_sharing/02_sharing.html#introduction",
    "title": "Sharing Your Work",
    "section": "",
    "text": "This lab focuses on getting your computer set up to communicate with Github and to introduce several workflows for publicly sharing your work. We will use these workflows throughout the course to share work and record feedback. Github represents a standard industry tool for code version control, collaborative development, debugging, and documentation.\nWhile code version control isn’t directly a focal point within practices of neighborhood analysis, we leverage it as a strategy to achieve one of our course goals. As we have discussed in our introductory framing to the course, transparency is an important principle of accountability within data analysis. Version control systems allow us to track changes in our code over time. These systems can help to facilitate accountability by making code accessible and easy to share. Version control systems also opens up the opportunities for collaboration, especially around the debugging of code.\n\n\n\n\n\nFor those people who have not used these systems before, there’s some core concepts to master. Once you do, integrating these systems into your workflow will become much easier. By the end of the class, you will have a lot of experience leveraging these tools as part of your workflows.\nVersion control systems like Github are also well integrated with several other tools that make it really easy to quickly and effectively share analysis via the internet. Again, this form of sharing is not integral to neighborhood analysis, but it does help us achieve a course goal focused on accessibility of analysis. While print and paper documents still rightfully have their place in our workflows, increasingly, our work is shared and consumed digitally. Being able to produce documents that are publicly accessible via the internet, therefore, becomes an important workflow for us to master.\n\n\nPlease read this lab background in its entirety before you proceed to engage the lab prompts."
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#goals",
    "href": "assignments/labs/02_sharing/02_sharing.html#goals",
    "title": "Sharing Your Work",
    "section": "Goals",
    "text": "Goals\n\nSet up your computer so that RStudio can communicate with Github.\nLearn several workflows for translating Quarto documents into simple websites.\nSubmit your previous lab via Github."
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#core-concepts",
    "href": "assignments/labs/02_sharing/02_sharing.html#core-concepts",
    "title": "Sharing Your Work",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\nTerminal\n\n\n\nGithub\n\nPush\nPull\nPull request\nRepository\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#github-lab-repository",
    "href": "assignments/labs/02_sharing/02_sharing.html#github-lab-repository",
    "title": "Sharing Your Work",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nLink to the Github lab repository. Please accept the Github Classroom assignment repository link for Lab 2: Sharing your Work. Before you attempt to do this, please see the section below on configuring your computer to talk to Github."
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#configure-your-computer-to-talk-to-github",
    "href": "assignments/labs/02_sharing/02_sharing.html#configure-your-computer-to-talk-to-github",
    "title": "Sharing Your Work",
    "section": "Configure Your Computer to Talk to Github",
    "text": "Configure Your Computer to Talk to Github\nOur first task is to configure your computer to talk to Github. This assumes that you already have a Github account set up, but that you have not linked this account to your local computer. A great resource that takes you step by step through Github workflows is Jenny Bryan’s Happy Git and GitHub for the useR. The recommended configuration below paraphrases the options found on that site.\nLet’s get started!\n\nRegister a Github Account\nGithub is an implementation of Git, which is a version control system designed to help you keep track of files (especially code) which are likely to be updated often. GitHub has some features similar to Dropbox or Box, but with far more emphasis on versioning your work and tracking changes. Within our class, we’ll use GitHub as a location where you’ll store work outputs which you will receive feedback.\n\n\nLet’s get this set up:\n\nIf you do not already have a GitHub account, go to GitHub.com and then click on Sign Up to create a new account. You will make a username (see some sage username advice here), enter your email address, and password, and then hit create account. I recommend using your UIUC email address for this step if you’re creating a new account (for my rationale, see the next step).\nGitHub offers some services for free which you can access with your basic account, however, as a student at an educational institution, you can register with GitHub for Education to receive enhanced benefits and services. You will be asked to verify your association with an academic institution, which you can do by entering your [netid]@illinois.edu email address and taking a picture of your I-Card.\n\n\n\n\n\n\n\n\nConfigure Your Local Machine to Talk to GitHub\nYou can upload files directly to a GitHub repository in the GitHub web interface, but as you create more complex files and file structures, that’s not going to be a feasible way to manage your work. That strategy also overlooks most of the features of what GitHub is at its core - a version control system.\n\n\n\n\n\nWe might want to version our work for several reasons:\n\nTo keep track of changes we’ve made, so that we can track when code works and when we’ve broken it\nTo be able to share code with collaborators and work on the same files at the same time and then reconcile and combine work all together\nTo be able to share code and data publicly with others\n\nAll three of these rationales are important motivations within the context of our class. There are two strategies which you may want to try to integrate GitHub into your R workflow.\nRStudio has GitHub functionality built right in, which is very handy for integrating version control into your R workflow. Assuming you’ve already got R and RStudio installed on your system and have already set up a GitHub account, start here:\n\nCheck to see if Git is installed on your computer, and if it isnt, Install Git.\nAssociate git on your computer with your GitHub account.\nInstall a Git client like GitHub Desktop.\nConnect to GitHub\nConnect RStudio to GitHub\nGet Started using GitHub integrated with R. This video shows you how to communicate with your Github repository using RStudio:\n\n\n\n\n\n\n\nWe will ease our way into using Github, and will also devote some time to troubleshooting in class. I promise, it’s going to make sense soon!"
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#communicating-with-github-from-rstudio.",
    "href": "assignments/labs/02_sharing/02_sharing.html#communicating-with-github-from-rstudio.",
    "title": "Sharing Your Work",
    "section": "Communicating with GitHub from Rstudio.",
    "text": "Communicating with GitHub from Rstudio.\nAlthough this lab is formally introducing GitHub’s functionality as part of our class, you have already gotten some experience interacting manually with GitHub as you needed to download a repository in order to complete your first lab. From here on out, you can use RStudio to communicate directly with GitHub so that pushing and pulling repositories can all happen within your RStudio session. By the end of this lab, you will have gained some experience pushing and pulling files (specifically your lab files). From here on out in class, we will assume that you will use GitHub to submit your code for review. Instructions will be clearly posted with both lab and homework assignments.\nWith those things in mind, let’s think about how we communicate with Github from RStudio.\n\nBasic Workflows\nAs you have read on the Happy with GitR site, GitHub’s utility comes from allowing us to store remote versions of our code which others can then access, comment on, and modify. GitHub allows us to track versions of our code, and if we need to, we can revert to earlier versions, or fork branches - make special versions of our code which we may want to experiment with or modify. We can also reconcile multiple versions of code back into a single main version, which is particularly useful when we are collaboratively writing or editing code.\nFor the most part, our course focuses on building a workflow involving a single code author, and not on building collaborative workflows, although you should try collaborating with each other on modifying code. Our end goal in this class is to develop workflows for transparency, accountability, and accessibility.\n\n\nReviewing Common Operations\n\n\n\nImage by Allison Horst from https://allisonhorst.com\n\n\n\nLocal vs. Remote\nLocal vs Remote: Let’s start with the basic distinction between local and remote versions of our work. Local versions are contained on the hard drive of the computer we are working on. Remote versions are in the cloud, in this case, on GitHub’s servers. We will typically work on editing data locally, and will then push our modifications to be integrated into the remote version.\n\n\nPush vs. Pull\nPush vs Pull: To understand this, it’s useful to think about what’s happening from the perspective of your local computer. We pull a version of the repository from the remote repository to our local computer. We push a version of the repository from our local computer to the remote repository.\n\n\nCommit\nCommit: As we make changes to our local repository, GitHub is comparing these changes to the prior version of our repository. When we commit changes, we are essentially asking GitHub to create a markerpost that we could return to in the future. We even label our commits with a brief descriptive note so we can remember what that marker represents. Note that when you commit changes locally, they are not automatically pushed to your remote repository - you have to do that as an extra step. You can, for instance, make multiple local commits to a local repository and then push them to your remote repository.\nCommits help to keep us organized. While your commits do not have to be linear or follow any particular order within your data and analysis workflow, they can help you track contributions to what has been done.\n\n\n\nImage by Allison Horst from https://allisonhorst.com\n\n\nCommits serve as failsafe points - we can always revert or rewind our code to where we made commits if we need to debug or if something goes wrong. \n\n\n\nCommon Operations\nSo how do we put these pieces together?\n\nIf we already have a remote repository with code or a file structure in it, we create a connection to that remote repository and create a local repository by pulling it from GitHub.\nWe then make changes to the repository - this might be in RStudio, or we might add, move, or delete things manually.\nWhen we get to a point where we want to create a failsafe point in our code, we commit the changes we made (and add a plain text description of what our commit contains).\nWe push our commit to our remote version where it will become to version of record (GitHub is tracking the changes we made, so if we need to revert to our previous commit, we could).\nJust to be safe, after we push our commits, we may want to pull from the remote server again just to confirm that the version on our local machine is up to date.\n\nFrom there, we can continue making changes until we are ready to make another commit.\n\n\n\nImage by Allison Horst from https://allisonhorst.com\n\n\n\n\nWorkflow - Setting up a Local Repository from an Existing GitHub Repository\nFor interaction with our labs (which are already generated repositories on GitHub), you’ll want to follow the instructions for “Existing Project, GitHub First” on the Happy with Git and GitHub for the useR site. The site presents multiple ways to do this. I suggest starting with 16.2.2 where the instructions walk through using the RStudioIDE to connect to an existing remote GitHub repository.\nFor all UP 570 labs and assignments, existing repositories are provided for you (when you click the lab URL to accept the repository, a user-specific version is created for you from a master copy stored on GitHub). You can then connect to this repository in RStudio, create a local copy on your computer, modify that local repository by making changes, adding data and writing, as well as proejct documentation. You can then stage the changes you’ve made, commit your changes locally, and push your changes to the remote repository on GitHub."
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#publish-using-quarto-pub",
    "href": "assignments/labs/02_sharing/02_sharing.html#publish-using-quarto-pub",
    "title": "Sharing Your Work",
    "section": "Publish using Quarto Pub",
    "text": "Publish using Quarto Pub\nQuarto Pub is a free service that allows you to quickly publish Quarto documents. Quarto Pub represents one of the easiest ways to publish documents.\n\n1. Create Account2. Preview3. Publish4. Options5. Finished!\n\n\nIn order to use Quarto Pub, you’ll need to create an account. Visit https://quartopub.com and create an account. I chose to use my GitHub username, but you can create an account with any name you wish (please keep in mind that the account name will become part of the URL which others will see).\n\n\n\nIt’s a good idea to preview your document before publishing it to see how it will look in a web browser. Render your document so that you can see what the final document will look like online.\n\n\nWhile we are using Quarto within RStudio, Quarto can also be run at the command line to render a range of document types. We will briefly use the command line in order to publish out document to Quarto Pub.\nIn your RStudio session, click on the terminal tab and type:\n\n\n\n\n\n\nTerminal\n\n\n\nquarto publish quarto-pub\n\n\nPut in plain language we are asking Quarto quarto to publish our document publish via Quarto Pub quarto-pub.\n\n\n\nYou may be prompted to log in to your Quarto Pub account on the internet before proceeding. Depeding upon the name of your document, you may also need to provide a shorter name (you will be prompted in the terminal window if that is the case). Quarto will then publish the document and will open a window that takes you to your published document.\n\n\n\nYou can then examine your published document at the URL that’s generated for you. For example: https://agreen4.quarto.pub/cca/\n\n\nIn the future if you need to update the document, you can run the publish step again."
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#publish-using-github-and-netlify",
    "href": "assignments/labs/02_sharing/02_sharing.html#publish-using-github-and-netlify",
    "title": "Sharing Your Work",
    "section": "Publish using GitHub and Netlify",
    "text": "Publish using GitHub and Netlify\nQuarto Pub represents a really fast and easy option for creating and sharing Quarto documents. We’ll also learn another more involved strategy which you may choose to employ. This involves pushing published code and HTML content that represents a publishable document or website to Github and then linking this repository to a web hosting service like Netlify which will then publish the site for you.\nThis strategy can be useful in that you end up with two deliverables - your code base and associated HTML files on GitHub and a website hosted by Netlify. Our course website - the very website you are reading right now - is published using this pipeline. The webpages are each a Quarto Markdown file. The files are pushed to a Github repository. Netlify links to this repository and when new updates are pushed to Github, Netlify “refreshes” the website, typically within a minute or less of the push being complete.\nIf you simply want a quick and easy way to share your work, Quarto Pub may be completely sufficient! If you want more advanced web hosting options and configurability (like domain management), Netlify may be a more appropriate solution. We will learn both strategies, and you should take the time to become comfortable using either strategy.\n\nWhat is Netlify?\nNetlify is a web development platform that automates the building a serving of websites. While certain aspects of the Netlify platform are monetized, the types of basic elements which we can use to create and serve a website are available for free.\nWhen we grant Netlify access to a GitHub repository, it looks for files that can be rendered as html and then serves those at a unique URL. When we commit and push updates to that repository, Netlify updates the files that are being served to the internet.\nQuarto Markdown can produce outputs in a variety of formats including html. Netlify is built around looking for html files in our GitHub repository. Typically, when we render Quarto Markdown as HTML locally, it will create a _site file in our root folder. That folder has all of the content that we want Netlify to serve on the internet.\nWith these things in mind, let’s try getting our lab files on Netlify!"
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#setting-up-our-project",
    "href": "assignments/labs/02_sharing/02_sharing.html#setting-up-our-project",
    "title": "Sharing Your Work",
    "section": "Setting Up our Project",
    "text": "Setting Up our Project\nWe have to make two tweaks to our project in order to get them to work properly with Netlify.\n\nAdd a _quarto.yml file\nAll Quarto websites require a special YAML file to tell them how to render. In a text editor of your choice, create a new file called _quarto.yml and save it in the project root directory.\nInside the _quarto.yml file, include the following:\nproject:\n  type: website\nNow when you render your project, it should include a folder called “_site” which contains a self-contained version of the elements of your document. You can explore the many other options to style your site here.\n\n\nRename our document index.qmd\nEvery website needs a homepage. By default, the name of this page is “index.html”. Netlify will always start serving from this page. Rename your Quarto document “index.qmd” and then render this to produce the associated html files.\n\n\nPush to Github\nPush your entire repository including the _site folder to Github.\n\n\nOpen Netlify\nNavigate to Netlify’s website. If you haven’t already done so, create a new account (you can log in and link this directly to your existing GitHub account).\nOnce you have an account, navigate to Sites and click “New Site”.\nNetlify will ask you if you want to create a site from an existing GitHub repository, which you do! Navigate to the repository that contains the lab files and _site folder you want to serve and select this repository.\n\n\n\n\n\n\nImportant\n\n\n\nThere’s one setting we need to modify in order to get this to work properly! Under Build settings, there’s a blank area called Publish directory. In this box, you need to point Netlify to _site as your specified publish directory - this is where Netlify will look for updated files to publish.\n\n\nAfter you do this, click ok. Netlify will build your site (this may take a few minutes) and share a url with you. When you click on it, you should see a live version of your website.\n\n\nNotes\nThis approach is a little more complex, but it does offer you a lot of flexibility to create sites for production. You will likely want to start off using Quarto pub, but as you progress in the complexity of your analysis, you may want to spend more time experimenting with Netlify as a web deployment platform."
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#lab-evaluation",
    "href": "assignments/labs/02_sharing/02_sharing.html#lab-evaluation",
    "title": "Sharing Your Work",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nSuccessfully create and publish a quarto page via Quarto Pub.\nSuccessfully create a publish a quarto page based upon your first lab assignment via GitHub and Netlify.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/02_sharing/02_sharing.html#references",
    "href": "assignments/labs/02_sharing/02_sharing.html#references",
    "title": "Sharing Your Work",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments Overview",
    "section": "",
    "text": "Strategy\nMaster’s Students\nPh.D. Students\n\n\n\n\nLearn\nLabs\nLabs\n\n\nApply\nPlace Selection Memo\nPlace Background Memo\nPopulation Memo\nPolicy Memo\nPaper Proposal\nPaper Outline\nPaper Draft\n\n\nCommunicate\nFinal Report\nTerm Paper\n\n\nEngage\nParticipation and Engagement\nParticipation and Engagement"
  },
  {
    "objectID": "assignments/index.html#assessment",
    "href": "assignments/index.html#assessment",
    "title": "Assignments Overview",
    "section": "Assessment",
    "text": "Assessment\nMost of the methods classes you have probably taken have focused on grading you on your skill at reproducing specific outcomes. I take a different approach in this class. While it is possible to measure your reproduction of specific outcomes, what is more important to assess is the process of learning, and an important proxy is the effort you put into that process. In this class, you will contract for a grade based upon your choices regarding your learning process and the amount of effort you want to put in. Here’s some more detail on my thinking:\n\nA unitary assessment of performance in a 500-level class is not likely to serve students well. Each of you has proven time and time again in other classes that you can reproduce certain expected outcomes. The challenge in this class will focus on assessing your effort and learning process as opposed to the reproduction of outcomes.\nImplementing a contract-based assessment approach mirrors course goals regarding our understanding and practice around the power of data, particularly with regards to public deliberation, decision-making, and governance. Dealing differently with power around evaluation in our classroom mirrors how we might deal differently with power and power relationships in professional practice.\nThere is increasing evidence in higher education settings that unitary grading measures tend to hamper individual learning, and have the potential to be biased towards certain types of students. Put simply, obsessing about grades in advanced coursework is counter-productive to advanced learning. This course adheres to the pedagogy that when given more control and ownership over the terms of evaluation, a student is more likely to embrace the challenges presented to them within the classroom, and are more likely to take ownership of their work (see, for instance Elbow and Inoue).\n\n\nContract Expectation Summary\nEach of you begins the semester with an “A” in the class. You may alter your grade based upon the choices you make regarding the following accountability expectations:\n\n\n\nGrade\nAbsences\nLate\nMissed\nIgnored\n\n\n\n\nA\n3\n1\n0\n0\n\n\nB\n3\n2\n1\n0\n\n\nC\n4\n2\n1\n1\n\n\nD\n5\n3\n2\n1\n\n\nF\n6\n4\n3\n1\n\n\n\n\nAbsences indicate the number of times you are absent from class. Additional excused absences may be allowed due to documented extenuating circumstances.\nLate Assignments are assignments and labs submitted after a due date but within 48 hours of the due date.\nMissed Assignments are assignments and labs submitted more than 48 hours after the due date.\nIgnored Assignments are assignments and labs that are not submitted by the last day of class (excluding your final assignment). Ignored assignments are a more serious breach of contract than missed assignments as they reflect contracted work which you have not completed.\n\nPlease note that assessment of these contracted standards are based upon what you do. The contracted grade does assess of the quality of your work. You will reflect upon and will receive direct feedback on the quality of your work as part of the assignment review process. Note that I reserve the right to adjust your grade based upon exceptionally strong or weak engagement within the class. I will provide sufficient feedback over the course of the semester so that your final grade in the class should not be a surprise based upon what you do and how you reflect upon your learning over the course of the semester."
  },
  {
    "objectID": "assignments/05_term.html",
    "href": "assignments/05_term.html",
    "title": "Term Assignment",
    "section": "",
    "text": "For your term assignment, you will independently apply the skills and principles of data analysis we’ve learned over the course of the semester to produce a policy-relevant analysis and report. Building upon your memorandums, produce a publication ready policy report for your place as well as complimentary reproducible documentation.\nAt a minimum, your report to address the following:\n\nContext and Background: Drawing upon secondary sources, craft a narrative context and background for your analysis. This may include in-depth profiling of the neighborhood(s) of interest or may involve broader contextualizing drawing from the history and background of your place. Your narrative does not have to be long, but it should help to contextualize the questions you are engaging with and the methods you use to engage those questions.\nMethods and Approach: Describe your approach to analysis - what is the specific question or problem you are taking on? What precedents or work that others have done is helping to inform your approach? Why is the selected geographic scale of your analysis appropriate?\nData Sources: Describe the data source(s) that you are using for your analysis. Where are these data coming from? What are each contributing? What are the specific indicators you are using from these data?\nData Description, Analysis and Interpretation: Please describe the analysis which you’ve done with the data, as well as your written interpretation based upon your analysis.\nImplications for Policy and Impact: Share your thoughts on how your findings can or should influence policy or otherwise have an impact within your place. You may also wish to provide some thoughts on how you or others may expand upon this work in the future or describe a set of future questions or analyses that build upon what you’ve done.\n\nThe actual structure of your report is up to you, and will likely vary depending upon your place and the questions you seek to engage."
  },
  {
    "objectID": "assignments/05_term.html#introduction",
    "href": "assignments/05_term.html#introduction",
    "title": "Term Assignment",
    "section": "",
    "text": "For your term assignment, you will independently apply the skills and principles of data analysis we’ve learned over the course of the semester to produce a policy-relevant analysis and report. Building upon your memorandums, produce a publication ready policy report for your place as well as complimentary reproducible documentation.\nAt a minimum, your report to address the following:\n\nContext and Background: Drawing upon secondary sources, craft a narrative context and background for your analysis. This may include in-depth profiling of the neighborhood(s) of interest or may involve broader contextualizing drawing from the history and background of your place. Your narrative does not have to be long, but it should help to contextualize the questions you are engaging with and the methods you use to engage those questions.\nMethods and Approach: Describe your approach to analysis - what is the specific question or problem you are taking on? What precedents or work that others have done is helping to inform your approach? Why is the selected geographic scale of your analysis appropriate?\nData Sources: Describe the data source(s) that you are using for your analysis. Where are these data coming from? What are each contributing? What are the specific indicators you are using from these data?\nData Description, Analysis and Interpretation: Please describe the analysis which you’ve done with the data, as well as your written interpretation based upon your analysis.\nImplications for Policy and Impact: Share your thoughts on how your findings can or should influence policy or otherwise have an impact within your place. You may also wish to provide some thoughts on how you or others may expand upon this work in the future or describe a set of future questions or analyses that build upon what you’ve done.\n\nThe actual structure of your report is up to you, and will likely vary depending upon your place and the questions you seek to engage."
  },
  {
    "objectID": "assignments/05_term.html#design-considerations",
    "href": "assignments/05_term.html#design-considerations",
    "title": "Term Assignment",
    "section": "Design Considerations",
    "text": "Design Considerations\n\nYou have all of the raw material you need from your prior memorandums, however, you probably received comments from your peers and your instructor about how you might refine your analysis. Now is the time to implement that advice.\nYou are not required to include all of the tables and graphics you produced for your drafts and exercises. Narrow the focus to the most significant trends.\nYou are encouraged to bring in other material (e.g., existing reports or studies) that will help you provide context for your discussion or help you interpret trends. Cite accordingly.\nYou should strive to write this report in fewer than 4,000 words, not including references, tables, figures, or appendices. The aim is to produce a narrative that is concise and focused. Highlight what matters in the context of your narrative. Follow the strategies for effective professional writing—including use of active headings and concise reporting of trends—to convey the information succinctly. Title your report appropriately."
  },
  {
    "objectID": "assignments/05_term.html#outputs",
    "href": "assignments/05_term.html#outputs",
    "title": "Term Assignment",
    "section": "Outputs",
    "text": "Outputs\n\nReport: Prepare a professional report of around 4,000 words. In April, you will share a draft report and will receive feedback during a peer review session. Your final report will be due after the conclusion of classes. Your report may be formatted as a stand alone document or may be shared as a website.\nPresentation: Prepare and deliver an 8-minute professional presentation that summarizes your policy issue, analysis, and policy recommendations. We will complete written feedback and reactions that will be shared after the conclusion of your presentation."
  },
  {
    "objectID": "assignments/05_term.html#submission-instructions",
    "href": "assignments/05_term.html#submission-instructions",
    "title": "Term Assignment",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFollow this link to accept the lab Github Classroom assignment repository. Save your memorandum as a Quarto markdown document and upload along with your post-assignment reflection to GitHub.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "assignments/03_population.html",
    "href": "assignments/03_population.html",
    "title": "Population Memorandum",
    "section": "",
    "text": "Your prior two memorandums focused on using secondary sources to scope analysis for your place. In this memorandum you will draw upon multiple sources of available quantitative data to describe the population of your place and their characteristics. This background on the population will help you to identify a specific policy issue or other questions which you will analyze in your next memorandum."
  },
  {
    "objectID": "assignments/03_population.html#introduction",
    "href": "assignments/03_population.html#introduction",
    "title": "Population Memorandum",
    "section": "",
    "text": "Your prior two memorandums focused on using secondary sources to scope analysis for your place. In this memorandum you will draw upon multiple sources of available quantitative data to describe the population of your place and their characteristics. This background on the population will help you to identify a specific policy issue or other questions which you will analyze in your next memorandum."
  },
  {
    "objectID": "assignments/03_population.html#goals",
    "href": "assignments/03_population.html#goals",
    "title": "Population Memorandum",
    "section": "Goals",
    "text": "Goals\n\nIntegrate analytical techniques learned from labs into targeted demographic analysis of your place\nCritically assess the story of your region and use it as a framework for writing and description\nDevelop baseline analysis that supports future policy analysis"
  },
  {
    "objectID": "assignments/03_population.html#your-population-memorandum",
    "href": "assignments/03_population.html#your-population-memorandum",
    "title": "Population Memorandum",
    "section": "Your Population Memorandum",
    "text": "Your Population Memorandum\nPrepare a memorandum of around 2,000 words (not including tables and graphics) that provides an overview of the population of your place, and that identifies important policy-relevant population subgroups. Your memorandum should summarize for your place major characteristics and trends in:\n\nPopulation and population change;\nRacial and ethnic composition;\nAge structure;\nIncome, wages, and poverty;\nEducational attainment and employment by industry\n\nThe objective is to provide a clear portrait of the demographic characteristics of your place and how they are changing over time and space. Support your discussion with tables and figures as appropriate, and include at least one map displaying the geography of your place."
  },
  {
    "objectID": "assignments/03_population.html#memorandum-preparation-tips",
    "href": "assignments/03_population.html#memorandum-preparation-tips",
    "title": "Population Memorandum",
    "section": "Memorandum Preparation Tips",
    "text": "Memorandum Preparation Tips\n\nProvide some context, namely information on the area’s physical features and a very brief history. This does not need to be done in a separate section and it should not adopt the “booster” language of your typical Chamber of Commerce brochure. Sometimes it can be accomplished in a sentence or two, or in a few well-placed sentences in the report.\nLook for a story line in the data. Often there are some important local trends (sustained economic distress, rapid economic growth, high rates of international migration, etc.) that provide a good organizing framework for more specific social and demographic trends.\nYou already have most of the data and code you need from class lab sessions. You should plan on drawing from these, but also expanding upon them. Expansion may mean looking at data sources or indicators we have not explored before. We are looking for evidence that you are using the most appropriate information to follow through on your lines of argumentation. What is important is dictated by… your place!\nLikewise, you are not required to include all of the types of tables, graphics, and analyses you produced for the prior exercises. Now is the time to narrow your focus to the most significant local characteristics and trends.\nFeel free to incorporate other material (e.g., existing studies or reports) that will help you provide context for your discussion or help you interpret trends. Cite accordingly.\nYou should strive to write this memorandum in fewer than 2,000 words, not including references, tables, and figures. Two thousand words is a maximum, not a minimum. Follow the strategies for effective professional writing—including use of active headings and concise reporting of trends—to convey the information concisely."
  },
  {
    "objectID": "assignments/03_population.html#submission-instructions",
    "href": "assignments/03_population.html#submission-instructions",
    "title": "Population Memorandum",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFollow this link to accept the lab Github Classroom assignment repository. Save your memorandum as a Quarto markdown document and upload along with your post-assignment reflection to GitHub.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "assignments/01_place_selection.html",
    "href": "assignments/01_place_selection.html",
    "title": "Place Selection Memorandum",
    "section": "",
    "text": "A very important early task in Neighborhood Analysis is to select the place that you will spend time examining in depth this semester. The place you select will be the location that you focus your social, economic, and demographic analysis on. Given the structure of this class, you need to choose a location in the United States.\nYour work will include thinking about how that place fits within its larger “region.” What do we mean by “place”? It might be a city or town—the City of Champaign—or even twin cities—the cities of Champaign and Urbana. It could also be a county. For our purposes, it is not an entire metropolitan area or a larger macro region (such as the Great Plains or Mississippi Delta) - these would be considered part of the place’s region."
  },
  {
    "objectID": "assignments/01_place_selection.html#introduction",
    "href": "assignments/01_place_selection.html#introduction",
    "title": "Place Selection Memorandum",
    "section": "",
    "text": "A very important early task in Neighborhood Analysis is to select the place that you will spend time examining in depth this semester. The place you select will be the location that you focus your social, economic, and demographic analysis on. Given the structure of this class, you need to choose a location in the United States.\nYour work will include thinking about how that place fits within its larger “region.” What do we mean by “place”? It might be a city or town—the City of Champaign—or even twin cities—the cities of Champaign and Urbana. It could also be a county. For our purposes, it is not an entire metropolitan area or a larger macro region (such as the Great Plains or Mississippi Delta) - these would be considered part of the place’s region."
  },
  {
    "objectID": "assignments/01_place_selection.html#goals",
    "href": "assignments/01_place_selection.html#goals",
    "title": "Place Selection Memorandum",
    "section": "Goals",
    "text": "Goals\n\nIdentify a place that you will focus your analytic work on over the course of the semester\nThink critically about place, geography, and audience"
  },
  {
    "objectID": "assignments/01_place_selection.html#place-selection-considerations",
    "href": "assignments/01_place_selection.html#place-selection-considerations",
    "title": "Place Selection Memorandum",
    "section": "Place Selection Considerations",
    "text": "Place Selection Considerations\n\nBig places—for instance the cities of Chicago, Los Angeles, and New York—are interesting. They are socioeconomically, racially, and ethnically diverse and it is usually not hard to find stories in the data with respect to neighborhood trends. However, large cities are very complex. Properly understanding the past, present, and future of such places and their neighborhoods can be a herculean task. Unless you are very familiar with a big place, I suggest focusing on a smaller place. Another option for working on a big place might be to pick a subregion of a large place (the borough of Queens in New York City, or the south side of Chicago, for example).\nMany of the techniques you will learn use methods that draw on data available at the census tract level. Census tracts are frequently used as (imperfect) proxies for neighborhoods. As you work to select your place, you need to consider the number of census tracts are within your place. As a rule of thumb, your place should have no fewer than 15 tracts. More is certainly ok, but let’s talk if you are considering a place with fewer than 15 tracts.\nConsider selecting a place in a broader area of particular policy concern, such as an economically distressed region (central Appalachia, the Mississippi Delta, or the Black Belt). Many communities in the Great Plains and Mountain states are not distressed by conventional measures but they face significant out-migration and population decline, which introduce their own challenges at the local level. Places around international borders can also be challenging yet interesting to examine.\nConsider selecting a place within driving distance of Champaign-Urbana (although you may not select Champaign County or it’s constituent cities as your place). Making a site visit to the county and its principal urban centers could help you understand its conditions and context better, giving you the ability to write about it with more authority.\nHaving some personal experience in the place you select can be helpful. Many students select their hometown or other community/county in which they have lived, provided the criteria above are satisfied. Some students also select places there are interested in living or working in.\nIslands and territories (such as Puerto Rico and Hawaii) and remote areas (such as Alaska) are interesting, but geography and data availability can make them challenging places to analyze. Students have successfully analyzed all of these places in recent years- but with extra effort needed to address data gaps and limitations. You may want to explore these issues before committing to one of these places.\nIf you have any questions about the places you are considering, or you wish to talk through your choices, please talk with me."
  },
  {
    "objectID": "assignments/01_place_selection.html#your-place-selection-memorandum",
    "href": "assignments/01_place_selection.html#your-place-selection-memorandum",
    "title": "Place Selection Memorandum",
    "section": "Your Place Selection Memorandum",
    "text": "Your Place Selection Memorandum\nPrepare a memorandum of around 500 words that identifies the place that you propose focusing your analysis on this semester.\nAt a minimum, the narrative associated with your memorandum should address the following:\n\nWhat is the name of the place which you have selected, and where is it located?\nWhat is the specific geography associated with this place (city, county, etc.)?\nWhat characteristics of this place make its neighborhoods important to examine?\nBased upon your preliminary background research, what types of policy issues may be important to examine through your analysis?"
  },
  {
    "objectID": "assignments/01_place_selection.html#submission-instructions",
    "href": "assignments/01_place_selection.html#submission-instructions",
    "title": "Place Selection Memorandum",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFollow this link to accept the lab Github Classroom assignment repository. Save your memorandum as a Quarto markdown document and upload along with your post-assignment reflection to GitHub.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "assignments/02_place_background.html",
    "href": "assignments/02_place_background.html",
    "title": "Place Background Memorandum",
    "section": "",
    "text": "In your place selection memorandum, you identified a place to focus on and provided a preliminary justification for your choice. In this memorandum, you will leverage additional research to identify priority populations, issues, and neighborhoods, and also to lay out an analysis roadmap that will guide your subsequent analysis."
  },
  {
    "objectID": "assignments/02_place_background.html#introduction",
    "href": "assignments/02_place_background.html#introduction",
    "title": "Place Background Memorandum",
    "section": "",
    "text": "In your place selection memorandum, you identified a place to focus on and provided a preliminary justification for your choice. In this memorandum, you will leverage additional research to identify priority populations, issues, and neighborhoods, and also to lay out an analysis roadmap that will guide your subsequent analysis."
  },
  {
    "objectID": "assignments/02_place_background.html#goals",
    "href": "assignments/02_place_background.html#goals",
    "title": "Place Background Memorandum",
    "section": "Goals",
    "text": "Goals\n\nAdd more thought and detail to your previous place selection memorandum\nDraw upon a range of sources to critically analyze the place you selected\nUse research to develop priorities for exploratory analysis"
  },
  {
    "objectID": "assignments/02_place_background.html#your-place-background-memorandum",
    "href": "assignments/02_place_background.html#your-place-background-memorandum",
    "title": "Place Background Memorandum",
    "section": "Your Place Background Memorandum",
    "text": "Your Place Background Memorandum\nDrawing from external sources and your own research, prepare a memorandum of around 2,000 words that provides background on your chosen place. Your memorandum should provide some general background on your place, and should then identify and describe one or several key issues that will guide your subsequent analysis and work over the course of the semester. Key issues should be guided by a combination of your interests and what you can justify as relevant given your background research.\nBelow, please find some guiding questions to help you structure your memorandum:\n\nWhat is the overall history of your place? What are the types of issues that have mattered in the past, and what types of issues matter at present?\nWhich neighborhoods or subareas of your place are frequently mentioned in local plans, reports, or journalistic accounts? What differentiates those neighborhoods? At what geographic scale can these subareas be classified or analyzed?\nBased upon your background research, are there any population groups that are likely to warrant special attention. Why?\nBased upon your background research, are there policy issues or debates that warrant special attention or that could benefit from detailed analysis. What are these, and what factors are important for understanding them?\nBased upon your exploration, identify key themes and indicators which you believe will be important to focus on in your subsequent analyses. What types of strategies do you intend to use in order to examine these themes and indicators?"
  },
  {
    "objectID": "assignments/02_place_background.html#submission-instructions",
    "href": "assignments/02_place_background.html#submission-instructions",
    "title": "Place Background Memorandum",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFollow this link to accept the assignment Github Classroom assignment repository. Save your memorandum as a Quarto markdown document and upload along with your post-assignment reflection to GitHub.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "assignments/04_policy.html",
    "href": "assignments/04_policy.html",
    "title": "Policy Memorandum",
    "section": "",
    "text": "In your prior memoranda you developed a preliminary narrative around your place based upon the characteristics of the population. Simultaneously, in your labs, you’ve been exposed to a range of analysis strategies. In this memorandum, you will identify a specific policy-based problem and will develop and implement an appropriate analysis to better inform the problem. This is the final sub-element of your final policy report which will draw from your prior writing and analysis to present a polished report for your place."
  },
  {
    "objectID": "assignments/04_policy.html#introduction",
    "href": "assignments/04_policy.html#introduction",
    "title": "Policy Memorandum",
    "section": "",
    "text": "In your prior memoranda you developed a preliminary narrative around your place based upon the characteristics of the population. Simultaneously, in your labs, you’ve been exposed to a range of analysis strategies. In this memorandum, you will identify a specific policy-based problem and will develop and implement an appropriate analysis to better inform the problem. This is the final sub-element of your final policy report which will draw from your prior writing and analysis to present a polished report for your place."
  },
  {
    "objectID": "assignments/04_policy.html#goals",
    "href": "assignments/04_policy.html#goals",
    "title": "Policy Memorandum",
    "section": "Goals",
    "text": "Goals\n\nDraw from your prior population profiles and analyses to identify a policy issue or question for analysis.\nBuild off of the analysis strategies we’ve learned about in class to develop an appropriate analytic strategy for the identified policy issue of your choice."
  },
  {
    "objectID": "assignments/04_policy.html#memorandum-preparation-tips",
    "href": "assignments/04_policy.html#memorandum-preparation-tips",
    "title": "Policy Memorandum",
    "section": "Memorandum Preparation Tips",
    "text": "Memorandum Preparation Tips\n\nThe challenge of this assignment is picking an appropriate policy question to engage with. Please rely upon each other as peer reviewers and on your instructors for help in identifying an appropriate policy or research question to focus your memorandum on.\nBased upon your prior work in choosing a place and in analyzing some of the main demographic characteristics of that place, you likely have a good sense for where to go with this assignment. If you are struggling to identify a policy issue that is of interest or struggling to think about how you will conceptualize your analysis of that issue, please come speak with us early so that we can provide feedback and guidance.\nPolicy analysis depends upon following a logical chain of thought through existing policies and questions. Be sure to review and understand these before attempting to develop your own approach. Please also think carefully and critically about how your your chosen analysis may iterate over various neighborhoods in your place.\nAt this point in the semester, I expect to see refined maps, data visualizations, and tables in your memos. Please allocate appropriate time and effort to refine these elements of your memo.\nThink through how the story of your policy analysis relates back to the other stories you’ve told in previous memos over the course of the semester. Your final assignment depends upon you integrating these items into a final policy report, so having a good understanding of how these stories are related will make it easier to develop a coherent narrative in your final report."
  },
  {
    "objectID": "assignments/04_policy.html#submission-instructions",
    "href": "assignments/04_policy.html#submission-instructions",
    "title": "Policy Memorandum",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nFollow this link to accept the lab Github Classroom assignment repository. Save your memorandum as a Quarto markdown document and upload along with your post-assignment reflection to GitHub.\n\nAssignment Evaluation\nWe are looking for a well-told story that connects us to the policy issue in question, identifies who is impacted, and provides analysis in service of informing policy deliberation, reflection, or action. Our feedback on this assignment will focus on the following elements:\n\nA clear understanding and command of the policy issue in question, drawing from local and national evidence.\nA clear sense for how neighborhood analysis techniques might be applied to describing the policy issue.\nA clear description of the impacts of the policy issue and a clear sense of who and where these impacts are likely to occur.\nA clear and coherent analysis that adapts some of the techniques we have used over the course of the semester.\n\n\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "assignments/06_phd.html",
    "href": "assignments/06_phd.html",
    "title": "Ph.D. Term Assignment",
    "section": "",
    "text": "Ph.D. students in UP 570 have the option of either completing the series of assignments which Master’s degree students complete, or may opt to complete a term assignment that is focused on conducting research in an area of your choice that incorporates some of the principles, tools, and strategies from the class. The goal is to produce a complete draft of a scholarly article suitable for publication in a peer reviewed journal by the end of the semester."
  },
  {
    "objectID": "assignments/06_phd.html#introduction",
    "href": "assignments/06_phd.html#introduction",
    "title": "Ph.D. Term Assignment",
    "section": "",
    "text": "Ph.D. students in UP 570 have the option of either completing the series of assignments which Master’s degree students complete, or may opt to complete a term assignment that is focused on conducting research in an area of your choice that incorporates some of the principles, tools, and strategies from the class. The goal is to produce a complete draft of a scholarly article suitable for publication in a peer reviewed journal by the end of the semester."
  },
  {
    "objectID": "assignments/06_phd.html#outputs",
    "href": "assignments/06_phd.html#outputs",
    "title": "Ph.D. Term Assignment",
    "section": "Outputs",
    "text": "Outputs\n\nPaper Proposal\nPrepare a 3-4 page paper proposal that describes the research question, rationale, and approach which you propose taking over the course of the semester. Your proposal should highlight and review 5-10 relevant key sources (at a minimum), should articulate a clear rationale for the methods you intend to use, should describe any specific skills or methods which you propose developing or applying over the course of the semester, and should describe the overall analytic outputs and prospective audience for your work. Your proposal should also describe several candidate publication venues (journals) for which you might submit your article to.\nDue Date: February 2\nPaper Proposal Repository \n\n\nPaper Outline\nPrepare an annotated paper outline that fleshes out the structure of your paper including a complete written introduction, summary of the argument(s) you intend to make in your literature review, research question(s), a written summary of your methods, and a proposed structure for your results, discussion, and conclusion. The detail you invest in this outline will help us to provide feedback to help you as you develop your paper draft. In your outline, indicate the prospective journal you plan to submit your final paper to and ensure that your outline conforms to the structure of articles submitted to that journal.\nDue Date: March 4\nPaper Outline Repository \n\n\nPaper Draft\nPrepare a complete first draft of your paper that includes an introduction, literature review, research questions, methods section, results, discussion, and conclusion (again, conforming to the accepted structure of articles in the journal you are submitting to).\nDue Date: April 15\nPaper Draft Repository \n\n\nFinal Paper\nPrepare a polished draft of your paper incorporating feedback from your previous draft. Separate from your main document, include 1-2 page summary that is suitable for a non-technical non-academic audience that summarizes key takeaways.\nDue Date: May 10\nFinal Paper Repository"
  },
  {
    "objectID": "assignments/06_phd.html#submission-instructions",
    "href": "assignments/06_phd.html#submission-instructions",
    "title": "Ph.D. Term Assignment",
    "section": "Submission Instructions",
    "text": "Submission Instructions\nWhen you are ready to begin work on each assignment component, please accept the following GitHub Classroom assignments (one for each component). Upload your document, reflection and any associated files. Due to GitHub’s file size limitations, you may need to store large files (datasets, etc.) on Box and reference them separately.\n\nAssignment Reflection\nIn the repository you downloaded for your assignment, you will find a separate reflection document. Please respond to the following prompts in that document and submit along with your assignment repository.\n\nHighlight one or two things you are especially proud of regarding your submission. This could be a particular element within the assignment or could be part of your process (e.g. time management, applying new techniques, etc.).\nIf you were to start this assignment over again, what are one or two things that you might do differently?\nAre there any aspects or areas in your submission where you would like us to focus our feedback?\nOn a scale of 1 to 10, please rate how ready you feel this work is for sharing with a public audience (where 1 is not at all ready to be shared and 10 is polished and ready for public dissemination)."
  },
  {
    "objectID": "assignments/labs/01_datapipeline/01_datapipeline.html",
    "href": "assignments/labs/01_datapipeline/01_datapipeline.html",
    "title": "Building a Data Pipeline",
    "section": "",
    "text": "In this lab, we’ll explore some of the basic workflows which we’ll use over the course of the semester to package and share analysis. A primary principle of our class is that our analysis be reproducible by others - this is an important principle both for supporting the validity of our arguments, and also accountability to those neighborhoods and communities we are analyzing and to the broader audience who may use our work in their deliberation or who might adapt our workflows to address similar questions in their areas of focus.\nOur class will focus on developing data analysis and sharing pipelines that make use of R and RStudio’s core functions. We’ll gain familiarity with RStudio’s Quarto markdown format which allows for the integration of plain text, code, and code output in the same documents and which is focused on easily translating text and code into multiple output formats. We’ll also become more familiar with Github, which we’ll use as a tool to share code and analysis."
  },
  {
    "objectID": "assignments/labs/01_datapipeline/01_datapipeline.html#introduction",
    "href": "assignments/labs/01_datapipeline/01_datapipeline.html#introduction",
    "title": "Building a Data Pipeline",
    "section": "",
    "text": "In this lab, we’ll explore some of the basic workflows which we’ll use over the course of the semester to package and share analysis. A primary principle of our class is that our analysis be reproducible by others - this is an important principle both for supporting the validity of our arguments, and also accountability to those neighborhoods and communities we are analyzing and to the broader audience who may use our work in their deliberation or who might adapt our workflows to address similar questions in their areas of focus.\nOur class will focus on developing data analysis and sharing pipelines that make use of R and RStudio’s core functions. We’ll gain familiarity with RStudio’s Quarto markdown format which allows for the integration of plain text, code, and code output in the same documents and which is focused on easily translating text and code into multiple output formats. We’ll also become more familiar with Github, which we’ll use as a tool to share code and analysis."
  },
  {
    "objectID": "assignments/labs/01_datapipeline/01_datapipeline.html#goals",
    "href": "assignments/labs/01_datapipeline/01_datapipeline.html#goals",
    "title": "Building a Data Pipeline",
    "section": "Goals",
    "text": "Goals\n\nBecome familiar with core programmatic ways to manipulate files in R and RStudio.\nBecome familiar with the core features and outputs related to Quarto.\nSet up your computer so that RStudio can communicate with your Github account."
  },
  {
    "objectID": "assignments/labs/01_datapipeline/01_datapipeline.html#core-concepts",
    "href": "assignments/labs/01_datapipeline/01_datapipeline.html#core-concepts",
    "title": "Building a Data Pipeline",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\nProject\nWorking Directory\nWorkspace\nQuarto Markdown\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/01_datapipeline/01_datapipeline.html#github-lab-repository",
    "href": "assignments/labs/01_datapipeline/01_datapipeline.html#github-lab-repository",
    "title": "Building a Data Pipeline",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/01_datapipeline/01_datapipeline.html#workflow-1-set-up-a-new-project",
    "href": "assignments/labs/01_datapipeline/01_datapipeline.html#workflow-1-set-up-a-new-project",
    "title": "Building a Data Pipeline",
    "section": "Workflow 1: Set up a New Project",
    "text": "Workflow 1: Set up a New Project\nThe first workflow we need to develop is proper project setup in R. In our next lab, we’ll add some more complexity to this project setup by incorporating version control and sharing, but we’ll start by getting a handle on projects that reside on our local computer. Let’s start with a quick review of file system and structure, and then transition into the nuts and bolts of project setup.\n\nFile System and Structure\n\nWhen you started learning R and RStudio, you likely struggled a bit with some of the core concepts related to how R views your computer’s file system and where it looks for files and stores work. There’s nothing to complex about what R is doing, but as computers have become better at making sensible decisions for us, it can be easy to lose track of some of the basics of how we need to set up file structures that help us to actively manage our workflows and outputs.\nThere are a few basics which we need to have down. The first is a gentle reminder about how computer file systems work. Back when I was a kid in the early 1980s, the primary way we interacted with computers was at the command line. When you started a computer, you might be greeted with something that looks like the terminal tab in your RStudio session:\n From here, you’d type commands to either navigate between directories, list files in a directory, or execute files. For instance, we can use ls to list all the files in my home directory: \nAnd then take a look at the results: \nWhich is the equivalent of looking at what files are in my computer’s home directory in a graphical user interface: \nWhile for the most part we wont be bothered with accessing our file system this way, it’s useful to understand what’s going on with the pathnames that are shown. Whether you’re used to working with Mac, PC, or Linux operating systems, the approach to file systems is sufficiently similar to talk about them all together.\nMost modern operating systems operate using relative path names which allow us to be more efficient in referencing where files are located in our computer. The path that you saw above on my computer points to the “home directory” which is the base directory associated with my user account on the computer. My home directory is situated however within other directories that are part of the operating system and that are ultimately on a hard disk drive.\n So why is all of this important? This underscores the rationale for and utility of relative paths in our filesystem. My mac computer’s home directory is Machinsosh HD/Users/andrewgreenlee/ which means that rather than having to type out all of that content, I can just start instructions to paths below that directory. For instance, let’s navigate to my desktop folder: \nWhich is the same as navigating from the root folder of our hard drive to the desktop folder: \nThis certainly makes things more convenient for navigating files in Terminal and in our file system. It has a much deeper utility when we are thinking about reproducible data analysis. Let’s explore in more detail.\n\nDirectory Structure and R\nOftentimes when people begin learning in R and RStudio, they simply open the program and start typing commands. This works well up until the point when one needs to get data into or out of R. In general, R is agnostic about where to look for files - if you open RStudio without specifying where to look for files, it assumes you want to work out of your home directory (the getwd() command returns the current working directory R is using): \nWe typically do not want to work out of our home directory, but would like to set another directory to work out of. When new R users start to work in R, many are taught to use setwd() to explicitly set a directory to work out of. This will work, and sets a “home” directory for the particular work you are doing.\n\n\nCode\nsetwd(\"Desktop/andrew-home/neighborhood_types/\")\n\n\nFor the sake of illustration, let’s say we wnat to open a dataset containing a list of community area definitions for Chicago:\n\n\nCode\ndataset &lt;- read.csv(\"CommAreas.csv\")\ndataset\n\n\nWe create a script in our working directory that sets the directory, reads the file, and then shows the file to us.\nWhat happens when you move your working folder to another computer or share it? If you explicitly set your working directory, it won’t be able to access that directory which is at a path that doesn’t exist on your other computer:\n R Projects help us to address this problem. Projects add a file to the folder we wish to set as our home directory for our project which establishes where R will look for files.\nIn the right top corner of your RStudio window, you’ll note a blue cube which leads to a project management menu:\n If you click “New Project”, a dialog box will open up which allows you to specify project options:\n\nYou have options here - you can either create a new project in a new folder, use an existing folder which you’ve already created, or check a project out from Github or another vesion control system. Since we already have a directory we’ve been working out of, we can use the existing directory dialog, and then set the directory where our script and data are located.\n\nFrom here, so long as we open our work by clicking on the project file, we’ll be taken into the project which will remember what the “home” directory is for our work. \nThe benefit of this approach is that if we move our project directory to another computer and open our project, R understands that the folder with the project file reflects the relative path to where we will find the other files referenced in our project. This means that if we share our working folder with others, they can reproduce what we did so long as all files are contained within that project directory.\nProject files represent mileposts that set the relative path at which RStudio will look for files within our particular project. This is useful for creating a project file structure within which we can create reproducible and easily shareable analysis. You should be in the habit of starting each new analysis you do by setting up a new work folder with a new project file.\n\n\nProjects vs. Workspaces\nEach time you close your RStudio session, you will be faced with a temptation - RStudio will ask whether you want to save your workspace.\n\nWhat’s the difference between a script, a project, and a workspace?\n\nScript: A script contains the code which tells R what you want it to do. Script commands are run in the console in the order they are written. You want to get in the habit of saving your script files often.\nProject: Your project file tells R the path to look for files and folders at. This sets a relative path for your work in that project.\nWorkspace: A workspace file saves the exact image of what’s going on when you close out of RStudio - that may include loaded datasets, functions, and objects.\n\nYou have likely been tempted to save your workspace when you are closing your R session. While tempting, I encourage you not to get in the habit of saving your workspace. This runs counter to most other interactions you’ve had with computers - saving documents, video games, or survey responses. Saving your workspace can lead to bad habits with regards to reproducibility. You want to allow your scripts to serve as a repository for all steps that go from your raw data to your analysis and outputs. Unless your script has time or computationally intensive steps, your scripts and computer can re-run your workflow to return to the point that you left off when you last worked on the project.\n\n\nLater in the class, we’ll introduce more powerful strategies like the renv and targets packages which can help to manage your workflows and their dependencies.\n\n\n\n\nProject Setup\nEach project you create should have it’s own directory which will be a folder with a descriptive file name. This root folder will contain a directory structure which allows you to organize your data and workflows. There’s no standardized format which this directory structure has to take, however there are some useful principles that will help you stay organized.\n\nProject Organization Principles\n\nUse subdirectories to stay organized. While R doesn’t care whether your project directory contains subdirectories, subdirectories are useful for keeping track of your files. As you build out increasingly complicated projects, you will likelly generate lots of data, metadata, scripts, and outputs. Subdirectories can help you to keep all of this information organized, and will make it easier for others to navigate your project.\nKeep raw and processed data separate. We want to set up workflows that allow us to consistently be able to transition from raw data to processed data, to outputs (that will end up in our outputs folder). If our project relied upon locally stored datasets, raw data should be placed in a separate folder for raw data. After it exists, raw data should only be read - it shouldn’t be written to. Depending upon the considerations of your workflow, you may want to create a separate folder for processed data - unlike your raw data folder, you may both read and write data to this folder.\nMaintain a local copy of data documentation. Whenever possible, download or include data documentation to go along with your raw data. This will be useful to refer to as you’re working, and will also help others who make work on the project in the future. As you modify data and create your own datasets, you should also consider creating documentation for your working data. Again, this will help you or others in the future.\nOutputs get their own subdirectory. Outputs may include tables, graphics, or any other output created by your analysis. Depending upon the quantity of outputs, you may choose to add additional subdirectories to your output folder to help organize your outputs.\nOrder your scripts. In addition to creating a dedicated subdirectory for your scripts, consider naming them in such a way that they are ordered and follow your data workflow. You may also want to develop a master script that sequentially runs your other scripts. You can use source() in this master script to run other scripts.\nQuarto Markdown gets its own subdirectory. Depending upon the complexity of your project, it may be useful to create a directory for any Quarto Markdown documents you create.\n\nYour base project directory and structure will end up looking something like this:\nRoot\n |\n |_ data\n |   |_ data_raw\n |   |_ data_processed\n |\n |_ documentation\n |_ output\n |_ scripts\n |   |_ 01_master.r\n |   |_ 02_get_data.r\n |   |_ 03_plots.r\n |_ quarto\n |    |_ 01_report.qmd\nAgain, there’s no standardized format which your directory structure has to take. Your structure may also change depending upon the specific considerations and workflows involved for a given project. Over time, you’ll develop a default folder structure that makes sense for how you tend to work.\n\nWhen we assess your work in UP 570, we will be looking for evidence that you are making effective use of project directory structures. Unless we state so, there’s no required directory structure format, however, we want to see you applying principles of good directory management in your work.\n\n\n\nProject Organization Workflows\nAfter we create our project directory and create an r project file, we can programmatically create our directory structure using the dir.create() command. For example:\n\n\nCode\n# Create project subdirectories\n\ndir.create(\"data\")\ndir.create(\"data/data_raw\")\ndir.create(\"data/data_processed\")\ndir.create(\"documentation\")\ndir.create(\"output\")\ndir.create(\"scripts\")\ndir.create(\"quarto\")\n\n\nWe could also save ourselves repetition in our code using the base R lapply() syntax. We create a list with our project directory names and assign that into an object. We then feel lapply() the list along with the function we wish to apply to each element of that list - in this case, dir.create():\n\n\nCode\n# Create project subdirectories\n\nproject_dirs &lt;- c(\"data\", \"data/data_raw\", \"data/data_processed\", \"documentation\", \"output\", \"scripts\", \"quarto\")\n\nlapply(project_dirs, dir.create)\n\n\n\n\nScript Headers and Structure\nWhile a lot of our coding can be done directly in Quarto Markdown documents, you’ll also want to use scripts to store code and develop workflows that may eventually be represented as output in your Quarto document. Script headers can help to organize the work we do in scripts, and can help when we get ready to share our work with others or pick up work on files we have not looked at in a while.\nTaking a few extra minutes to develop a nice script header will be beneficial in the future - your future self, your collaborators, and others who use your code will appreciate it!\nLike so many other R things, there’s no standardized example of what a script header needs to include, however, here’s a basic header template to start with:\n\n\nCode\n# Title: [Insert script title here]\n# Author: [Your name]\n# Email: [Your email address]\n# Date: [Insert date here]\n\n# Script Name: [Script Name]\n\n# Description: [Insert a brief description of the script and its purpose]\n\n# Notes: [Insert any process notes or potential revision plans here]\n\n# Set up the working environment\n\n# Load required libraries\nlibrary(library1)\nlibrary(library2)\n\n# Set global options\noptions(option1 = value1, option2 = value2)\n\n# Define functions\nfunction1 &lt;- function(input1, input2) {\n  # function code goes here\n}\n\nfunction2 &lt;- function(input1, input2) {\n  # function code goes here\n}\n\n# Main script\n\n# End of script\n\n\nThe different elements here help record who coded the script, what it is designed to do, and provides a common place to set options, load packages, and define functions. Although these components could occur anywhere in the script before they are called upon, includig them in the header helps track what’s going on in the script.\nWhile you could easily type out each element of your header every time that you develop a new script, you can also save your script as a code snippet within RStudio, which will allow you to easily insert the header template into new scripts you create.\n\n\nThis code snippet trick is paraphrased from the description provided by Dr. Timothy S. Farewell.\n\nIn RStudio, go to Tools -&gt; Global Options -&gt; Code -&gt; Snippets -&gt; Edit Snippets. This will bring up a block of code - scroll to the bottom.\nModify the below code header template to include your information and then copy / paste this into the snippets code block.\n\n\n\nCode\nsnippet header\n    # Title: \n    # Author:\n    # Email:\n    # Date:\n\n    # Script Name:\n\n    # Description:\n\n    # Notes:\n\n    # Working Environment Setup\n\n    # Load required libraries\n    load_packages &lt;- c(\"tidyverse\", \"sf\") # Add more packages here as needed\n    lapply(load_packages, library, character.only = TRUE)\n\n    # Set global options\n\n    # Define functions\n\n\n 3. Save and close the window.\nYou have now added this snippet to your library. When you start a new script, simply type “header” and press tab and RStudio will fill in your header template.\n\n\nSnippets are handy - you may soon identify other common bits of code that you use frequently that you want to make snippets out of.\n\n\nNaming Files\nAs mentioned in the Project Organization Principles section, it’s important to spend a little time thinking about conventions for your filenames. Some useful principles:\n\nAvoid using special characters (for instance, \\ / : * ? ” &lt; &gt; |) in file names.\nDo not use spaces in filenames. You may use an underscore (e.g. map_census_tracts.png) to help with readability.\nIf you choose to use dates as part of your filename, order them yyy-mm-dd (e.g. 20230102 would be January 2, 2023). If you have multiple versions of the file from different dates, this ensures that your files will be properly ordered by the date.\n\n\n\nThis date format (yyy-mm-dd) conforms to ISO 8601 format.\n\nWhen creating ordered lists for file names (ordered by number) use at least two digits (e.g. “01_Main”, “02_Download”, “03_Map”). This will keep your files in order once you get past 10."
  },
  {
    "objectID": "assignments/labs/01_datapipeline/01_datapipeline.html#workflow-2-working-in-quarto",
    "href": "assignments/labs/01_datapipeline/01_datapipeline.html#workflow-2-working-in-quarto",
    "title": "Building a Data Pipeline",
    "section": "Workflow 2: Working in Quarto",
    "text": "Workflow 2: Working in Quarto\nQuarto is a tool built into RStudio which allows you to create a variety of documents that can integrate plain text and code. It’s utility comes both in it’s ability to translate plain text into a variety of well-formatted outputs, but also due to the ease with which we can use this format to share code, analysis, and writing with others. Most of your written assignments in class will be produced in Quarto, therefore, you’ll want to become familiar with the basic syntax and logic.\nQuarto’s plain text formatting is handled using Markdown syntax which is designed to be simple, easily interpreted as text with formatting in place, and readable on most computers. An increasing number of note systems and word processors have adopted Markdown as their language.\n\n\nSome examples include Bear, Obsidian, Notion, Ulysses. Mentions are not endorsements.\n\nFormatting Your Documents\nQuarto makes use of a modified version of Markdown syntax. The Quarto website maintains excellent documentation regarding basic markdown formatting options as well as Quarto-specific formatting options:\n\nYou should familiarize yourself with how to format text, how to use section headings, and how to link files and insert references to other files.\n\n\nAdding References\nQuarto can help you either manage references in very sinple ways or more complex ways. You can either add references directly to your text as either footnotes or endnotes.\n\nFootnotes\nWe oftentimes use footnotes as an easy way to convey references or links to details or important information that does not need to be in our main body of text. Quarto allows you to create both footnotes and long notes. Footnotes are typically short and take up only a line or two. Long notes are longer and may contain multiple paragraphs.\n  \nHere is some text with a footnote.[^1]\n\n[^1]: And here is the footnote that accompanies that text in the footnotes section.\n\nHere is also some text with an inline footnote.^[Here is the text associated with the inline footnote - this may be easier to track because it is in line with the text.]\n\nHere is some text with a longnote.[^longnote]\n\n[^longnote]: And here is a longnote that takes up multiple lines. You can experiment in your text to see whether a footnote or longnote will serve you better for a given situation.\n  \n  This text is also part of the long note - the indent signals that it remains part of the note.\n  \nAnd this text would be part of the next paragraph that's outside of the note because it is not indented.\nHere is some text with a footnote1\nHere is also some text with an inline footnote.2\nHere is some text with a longnote3\nAnd this text would be part of the next paragraph that’s outside of the note because it is not indented.\n\n\nAsides\nYou can also add asides to documents which places a note in line with text in the margin.\n\n\nFor a very effective example of asides, see Kieran Healy’s book Data Visualization: A Practical Introduction.\n:::{.aside}\nHere is an aside placed in line with text in the margin.\n:::\n\n\nHere is an aside placed in line with text in the margin.\n\n\nCitations and Bibliographies\nFor longer or more advanced documents, you may wish to rely upon Quarto’s support for using and managing references with BibTex, a standard reference format. This allows you to add references to your document and then create in line citations and a formatted bibliography to accompany your writing.\nIn order to create a bibliography that is associated with a document, you’ll need three files:\n\nA main file written in Quarto Markdown (.qmd). This file will be formatted to include citation references.\nA bibliographic data source, typically a Bibtex (.bibtex) file.\nA csl file - this specifies the format for your bibliography. You can find and include different csl files to produce a bibliography using different reference styles.\n\n\nStep 1: Create a bibliography file.\nUsing a text editor, create a plain text file that will contain your bibliographic references. In this case, we’ll create a text file called “references.bib”. Note that this file should end in .bib to signify that it is a bibliographic reference file.\n\n\nStep 2: Refer to the bibliography file in your YAML Header\nNext, in your Quarto Markdown document, you’ll add a line to your YAML header that refers to the location and name of your bibliography file.\n\n\nCode\n---\ntitle: \"Neighborhood Analysis: Place Selection Memorandum\"\nbibliography: references.bib\n---\n\n\n\n\nStep 3: Define a citation style\nLike most other reference management systems, you can define what reference syle your references will be rendered as, and change this as needed. The Citation Style Language (CSL) project defines a common language and structure for styles. These are stored in .csl files which you will associate with your document. You can find and download .csl files in a range of formats from the CSL Project’s repository.\n\n\nYou can also find and download csl files in Zotero’s style repository. If you hover over a style format, it provides examples of what a citation will look like in your rendered document.\nFor the sake of example, we’ll format our document using American Psychological Association (APA) format. We’ll search the CSL Project’s repository to find an appropriate APA style .csl file. Once you find the appropriate file, then download the file and place it in the same directory as our .bib file.\n Click on the .csl style format you want to use. You’ll then be taken to a page that includes the specific file’s contents.\n\nFrom here, you can right click on the button that says “raw” and download the .csl file.\n Note that before you move this file from your download folder to the folder where your .bib file is located, you may need to alter the file extension to indicate that this is a .csl file. Once you have done this, move it to the folder where your .bib file is located and then add a csl reference to your YAML header.\n\n\nCode\n---\ntitle: \"Neighborhood Analysis: Place Selection Memorandum\"\nbibliography: references.bib\ncsl: apa-6th-edition.csl\n---\n\n\n\n\nStep 4: Add references to your document\nYou’re now set up to add references to your document. You can add references in lots of different ways. If you’re searching for references using a service like Google Scholar, under the “cite” options for a given reference, you’ll see a URL to download the BibTeX citation associated with that reference.\n Click on BibTeX and the citation contents will show:\n\nCopy this text, and paste it into your bibliographic reference file (in our example, references.bib).\n\nYou can then cite this reference while you write by referring to it as follows:\n\n\nCode\n[@healy2018data] provides useful examples for leveraging GGPlot to create data visualization.\n\n\n(Healy, 2018) provides useful examples for leveraging GGPlot to create data visualization.\nNote that the actual reference corresponds to the first clause in the bibtex reference.\n\n\nYou can change the reference name if you’d like to something you’ll remember as you cite.\nYou can paste more references into the same references.bib file and cite them as well.\n\n\nYou can find more details on formatting citations on the Quarto website.\nOnline tools like Citation Machine can help you to build references for a range of online sources. (Feder, n.d.) takes a little while to get used to, but these tools can help you to build out an effective citation workflow.\n\n\nStep 5: Place your bibliography\nQuarto will automatically place your references at the end of your document (or wherever your style definition file calls for references to be placed). You can also explicitly tell Quarto where to place your references by creating a div as follows:\n\n\nCode\n\n## References\n\n::: {#refs}\n:::\n\n\nYou should end up with a formatted reference section at the end of your document.\n\n\n\n\n\nReference Considerations\nIt’s ultimately up to you how you manage and generate references within your documents. If you have a document with very few references, footnotes and/or endnotes may be an appropriate strategy. A more complex document with many references may be better managed using BibTeX. You can also use another reference management system and then export citations in BibTeX format which can then be added to your Quarto Markdown document."
  },
  {
    "objectID": "assignments/labs/01_datapipeline/01_datapipeline.html#lab-evaluation",
    "href": "assignments/labs/01_datapipeline/01_datapipeline.html#lab-evaluation",
    "title": "Building a Data Pipeline",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nImplementing best practices in root directory structure, file naming, and organization.\nFormatting of a script file including the implementation of a header.\nFormatting of the Quarto Markdown document to refer to analytic output and which uses formatting features to structure the document.\nA brief but appropriate description of population change in Chicago Community Areas.\nImplementation of a bibliography using the MLA format in your Quarto Markdown file.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/01_datapipeline/01_datapipeline.html#footnotes",
    "href": "assignments/labs/01_datapipeline/01_datapipeline.html#footnotes",
    "title": "Building a Data Pipeline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd here is the footnote that accompanies that text in the footnotes section.↩︎\nHere is the text associated with the inline footnote - this may be easier to track because it is in line with the text.↩︎\nAnd here is a longnote that takes up multiple lines. You can experiment in your text to see whether a footnote or longnote will serve you better for a given situation.\nThis text is also part of the long note - the indent signals that it remains part of the note. You might include code snippets, images, or other items as part of your note.\nCreating footnotes is easy^[So long as you remember the correct syntax.]\nThis text would also be part of your long note.↩︎"
  },
  {
    "objectID": "assignments/labs/03_communicating/03_communicating.html",
    "href": "assignments/labs/03_communicating/03_communicating.html",
    "title": "Communicating Complex Information",
    "section": "",
    "text": "UP 570 assumes that you have a basic familiarity with core principles of data manipulation in R. As we move forward with our class, we will continue to add knowledge of new packages, tools, and data within R.\nThis communicating complex information lab is essentially your R Learner’s Permit for the class. It is designed to assess your prior knowledge of the core elements of the R language and software. The goal is to provide you and me with a better sense of your core knowledge of basic data manipulation that will form the basis for more advanced techniques we’ll learn over the course of the semester.\nDo not worry if some of the tasks remain challenging or if you are unable to complete them- a goal of the class is to continue adding complexity and opportunities to repeat tasks to reinforce your familiarity and comfort with their use."
  },
  {
    "objectID": "assignments/labs/03_communicating/03_communicating.html#introduction",
    "href": "assignments/labs/03_communicating/03_communicating.html#introduction",
    "title": "Communicating Complex Information",
    "section": "",
    "text": "UP 570 assumes that you have a basic familiarity with core principles of data manipulation in R. As we move forward with our class, we will continue to add knowledge of new packages, tools, and data within R.\nThis communicating complex information lab is essentially your R Learner’s Permit for the class. It is designed to assess your prior knowledge of the core elements of the R language and software. The goal is to provide you and me with a better sense of your core knowledge of basic data manipulation that will form the basis for more advanced techniques we’ll learn over the course of the semester.\nDo not worry if some of the tasks remain challenging or if you are unable to complete them- a goal of the class is to continue adding complexity and opportunities to repeat tasks to reinforce your familiarity and comfort with their use."
  },
  {
    "objectID": "assignments/labs/03_communicating/03_communicating.html#goals",
    "href": "assignments/labs/03_communicating/03_communicating.html#goals",
    "title": "Communicating Complex Information",
    "section": "Goals",
    "text": "Goals\n\nGet your hands dirty with what is likely an unfamiliar source of “real world” data.\nLearn more about your familiarity with basic dplyr data manipulation strategies.\nLearn more about your familiarity with basic data visualization using ggplot2.\nCommunicate this information using workflows from class."
  },
  {
    "objectID": "assignments/labs/03_communicating/03_communicating.html#core-concepts",
    "href": "assignments/labs/03_communicating/03_communicating.html#core-concepts",
    "title": "Communicating Complex Information",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\ndim()\nsummary()\ngroup_by()\nsummarise()\nleft_join\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/03_communicating/03_communicating.html#github-lab-repository",
    "href": "assignments/labs/03_communicating/03_communicating.html#github-lab-repository",
    "title": "Communicating Complex Information",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/03_communicating/03_communicating.html#instructions",
    "href": "assignments/labs/03_communicating/03_communicating.html#instructions",
    "title": "Communicating Complex Information",
    "section": "Instructions",
    "text": "Instructions\nFollow the instructions contained within the GitHub lab repository. Most instructions ask you to add or fill in code chunks. Others ask you to provide a written interpretation in the notebook portion of the document.\nComplete as many items as you can. If you run into trouble completing an item, add comments to your code or in the notebook describing where you are running into problems, and what you think the next step might be to solving the problem you’re having.\nComplete as many items as you can and then push your work to the appropriate repository on Github."
  },
  {
    "objectID": "assignments/labs/03_communicating/03_communicating.html#lab-evaluation",
    "href": "assignments/labs/03_communicating/03_communicating.html#lab-evaluation",
    "title": "Communicating Complex Information",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nYour code and the way in which you’re approaching problem solving.\nYour written analysis of how you are approaching problem solving in the lab.\nYour written analysis and interpretation of the lab materials.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class. It is okay to touch base with others as you work through the lab, however, please indicate where you are running into challenges with problem solving so we can factor this into our instruction."
  },
  {
    "objectID": "assignments/labs/03_communicating/03_communicating.html#references",
    "href": "assignments/labs/03_communicating/03_communicating.html#references",
    "title": "Communicating Complex Information",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/05_population_census/05_census.html",
    "href": "assignments/labs/05_population_census/05_census.html",
    "title": "Population and the Census",
    "section": "",
    "text": "This lab is designed to introduce you to some of the basic techniques for downloading data from the U.S. Census Bureau using R. Following elements from the Klosterman, et al. reading, we will replicate some of the tables present in that chapter.\nWe will also learn a new R package this week - the tidycensus package, which is designed to programmatically download and load census data directly into our R session."
  },
  {
    "objectID": "assignments/labs/05_population_census/05_census.html#introduction",
    "href": "assignments/labs/05_population_census/05_census.html#introduction",
    "title": "Population and the Census",
    "section": "",
    "text": "This lab is designed to introduce you to some of the basic techniques for downloading data from the U.S. Census Bureau using R. Following elements from the Klosterman, et al. reading, we will replicate some of the tables present in that chapter.\nWe will also learn a new R package this week - the tidycensus package, which is designed to programmatically download and load census data directly into our R session."
  },
  {
    "objectID": "assignments/labs/05_population_census/05_census.html#goals",
    "href": "assignments/labs/05_population_census/05_census.html#goals",
    "title": "Population and the Census",
    "section": "Goals",
    "text": "Goals\nPlanners frequently rely upon census data to help establish baseline portraits of places. Given the frequency with which planners tend to use Census data, can R help us to more efficiently access this information, and integrate it into analysis processes?\nIn this section, we will examine how Census data is typically accessed, and explore how to do so with R. By the end of this section, you will:\n\nReview how to manually access census data from data.census.gov.\nLearn how to construct basic calls for census data using R and the tidycensus package\nLearn the basic principles of joining data, and apply these"
  },
  {
    "objectID": "assignments/labs/05_population_census/05_census.html#core-concepts",
    "href": "assignments/labs/05_population_census/05_census.html#core-concepts",
    "title": "Population and the Census",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\nbind_rows()\nbind_cols()\nget_acs()\n%in% c()\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/05_population_census/05_census.html#github-lab-repository",
    "href": "assignments/labs/05_population_census/05_census.html#github-lab-repository",
    "title": "Population and the Census",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/05_population_census/05_census.html#accessing-census-data-via-american-fact-finder",
    "href": "assignments/labs/05_population_census/05_census.html#accessing-census-data-via-american-fact-finder",
    "title": "Population and the Census",
    "section": "Accessing Census Data via American Fact Finder",
    "text": "Accessing Census Data via American Fact Finder\nLet’s start working with Census data! Our goal here is to begin creating tables similar to the ones we see in Klosterman Chapter 2. For the purposes of this exercise, we’ll describe how Champaign, Illinois compares to other cities within Champaign County. We’ll then think about how we might be able to describe smaller geographies from there.\nBefore we get around to downloading census data using R, we’ll start by learning about some of the common ways in which census data are accessed, and will then spend a little time learning how to download data at different summary levels.\nMost casual users of Census data typically start by visiting the data.census.gov website:\n\nFor those users looking to download detailed demographic tables covering more than one geography, “Use Advanced Search” is best option (and the option that we will choose).\nThe Advanced Search window includes a series of query selectors, or if you know the exact name or table number you are looking for, you can use the search dialog. Even if you know the table number, it makes sense to start off by selecting your survey and the geography at which you wish FactFinder to return results.\nThe Census bureau tabulates demographic information for a number of different geographic types, some which are hierarchical, and some which aren’t:\n The Klosterman chapter presents data for a city, and then compares it to its constituent county and region. This diagram tells us that cities (census places) are nested within (and contained by) states, which are nested within census divisions, and census regions which are all contained within the nation. Census places do not have any subgeographies, however, we may also use other sub county geography, especially census tracts and block groups to help us describe the characteristics of census places.\nThe most common geographies used by planners are as follows:\n\nState\nCounty\nPlace (incorporated cities)\nTract (approximate neighborhoods based upon physical boundaries)\n\nSome specialized geographies like Traffic Analysis Zones (TAZs) are also frequently used, as are Zip Code Tabulation Areas (ZCTAs) which approximate postal service Zone Improvement Plan (ZIP) codes.\nIn our FactFinder search, let’s start off by looking for data on the racial composition of census places in Illinois from the 2019 5-Year American Community Survey. This corresponds to data used to create the second column in Table 2.1 of Klosterman. We can add some filters to identify the census data table containing the appropriate data.\nLet’s start by filtering to the 5-Year American Community Survey data (Browse Filters -&gt; Surveys -&gt; American Community Survey)\n\nWe now have 2 main choices - 1-Year estimates and 5-year estimates. What might the difference be between these two data products?\n\n1-Year Estimates are based off of the survey sample collected within one year and reflect the characteristics of the population within that given year.\n5-Year Estimates are based off of a pooled survey sample collected over a 5-year period and reflect the characteristics of the population over those 5 years.\n\nThe trade offs between the two samples are time specificity and certainty around the accuracy of the estimates. Particularly when we are looking at smaller geographies, this certainty question can become more of an issue. More on this later in our documentation.\nFor now, let’s select 5 year estimates, and then select “Detailed Tables”.\n\nAfter adding this filter, add an additional filter under Years for 2021.\n\nWe might also want to add a geographic filter to indicate that the data we want is for census places.\n And then select all places in Illinois…\n\nWith these filters in place we can now search for the race table.\n\nWe’ll then get a list of tables matching our criteria. There are a lot of them! You will need to click “View All Tables” to see the entire list. Let’s take a closer look at Table B02001: Race. We can click on it to view the table’s contents.\n This is useful - we can see the structure of the table. Rows contain information associated with racial identification groups, columns correspond to census places (cities and local governments). Observations include counts of the population that identify with each racial group.\nWe could download this file and open it in a spreadsheet program. \nWe could download this file and open it in a spreadsheet program or R. If you download the .csv file, we’d need to do some reformatting to start using the data in any serious way.\nAs we start manipulating the data with more clicks (say to remove the double header row, to delete the GEO.id column, or to work with estimates) we introduce the potential for user error, especially if we want to exactly reproduce what we’ve done over and over again. Of course the potential for error grows once our data takes on more dimensions or involves multiple tables or sources.\nAs an alternative strategy, let’s look at how we’d do the same thing programmatically in R."
  },
  {
    "objectID": "assignments/labs/05_population_census/05_census.html#workflow-downloading-census-data-programmatically-in-r",
    "href": "assignments/labs/05_population_census/05_census.html#workflow-downloading-census-data-programmatically-in-r",
    "title": "Population and the Census",
    "section": "Workflow: Downloading Census Data Programmatically in R",
    "text": "Workflow: Downloading Census Data Programmatically in R\nThe tidycensus package made by Kyle Walker downloads data directly from the U.S. Census Bureau’s application programming interface (API). While we reviewed using data.census.gov to manually select and download data from the Census, the benefit of using tidycensus and R is that the data pipeline is reproducible.\nIn order to use tidycensus to download data, you’ll need the following:\n\nA Census API key (if you don’t already have one, please request one from the census bureau)\nA desired geography to look up (for example, all census places in Illinois)\nThe census year you want data for (we’ll start with 2021)\nThe survey you’d like to use (Decennial Census, ACS 1 or 5 year data)\nThe number of the table we want to look up (in this case table B02001 which contains information on the total population by race from the American Community Survey)\n\nThere are two main functions in tidycensus:\n\nget_decennial() is designed to download data from the decennial census which is conducted every ten years by law (e.g. 1990, 2000, 2010, and 2020) for the purposes of allocating political representation in the US.\nget_acs() is designed to download data from the American Community Survey, which is conducted continuously, and which focuses on describing the characteristics of the population for the years in between the decennial census.\n\n\nInitial Set Up\nLet’s start by loading a few packages that will help us with our work.\n\n\nCode\nlibrary(gt)\nlibrary(tidyverse)\nlibrary(tidycensus)\n\n\nEach time you run a tidycensus function, R constructs a request to the Census API for data. This call needs to include your API key to authenticate your access to the data.The first time you use tidycensus on a new computer or R installation, you’ll need to set up your R environment to load your Census API key. You have two options -\n\nManually set the API key for use in your R script each time you use tidycensus:\n\n\n\nCode\nlibrary(tidycensus)\ncensus_api_key(\"Actual Key String Here\")\n\n\n\nStore your API key locally on your computer so that it does not need to be loaded manually.\n\n\n\nCode\nlibrary(tidycensus)\ncensus_api_key(\"Actual Key String Here\", install = TRUE)\n\n\nAgain, this looks largely the same. The first option simply sets the API key for use in a given rstudio session. Adding install=TRUE “installs” the API key for use any time you load RStudio, meaning you can load and use the package without having to use census_api_key(). I recommend installing your API key for two reasons - first, it is convenient to not have to copy and paste your API key into a new script or notebook; and second, if you share your code publicly, others will not have access to your api key because it is not included in your code (you might include a note in your documentation that lets other users know they will need to supply their own API key).\nWith that set up, we can move on…\n\n\nOur First Table Download\nTo familiarize yourself with tidycensus, have a look at the following code chunk. This gets the total population by race (Table B02001) for each census place in Illinois, and places that data into place_race_2021:\n\n\nCode\nplace_race_2021&lt;-get_acs(geography = \"place\", state = \"Illinois\", table = \"B02001\", year=2021, survey=\"acs5\", output=\"wide\")\n\n\nOur call says create an object names “place_race_2021” and place into it the results of a function called get_acs (which is the function to download ACS data from the census API). Within the get_acs() function, we specify several attributes:\n\ngeography = “place” allows us to specify the geography which we want returned - in this case estimates for census places.\nstate = “Illinois” allows us to specify that we want to download data for the state of Illinois.\ntable = “B02001” allows us to specify which table we want downloaded - in this case, we want to look in table B02001. As an alternate to using table = we can download specific variables (if we don’t need all of the table’s data using variables =.)\nyear = 2021 allows us to specify which vintage of ACS data we are downloading.\nsurvey=“acs5” specifies that we want to look at 5-year ACS data with year being the most recent endpoint (e.g. if we specify year = 2021 and survey = \"acs5\", we are specifying 2017-2021 5-year ACS data).\noutput=“wide” specifies that we want our data to be formatted with columns corresponding to different racial identification categories.\n\nLet’s have a look at what was just downloaded:\n\n\nCode\nplace_race_2021 |&gt; \n  slice_head(n = 10) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n      B02001_004E\n      B02001_004M\n      B02001_005E\n      B02001_005M\n      B02001_006E\n      B02001_006M\n      B02001_007E\n      B02001_007M\n      B02001_008E\n      B02001_008M\n      B02001_009E\n      B02001_009M\n      B02001_010E\n      B02001_010M\n    \n  \n  \n    1700113\nAbingdon city, Illinois\n3586\n385\n3474\n400\n63\n72\n0\n12\n0\n12\n0\n12\n21\n33\n28\n42\n0\n12\n28\n42\n    1700178\nAdair CDP, Illinois\n210\n126\n192\n106\n18\n29\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n    1700191\nAdams CDP, Illinois\n47\n55\n47\n55\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n    1700230\nAddieville village, Illinois\n359\n81\n348\n83\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n11\n11\n3\n6\n8\n9\n    1700243\nAddison village, Illinois\n35999\n43\n22423\n1211\n1702\n459\n223\n181\n3022\n668\n0\n25\n5547\n884\n3082\n891\n2230\n734\n852\n458\n    1700295\nAdeline village, Illinois\n95\n43\n94\n43\n1\n2\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n    1700516\nAlbany village, Illinois\n771\n165\n739\n157\n10\n18\n0\n12\n0\n12\n0\n12\n3\n7\n19\n20\n4\n6\n15\n20\n    1700555\nAlbers village, Illinois\n1431\n200\n1387\n205\n0\n12\n0\n12\n6\n7\n0\n12\n0\n12\n38\n55\n2\n6\n36\n54\n    1700568\nAlbion city, Illinois\n2122\n180\n2033\n179\n0\n12\n33\n32\n15\n11\n0\n12\n8\n12\n33\n29\n0\n12\n33\n29\n    1700594\nAlden CDP, Illinois\n188\n280\n188\n280\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n0\n12\n  \n  \n  \n\n\n\n\nExamining the place_race_2019 object, we have a table that includes 22 columns:\n\nGEOID a unique numeric code for each census place\nNAME the name of each census place\nEstimates estimates (in this case population estimates by race), with each column representing a row in the data table we pulled up on data.census.gov.\nMargin of Error the margin of error associated with each estimate.\n\nNote in the above code that we use slice_head() to display only the first 10 rows of a table that actually has 1,466 rows.\nThat wasn’t so hard! There are several benefits of downloading census data this way: 1. Once we know what tables or variables we are looking for, we don’t have to interact with data.census.gov very much (or at all). 2. R downloads the data and then it is immediately available for analysis as a formatted data frame. 3. We can easily modify existing calls to download data for other geographies.\nLet’s say we want to transition to looking at racial identification for counties in Illinois. How would we modify our previous code to match those specifications? - geography needs to be changed from “place” to “county” (find more information on specifying geography in the tidycensus documentation). - We need to add a new specification, state = \"IL\" (you’ll note that in the documentation on specifying a geography a column called “available by” which tells you what must be specified for a given summary level). - We need to change the object name that we’re putting our downloaded data into. Let’s call in “county_race_2021.\n\n\nCode\ncounty_race_2021&lt;-get_acs(geography = \"county\", state = \"IL\", table = \"B02001\", year=2021, survey=\"acs5\", output=\"wide\")\n\ncounty_race_2021 |&gt; \n  slice_head(n = 10) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n      B02001_004E\n      B02001_004M\n      B02001_005E\n      B02001_005M\n      B02001_006E\n      B02001_006M\n      B02001_007E\n      B02001_007M\n      B02001_008E\n      B02001_008M\n      B02001_009E\n      B02001_009M\n      B02001_010E\n      B02001_010M\n    \n  \n  \n    17001\nAdams County, Illinois\n65878\nNA\n60689\n144\n2615\n343\n51\n33\n420\n113\n8\n11\n153\n61\n1942\n382\n513\n191\n1429\n283\n    17003\nAlexander County, Illinois\n5488\nNA\n3502\n39\n1836\n70\n4\n9\n1\n3\n22\n32\n0\n17\n123\n58\n15\n18\n108\n53\n    17005\nBond County, Illinois\n16804\nNA\n15120\n135\n1184\n141\n112\n44\n165\n155\n0\n19\n36\n25\n187\n114\n88\n71\n99\n86\n    17007\nBoone County, Illinois\n53592\nNA\n44895\n968\n1437\n293\n160\n137\n523\n87\n0\n28\n4200\n1041\n2377\n515\n1243\n394\n1134\n321\n    17009\nBrown County, Illinois\n6330\nNA\n4876\n202\n1174\n170\n39\n33\n3\n5\n0\n17\n135\n70\n103\n58\n59\n46\n44\n37\n    17011\nBureau County, Illinois\n33338\nNA\n30452\n237\n189\n66\n66\n29\n298\n51\n0\n25\n1307\n256\n1026\n172\n564\n146\n462\n77\n    17013\nCalhoun County, Illinois\n4537\nNA\n4357\n45\n29\n35\n14\n18\n2\n4\n0\n12\n0\n12\n135\n52\n61\n49\n74\n29\n    17015\nCarroll County, Illinois\n15586\nNA\n14248\n276\n468\n128\n44\n22\n119\n73\n0\n19\n79\n50\n628\n240\n223\n117\n405\n212\n    17017\nCass County, Illinois\n13058\nNA\n11075\n281\n451\n150\n108\n112\n89\n25\n91\n135\n545\n251\n699\n253\n523\n240\n176\n75\n    17019\nChampaign County, Illinois\n206583\nNA\n144522\n757\n26996\n1074\n170\n82\n22204\n477\n115\n98\n2280\n513\n10296\n1300\n3005\n571\n7291\n1166\n  \n  \n  \n\n\n\n\nBy making those few changes, you’ll note that the newly downloaded data in county_race_2021 now includes names of counties in the NAME column, instead of places as in our first call. There are data for each of the 102 counties in Illinois.\nPretty cool, huh?\n\n\nBuilding Table 2.1: Population Breakdown by Race\nLet’s start by downloading data on population by racial identification for counties in Illinois from the 2021 5-year American Community Survey, and placing the data into a data frame called “place_race_2021”. To do so, we need to know some basic information to construct our data request to the census.\nSince we are downloading ACS data, we will use the get_acs() function. At a minimum, get_acs() needs to know the geography we want it to download, the name(s) of the variables we’d like to retrieve, and the year we want the data for. Check out the basic usage of tidycensus for more information on constructing calls for different geographies.\ntidycensus can make calls for most geographies programmatically.\nSo far, we have used table = to specify that we wanted all variables downloaded for each table. In some cases, we may not need all of that information. Instead, we can use the variables = specification to download specific columns from the larger census table.\nTable B02001 which we’ve been looking at is hierarchical. The first row contains the total population of all races. The second row contains the estimate for those individuals who identify as white alone. The eighth row contains the estimate for individuals of two or more races.\nTable 2.1 in the Klosterman book presents racial identification for White Alone, Black Alone, Other, and Hispanic ethnicity. Putting aside for a minute the Hispanic ethnicity statistic, how would we download and construct analogous data for Champaign, Illinois? Let’s think this through:\n\n\n\n\n\n\n\n\n\nIndicator\nRow Number\nTidycensus Variable\nConstructed Variable\n\n\n\n\nWhite Alone\n2\nB02001_002\nNA\n\n\nBlack Alone\n3\nB02001_003\nNA\n\n\nOther\n?\nNA\nB02001_001-(B02001_002+B02001_003)\n\n\n\nTo construct this table, we only need the variables corresponding to the first, second, and third rows.\n\nWhite alone and black alone are the second and third rows in the data.census.gov table.\nWe can construct our “Other” variable by subtracting from the total population, the number of white alone and black alone individuals.\n\nLet’s see this in practice:\n\n\nCode\nplace_race_2021&lt;-get_acs(geography = \"place\", state = \"IL\", variables = c(\"B02001_001\", \"B02001_002\", \"B02001_003\"), year=2021, survey=\"acs5\", output=\"wide\")\n\nplace_race_2021 |&gt; \n  slice_head(n = 10) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n    \n  \n  \n    1700113\nAbingdon city, Illinois\n3586\n385\n3474\n400\n63\n72\n    1700178\nAdair CDP, Illinois\n210\n126\n192\n106\n18\n29\n    1700191\nAdams CDP, Illinois\n47\n55\n47\n55\n0\n12\n    1700230\nAddieville village, Illinois\n359\n81\n348\n83\n0\n12\n    1700243\nAddison village, Illinois\n35999\n43\n22423\n1211\n1702\n459\n    1700295\nAdeline village, Illinois\n95\n43\n94\n43\n1\n2\n    1700516\nAlbany village, Illinois\n771\n165\n739\n157\n10\n18\n    1700555\nAlbers village, Illinois\n1431\n200\n1387\n205\n0\n12\n    1700568\nAlbion city, Illinois\n2122\n180\n2033\n179\n0\n12\n    1700594\nAlden CDP, Illinois\n188\n280\n188\n280\n0\n12\n  \n  \n  \n\n\n\n\nNote that our original place_race_2021 data frame had 22 variables. The one we downloaded with our updated call only has 8 variables. What’s going on in variables = c(\"B02001_001\", \"B02001_002\", \"B02001_003\")? The table is B02001, but then we also specify the specific row in the census data table, which actually gets represented as a column when downloaded here.\n\nB02001_001 contains the total population estimate\nB02001_002 contains the white alone population\nB02001_003 contains the total black alone population\n\nWe could use the tidyverse command mutate() to create our other category and place into a new column in our dataset:\n\n\nCode\nplace_race_2021 |&gt; \n  mutate(p_other = B02001_001E-(B02001_002E + B02001_003E)) |&gt; \n  slice_head(n = 10) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n      p_other\n    \n  \n  \n    1700113\nAbingdon city, Illinois\n3586\n385\n3474\n400\n63\n72\n49\n    1700178\nAdair CDP, Illinois\n210\n126\n192\n106\n18\n29\n0\n    1700191\nAdams CDP, Illinois\n47\n55\n47\n55\n0\n12\n0\n    1700230\nAddieville village, Illinois\n359\n81\n348\n83\n0\n12\n11\n    1700243\nAddison village, Illinois\n35999\n43\n22423\n1211\n1702\n459\n11874\n    1700295\nAdeline village, Illinois\n95\n43\n94\n43\n1\n2\n0\n    1700516\nAlbany village, Illinois\n771\n165\n739\n157\n10\n18\n22\n    1700555\nAlbers village, Illinois\n1431\n200\n1387\n205\n0\n12\n44\n    1700568\nAlbion city, Illinois\n2122\n180\n2033\n179\n0\n12\n89\n    1700594\nAlden CDP, Illinois\n188\n280\n188\n280\n0\n12\n0\n  \n  \n  \n\n\n\n\nWe might also want to rename our other two variables so that they are more readable, and might want to get rid of some of the other data we don’t need:\n\n\nCode\nplace_race_2021 |&gt; \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E)) |&gt;  \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E) |&gt; \n  select(Name, pop_tot, pop_white, pop_black, pop_other) |&gt; \n  slice_head(n = 10) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_other\n    \n  \n  \n    Abingdon city, Illinois\n3586\n3474\n63\n49\n    Adair CDP, Illinois\n210\n192\n18\n0\n    Adams CDP, Illinois\n47\n47\n0\n0\n    Addieville village, Illinois\n359\n348\n0\n11\n    Addison village, Illinois\n35999\n22423\n1702\n11874\n    Adeline village, Illinois\n95\n94\n1\n0\n    Albany village, Illinois\n771\n739\n10\n22\n    Albers village, Illinois\n1431\n1387\n0\n44\n    Albion city, Illinois\n2122\n2033\n0\n89\n    Alden CDP, Illinois\n188\n188\n0\n0\n  \n  \n  \n\n\n\n\nThe above code does the following:\n\nUses mutate() to create a new variable called pop_other that subtracts from the total population the white alone population and black alone population.\nPipes this through to a function which you haven’t seen before called rename() which allows us to rename columns by name with the specification rename(new name = old name)\nWe then pipe this through and use select() to select only the name of the place, and our four named population columns - pop_total, pop_white, pop_black, pop_other.\n\nLet’s filter out just the data for Champaign, Illinois:\n\n\nCode\nplace_race_2021 |&gt;  \n  filter(NAME == \"Champaign city, Illinois\") |&gt;  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E)) |&gt;  \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E) |&gt;  \n  select(Name, pop_tot, pop_white, pop_black, pop_other) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n18487\n  \n  \n  \n\n\n\n\nNote that if you look at the Name column that the name for each census place includes some additional information regarding what type of place it is (e.g. city, village, CDP). How would we filter for a place like Champaign if we didn’t know it was incorporated as a city?\nSome possible strategies:\n\nWe could search through all 1,369 records to find the specific record for Champaign\nWe could search manually but first sort alphabetically by NAME which would make it a little easier to find the value for Champaign. The arrange() command allows us to sort:\n\n\n\nCode\nplace_race_2021 |&gt;\n  arrange(NAME) |&gt; \n  slice_head(n = 20) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n    \n  \n  \n    1700113\nAbingdon city, Illinois\n3586\n385\n3474\n400\n63\n72\n    1700178\nAdair CDP, Illinois\n210\n126\n192\n106\n18\n29\n    1700191\nAdams CDP, Illinois\n47\n55\n47\n55\n0\n12\n    1700230\nAddieville village, Illinois\n359\n81\n348\n83\n0\n12\n    1700243\nAddison village, Illinois\n35999\n43\n22423\n1211\n1702\n459\n    1700295\nAdeline village, Illinois\n95\n43\n94\n43\n1\n2\n    1700516\nAlbany village, Illinois\n771\n165\n739\n157\n10\n18\n    1700555\nAlbers village, Illinois\n1431\n200\n1387\n205\n0\n12\n    1700568\nAlbion city, Illinois\n2122\n180\n2033\n179\n0\n12\n    1700594\nAlden CDP, Illinois\n188\n280\n188\n280\n0\n12\n    1700646\nAledo city, Illinois\n3860\n84\n3685\n105\n69\n36\n    1700659\nAlexander CDP, Illinois\n263\n134\n257\n134\n0\n12\n    1700672\nAlexis village, Illinois\n735\n166\n693\n160\n0\n12\n    1700685\nAlgonquin village, Illinois\n29869\n55\n24997\n659\n789\n362\n    1700737\nAlhambra village, Illinois\n627\n154\n609\n151\n0\n12\n    1700815\nAllendale village, Illinois\n709\n156\n633\n143\n0\n12\n    1700867\nAllenville village, Illinois\n111\n59\n111\n59\n0\n12\n    1700880\nAllerton village, Illinois\n226\n56\n197\n50\n19\n23\n    1700919\nAlma village, Illinois\n304\n77\n299\n76\n0\n12\n    1700958\nAlorton village, Illinois\n1864\n388\n33\n44\n1806\n388\n  \n  \n  \n\n\n\n\nIf we wanted to sort this list in the opposite direction, we can use the desc() command to do so:\n\n\nCode\nplace_race_2021 |&gt;\n  arrange(desc(NAME)) |&gt; \n  slice_head(n = 20) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n    \n  \n  \n    1784220\nZion city, Illinois\n24660\n74\n12130\n1175\n6447\n768\n    1784155\nZeigler city, Illinois\n1563\n200\n1512\n194\n7\n12\n    1784038\nYorkville city, Illinois\n20503\n638\n16005\n1706\n2367\n1259\n    1783817\nYates City village, Illinois\n711\n154\n683\n154\n5\n7\n    1783765\nYale village, Illinois\n84\n41\n76\n39\n0\n12\n    1783739\nXenia village, Illinois\n337\n97\n317\n95\n0\n12\n    1783687\nWyoming city, Illinois\n1428\n192\n1360\n187\n18\n18\n    1783622\nWyanet village, Illinois\n1015\n172\n969\n173\n2\n5\n    1783518\nWorth village, Illinois\n10909\n38\n8807\n702\n275\n139\n    1783505\nWorden village, Illinois\n800\n173\n762\n169\n0\n12\n    1783349\nWoodstock city, Illinois\n25829\n345\n21197\n1104\n558\n258\n    1783336\nWoodson village, Illinois\n570\n157\n554\n154\n0\n12\n    1783245\nWoodridge village, Illinois\n34161\n131\n22766\n976\n4097\n980\n    1783206\nWoodlawn village, Illinois\n647\n126\n597\n119\n7\n9\n    1783102\nWoodland village, Illinois\n286\n115\n263\n100\n5\n14\n    1783063\nWoodhull village, Illinois\n782\n147\n768\n145\n5\n8\n    1783271\nWood River city, Illinois\n10325\n28\n9324\n401\n499\n306\n    1782985\nWood Dale city, Illinois\n14034\n24\n10351\n912\n326\n247\n    1782855\nWonder Lake village, Illinois\n4032\n403\n3381\n651\n0\n12\n    1782725\nWitt city, Illinois\n638\n164\n629\n155\n4\n24\n  \n  \n  \n\n\n\n\n\nWe could call upon a flexible tool for identifying matches within a character vector from the stringr package called str_detect():\n\n\n\nCode\nplace_race_2021 |&gt;\n  filter(str_detect(NAME, \"Champaign\")) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n    \n  \n  \n    1712385\nChampaign city, Illinois\n88343\n45\n54115\n1105\n15741\n1104\n  \n  \n  \n\n\n\n\nThis says, look in the NAME column, and find those values where part of the row matches “Champaign”.\nLet’s look again at our cleaned up observations for Champaign:\n\n\nCode\nplace_race_2021 |&gt;  \n  filter(NAME == \"Champaign city, Illinois\") |&gt;  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E)) |&gt; \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E) |&gt; \n  select(Name, pop_tot, pop_white, pop_black, pop_other) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n18487\n  \n  \n  \n\n\n\n\nThere’s a fairly sizable population that falls into the “pop_other” category. We could not worry about this, but it probably makes sense to see if there’s another relatively large population group that we should itemize for in Champaign. Let’s go back and download the whole table (and overwrite our existing one) to be able to take a closer look:\n\n\nCode\nplace_race_2021&lt;-get_acs(geography = \"place\", state = \"IL\", table = \"B02001\", year=2021, survey=\"acs5\", output=\"wide\")\n\nplace_race_2021 |&gt; \n  filter(NAME == \"Champaign city, Illinois\") |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      NAME\n      B02001_001E\n      B02001_001M\n      B02001_002E\n      B02001_002M\n      B02001_003E\n      B02001_003M\n      B02001_004E\n      B02001_004M\n      B02001_005E\n      B02001_005M\n      B02001_006E\n      B02001_006M\n      B02001_007E\n      B02001_007M\n      B02001_008E\n      B02001_008M\n      B02001_009E\n      B02001_009M\n      B02001_010E\n      B02001_010M\n    \n  \n  \n    1712385\nChampaign city, Illinois\n88343\n45\n54115\n1105\n15741\n1104\n30\n29\n13310\n750\n24\n36\n1375\n464\n3748\n777\n950\n266\n2798\n684\n  \n  \n  \n\n\n\n\n\nTotal Population 88,343 (B02001_001E)\nWhite Population 54,115 (B02001_002E)\nBlack Population 15,741 (B02001_003E)\n\nThere’s a fairly sizable number of people in B02001_005E (13,310). What racial group is represented there? Given that large a number, let’s add this group into the racial summary table we’re working to develop:\n\n\nCode\nplace_race_2021 |&gt; \n  filter(NAME == \"Champaign city, Illinois\") |&gt;  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E+ B02001_005E)) |&gt; \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E, pop_asian = B02001_005E) |&gt; \n  select(Name, pop_tot, pop_white, pop_black, pop_asian, pop_other) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_asian\n      pop_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n13310\n5177\n  \n  \n  \n\n\n\n\nThe missing group with a fairly large population in this case is the Asian population. By adding this group, we lower the proportion of the total population in the “other” category substantially. This now looks a bit more presentable (and informative). There’s no hard and fast rule for which groups to represent. However, it’s important to note that it is often beneficial to make use of an “other” category in instances where you have small numbers (unless there’s detail there which your audience needs to see).\nLet’s go ahead and write this cleaned information back into place_race_2021:\n\n\nCode\nplace_race_2021&lt;-place_race_2021 |&gt; \n  filter(NAME == \"Champaign city, Illinois\") |&gt;  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E+ B02001_005E)) |&gt;  \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E, pop_asian = B02001_005E) |&gt;  \n  select(Name, pop_tot, pop_white, pop_black, pop_asian, pop_other)\n\nplace_race_2021 |&gt; gt()\n\n\n\n\n\n\n  \n    \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_asian\n      pop_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n13310\n5177\n  \n  \n  \n\n\n\n\nFrom here, we could do a few more things:\n\nConstruct an analogous table from our county data for Champaign County, Illinois.\nConstruct percentages for the values we’ve found, which can make comparison much easier.\n\n\n\nConstruct County Data\nPerform the same table cleaning and re-labeling for our county data that you did for our data for Champaign, and overwrite the existing data with this new data.\n\n\nCode\ncounty_race_2021 &lt;-get_acs(geography = \"county\", state = \"IL\", table = \"B02001\", year=2021, survey=\"acs5\", output=\"wide\") |&gt; \n  filter(NAME == \"Champaign County, Illinois\") |&gt;  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E+ B02001_005E)) |&gt;  \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E, pop_asian = B02001_005E) |&gt;  \n  select(Name, pop_tot, pop_white, pop_black, pop_asian, pop_other)\n\ncounty_race_2021 |&gt; gt()\n\n\n\n\n\n\n  \n    \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_asian\n      pop_other\n    \n  \n  \n    Champaign County, Illinois\n206583\n144522\n26996\n22204\n12861\n  \n  \n  \n\n\n\n\n\n\nConstruct Percentages\nNow that we have “clean” data for City of Champaign and Champaign County, calculate percentages of the total population for each racial category and add these as new columns in their respective data tables:\n\n\nCode\nplace_race_2021&lt;-\n  place_race_2021 |&gt;  \n  mutate(p_white = (pop_white/pop_tot)*100,\n         p_black = (pop_black/pop_tot)*100,\n         p_asian = (pop_asian / pop_tot)*100,\n         p_other = (pop_other/pop_tot)*100)\n\ncounty_race_2021&lt;-\n  county_race_2021 %&gt;% \n  mutate(p_white = (pop_white/pop_tot)*100,\n         p_black = (pop_black/pop_tot)*100,\n         p_asian = (pop_asian / pop_tot)*100,\n         p_other = (pop_other/pop_tot)*100)\n\n\nFrom here, we could copy and paste out values to construct a final table, or could combine values to create our own table:\n\n\nCode\nrace_2021&lt;-bind_rows(place_race_2021, county_race_2021)\n\nrace_2021 |&gt; gt()\n\n\n\n\n\n\n  \n    \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_asian\n      pop_other\n      p_white\n      p_black\n      p_asian\n      p_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n13310\n5177\n61.25556\n17.81805\n15.06628\n5.860113\n    Champaign County, Illinois\n206583\n144522\n26996\n22204\n12861\n69.95832\n13.06787\n10.74822\n6.225585\n  \n  \n  \n\n\n\n\nThe bind_rows() command allows us to take the rows in two separate data tables and combine them together based upon them having common column names (the columns don’t have to be in the same order). There’s also bind_columns() for the analogous operation on columns.\nThis puts us in a good position to make a final “pretty” table:\nFirst, for raw numbers:\n\n\nCode\nrace_2021 |&gt; \n  select(Name, pop_white, pop_black, pop_asian, pop_other, pop_tot) |&gt; \n  gt() |&gt; \n  fmt_number(2:6, decimals = 0) |&gt; \n  cols_label(\n    pop_white = \"White\",\n    pop_black = \"Black\",\n    pop_asian = \"Asian\",\n    pop_other = \"Other\",\n    pop_tot = \"Total Population\"\n    )\n\n\n\n\n\n\n  \n    \n    \n      Name\n      White\n      Black\n      Asian\n      Other\n      Total Population\n    \n  \n  \n    Champaign city, Illinois\n54,115\n15,741\n13,310\n5,177\n88,343\n    Champaign County, Illinois\n144,522\n26,996\n22,204\n12,861\n206,583\n  \n  \n  \n\n\n\n\nSecond, for percentages:\n\n\nCode\nrace_2021 |&gt; \n  select(Name, p_white, p_black, p_asian, p_other) |&gt; \n  gt() |&gt; \n  fmt_percent(2:5, decimals = 1, scale_values = FALSE) |&gt; \n  cols_label(\n    p_white = \"White\",\n    p_black = \"Black\",\n    p_asian = \"Asian\",\n    p_other = \"Other\"\n    )\n\n\n\n\n\n\n  \n    \n    \n      Name\n      White\n      Black\n      Asian\n      Other\n    \n  \n  \n    Champaign city, Illinois\n61.3%\n17.8%\n15.1%\n5.9%\n    Champaign County, Illinois\n70.0%\n13.1%\n10.7%\n6.2%\n  \n  \n  \n\n\n\n\nThe Klosterman book includes one more comparison - for the region. Champaign’s region includes Champaign, Ford, and Piatt Counties. We can construct regional numbers from the same county data which we used before. Unfortunately, we overwrote that existing data when we created our “clean” data just for Champaign County. Let’s download this again and construct our regional comparison:\n\n\nCode\nregion_race_2021&lt;- get_acs(geography = \"county\", state = \"IL\", table = \"B02001\", year=2021, survey=\"acs5\", output=\"wide\") |&gt;  \n  filter(NAME %in% c(\"Champaign County, Illinois\", \"Ford County, Illinois\", \"Piatt County, Illinois\")) |&gt; summarise(B02001_001E = sum(B02001_001E),\n            B02001_002E = sum(B02001_002E),\n            B02001_003E = sum(B02001_003E),\n            B02001_005E = sum(B02001_005E)) |&gt; \n  mutate(NAME = \"Region\") |&gt;  \n  mutate(pop_other = B02001_001E-(B02001_002E + B02001_003E+ B02001_005E)) |&gt;  \n  rename(Name = NAME, pop_tot = B02001_001E, pop_white = B02001_002E, pop_black = B02001_003E, pop_asian = B02001_005E) |&gt;  \n  select(Name, pop_tot, pop_white, pop_black, pop_asian, pop_other) |&gt;  \n  mutate(\n  p_white = (pop_white / pop_tot)*100,\n  p_black = (pop_black / pop_tot)*100,\n  p_asian = (pop_asian / pop_tot)*100,\n  p_other = (pop_other / pop_tot)*100)\n\n\nNote that the above code uses pipes to take all of the steps we had done above - downloading the data, querying out the counties of interest (remember that %in% allows us to match on multiple elements of a list), calculating values, to put them into one step. We now have a data processing pipeline which we only need to make minor modifications to in order to create a summary for any census geography. If you are sure that you have robust code present here, this will work just fine. It is perfectly acceptable to break this code into multiple chunks and then to overwrite once you are certain that the code will give you a desired outcome.\nAlso, did you catch how we combined information for Champaign, Ford, and Piatt counties into one “Region”? The following lines of code handle these steps:\n\n\nCode\nfilter(NAME %in% c(\"Champaign County, Illinois\", \"Ford County, Illinois\", \"Piatt County, Illinois\")) |&gt;  \n  summarise(B02001_001E = sum(B02001_001E),\n            B02001_002E = sum(B02001_002E),\n            B02001_003E = sum(B02001_003E),\n            B02001_005E = sum(B02001_005E)) |&gt; \n  mutate(NAME = \"Region\")\n\n\nWhat’s going on here?\n\nThe filter() section filters the data to the three rows in the dataset where the NAME field is equal to Champaign, Ford, or Piatt county.\nThe summarise() section is a little different than what we’ve seen before. In the past we’ve used this with group_by() in order to develop summaries for groups. Because we don’t have a group_by() clause here, R, summarizes all of the observations as one group. In this case, because we’re dealing with count data, we simply sum up all of the observations in a summary table where I have named the columns with the same names as the raw data had.\n\nFrom there, we can re-use the code we had already created to process the place and county data. With these things done, we can add our region values into our race_2021 summary table:\n\n\nCode\nrace_2021&lt;-bind_rows(race_2021, region_race_2021)\nrace_2021 |&gt; gt()\n\n\n\n\n\n\n  \n    \n    \n      Name\n      pop_tot\n      pop_white\n      pop_black\n      pop_asian\n      pop_other\n      p_white\n      p_black\n      p_asian\n      p_other\n    \n  \n  \n    Champaign city, Illinois\n88343\n54115\n15741\n13310\n5177\n61.25556\n17.81805\n15.066276\n5.860113\n    Champaign County, Illinois\n206583\n144522\n26996\n22204\n12861\n69.95832\n13.06787\n10.748222\n6.225585\n    Region\n236836\n173216\n27219\n22294\n14107\n73.13753\n11.49276\n9.413265\n5.956442\n  \n  \n  \n\n\n\n\nWhile the values here are transposed, we have created code which creates a table similar to table 2.1 in the Klosterman book."
  },
  {
    "objectID": "assignments/labs/05_population_census/05_census.html#closing-thoughts",
    "href": "assignments/labs/05_population_census/05_census.html#closing-thoughts",
    "title": "Population and the Census",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nThis tutorial has walked you through the steps necessary to start making basic calls to the Census API via tidycensus. There’s a lot more that you can do with tidycensus that we didn’t cover here, including accessing decennial census data, downloading census geometries along with data, and having tidycensus do some reshaping of your data. We will continue to explore these features as we add complexity in future labs."
  },
  {
    "objectID": "assignments/labs/05_population_census/05_census.html#lab-evaluation",
    "href": "assignments/labs/05_population_census/05_census.html#lab-evaluation",
    "title": "Population and the Census",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nUse of dplyr and tidyverse style formatting in your coding.\nRefined table output formatting using tools such as gt.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/05_population_census/05_census.html#references",
    "href": "assignments/labs/05_population_census/05_census.html#references",
    "title": "Population and the Census",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html",
    "href": "assignments/labs/06_segregation/06_segregation.html",
    "title": "Measuring Residential Segregation",
    "section": "",
    "text": "In this lab, we’ll explore some of the common measures of residential segregation and apply them at the city and regional levels.\nIn our last lab, we got a basic idea of how to summarize population racial characteristics for census tracts. Now we can start to think about summary measures of the extent to which these groups are segregated within the region. We will use census tracts as a unit of analysis to help us describe county-level racial segregation.\nMost measures of residential segregation are set up to summarize the level of heterogeneity (or homogeneity) observed between neighborhoods to a city or regional scale to produce a summary measure that then tells us how even populations are distributed at the neighborhood level.\nThere are also some measures that focus more on segregation at a neighborhood level or below. In this lab, we’ll focus on those measures that summarize characteristics to more aggregate levels such as cities or regions."
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#introduction",
    "href": "assignments/labs/06_segregation/06_segregation.html#introduction",
    "title": "Measuring Residential Segregation",
    "section": "",
    "text": "In this lab, we’ll explore some of the common measures of residential segregation and apply them at the city and regional levels.\nIn our last lab, we got a basic idea of how to summarize population racial characteristics for census tracts. Now we can start to think about summary measures of the extent to which these groups are segregated within the region. We will use census tracts as a unit of analysis to help us describe county-level racial segregation.\nMost measures of residential segregation are set up to summarize the level of heterogeneity (or homogeneity) observed between neighborhoods to a city or regional scale to produce a summary measure that then tells us how even populations are distributed at the neighborhood level.\nThere are also some measures that focus more on segregation at a neighborhood level or below. In this lab, we’ll focus on those measures that summarize characteristics to more aggregate levels such as cities or regions."
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#goals",
    "href": "assignments/labs/06_segregation/06_segregation.html#goals",
    "title": "Measuring Residential Segregation",
    "section": "Goals",
    "text": "Goals\n\nGain exposure to common measures of residential segregation.\nLearn how to apply simple measures within R / RStudio.\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#github-lab-repository",
    "href": "assignments/labs/06_segregation/06_segregation.html#github-lab-repository",
    "title": "Measuring Residential Segregation",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nFollow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#setup",
    "href": "assignments/labs/06_segregation/06_segregation.html#setup",
    "title": "Measuring Residential Segregation",
    "section": "Setup",
    "text": "Setup\nLet’s start as usual by loading the packages that we’ll need for our work.\n\n\nCode\nlibrary(gt)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(sf)"
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#two-useful-functions",
    "href": "assignments/labs/06_segregation/06_segregation.html#two-useful-functions",
    "title": "Measuring Residential Segregation",
    "section": "Two Useful Functions",
    "text": "Two Useful Functions\n\nSeparate\nSometimes we may have a variable that we need to separate into based upon a known delimiter. separate allows us to do so. Let’s explore an example. Here’s a table containing information on the name and location of Big 10 schools.\n\n\nCode\nInstitution &lt;- c(\n  \"University of Illinois\", \n  \"Indiana University\", \n  \"University of Iowa\", \n  \"University of Maryland\", \n  \"University of Michigan\", \n  \"Michigan State University\", \n  \"University of Minnesota\", \n  \"University of Nebraska-Lincoln\", \n  \"Northwestern University\", \n  \"Ohio State University\", \n  \"Pennsylvania State University\", \n  \"Purdue University\", \n  \"Rutgers University\", \n  \"University of Wisconsin-Madison\")\n\nLocation &lt;- c(\n  \"Champaign, Champaign County, Illinois\",\n  \"Bloomington, Monroe County, Indiana\",\n  \"Iowa City, Johnson County, Iowa\",\n  \"College Park, Baltimore County, Maryland\",\n  \"Ann Arbor, Washtenaw County, Michigan\",\n  \"East Lansing, Ingham County, Michigan\",\n  \"Minneapolis, Hennepin County, Minnesota\",\n  \"Lincoln, Lancaster County, Nebraska\",\n  \"Evanston, Cook County, Illinois\",\n  \"Columbus, Franklin County, Ohio\",\n  \"State College, Centre County, Pennsylvania\",\n  \"West Lafayette, Tippecanoe County, Indiana\",\n  \"Newark, Middlesex County, New Jersey\",\n  \"Madison, Dane County, Wisconsin\"\n  )\n\nbig_10 &lt;- tibble(Institution, Location)\n\nbig_10 |&gt; gt()\n\n\n\n\n\n\n  \n    \n    \n      Institution\n      Location\n    \n  \n  \n    University of Illinois\nChampaign, Champaign County, Illinois\n    Indiana University\nBloomington, Monroe County, Indiana\n    University of Iowa\nIowa City, Johnson County, Iowa\n    University of Maryland\nCollege Park, Baltimore County, Maryland\n    University of Michigan\nAnn Arbor, Washtenaw County, Michigan\n    Michigan State University\nEast Lansing, Ingham County, Michigan\n    University of Minnesota\nMinneapolis, Hennepin County, Minnesota\n    University of Nebraska-Lincoln\nLincoln, Lancaster County, Nebraska\n    Northwestern University\nEvanston, Cook County, Illinois\n    Ohio State University\nColumbus, Franklin County, Ohio\n    Pennsylvania State University\nState College, Centre County, Pennsylvania\n    Purdue University\nWest Lafayette, Tippecanoe County, Indiana\n    Rutgers University\nNewark, Middlesex County, New Jersey\n    University of Wisconsin-Madison\nMadison, Dane County, Wisconsin\n  \n  \n  \n\n\n\n\nThe Location field has quite a bit of information present that we might use to learn more about the locations of Big 10 schools. Let’s use separate() to split the city, county, and state into their own fields.\nTo use separate(), we specify the name of the field we want to separate, we specify the new column names we want to assign to each of the components we’re separating, and we indicate what character is the separator (in this case a comma “,”).\n\n\nCode\nbig_10 |&gt; separate(Location, into = c(\"City\", \"County\", \"State\"), sep = \",\") |&gt; gt()\n\n\n\n\n\n\n  \n    \n    \n      Institution\n      City\n      County\n      State\n    \n  \n  \n    University of Illinois\nChampaign\n Champaign County\n Illinois\n    Indiana University\nBloomington\n Monroe County\n Indiana\n    University of Iowa\nIowa City\n Johnson County\n Iowa\n    University of Maryland\nCollege Park\n Baltimore County\n Maryland\n    University of Michigan\nAnn Arbor\n Washtenaw County\n Michigan\n    Michigan State University\nEast Lansing\n Ingham County\n Michigan\n    University of Minnesota\nMinneapolis\n Hennepin County\n Minnesota\n    University of Nebraska-Lincoln\nLincoln\n Lancaster County\n Nebraska\n    Northwestern University\nEvanston\n Cook County\n Illinois\n    Ohio State University\nColumbus\n Franklin County\n Ohio\n    Pennsylvania State University\nState College\n Centre County\n Pennsylvania\n    Purdue University\nWest Lafayette\n Tippecanoe County\n Indiana\n    Rutgers University\nNewark\n Middlesex County\n New Jersey\n    University of Wisconsin-Madison\nMadison\n Dane County\n Wisconsin\n  \n  \n  \n\n\n\n\nWe get back three new fields that replace the existing location field.\n\n\nTally\nWe have used the combination of group_by() and summarise() on many occasions in order to aggregate characteristics of data by groups. For simple operations, there are some helper functions that we can use to simplify our aggregation of groups. tally() for instance is the equivalent of summarise(n = n()) which creates a count of the observations in each group.\nLet’s explore an example using our Big 10 data. Let’s say, for instance, that we want to count the number of Big 10 schools in each state. How would we do this using group_by() and summarise()?\n\n\nCode\nbig_10 |&gt; \n  separate(Location, into = c(\"City\", \"County\", \"State\"), sep = \",\") |&gt; \n  group_by(State) |&gt; \n  summarise(Institutions = n()) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      State\n      Institutions\n    \n  \n  \n     Illinois\n2\n     Indiana\n2\n     Iowa\n1\n     Maryland\n1\n     Michigan\n2\n     Minnesota\n1\n     Nebraska\n1\n     New Jersey\n1\n     Ohio\n1\n     Pennsylvania\n1\n     Wisconsin\n1\n  \n  \n  \n\n\n\n\nIn this case, we’re taking our raw Big 10 data, separating the location into three columns, and then building a summary based upon the state field.\nHere’s how we might do the same thing using group_by() and tally().\n\n\nCode\nbig_10 |&gt; \n  separate(Location, into = c(\"City\", \"County\", \"State\"), sep = \",\") |&gt; \n  group_by(State) |&gt; \n  tally() |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      State\n      n\n    \n  \n  \n     Illinois\n2\n     Indiana\n2\n     Iowa\n1\n     Maryland\n1\n     Michigan\n2\n     Minnesota\n1\n     Nebraska\n1\n     New Jersey\n1\n     Ohio\n1\n     Pennsylvania\n1\n     Wisconsin\n1\n  \n  \n  \n\n\n\n\nWe get output that is basically the same, just with a count column labelled “n”. We can specify the name of the count column so that we have a label that’s more descriptive:\n\n\nCode\nbig_10 |&gt; \n  separate(Location, into = c(\"City\", \"County\", \"State\"), sep = \",\") |&gt; \n  group_by(State) |&gt; \n  tally(name = \"Institutions\") |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      State\n      Institutions\n    \n  \n  \n     Illinois\n2\n     Indiana\n2\n     Iowa\n1\n     Maryland\n1\n     Michigan\n2\n     Minnesota\n1\n     Nebraska\n1\n     New Jersey\n1\n     Ohio\n1\n     Pennsylvania\n1\n     Wisconsin\n1"
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#building-census-geographies",
    "href": "assignments/labs/06_segregation/06_segregation.html#building-census-geographies",
    "title": "Measuring Residential Segregation",
    "section": "Building Census Geographies",
    "text": "Building Census Geographies\nWe are going to focus on analyzing population by race for New York City. We need to start by defining our geography for the city. The Tigris package can help us download census geometries directly from the census servers. We could also as tidycensus to download census geographies when we pull data, but let’s download geographies separately for now so that we can refine them before adding other data.\nLet’s start by downloading census tract boundaries for New York City’s Boroughs. We will then map these directly in RStudio. Having a borough map handy provides a good reminder of the city’s geography:\n The city consists of five boroughs, each of which is its own county (this is atypical, but it’s a New York thing).\n\nNew York City Boroughs\n\n\nFIPS Code\nCounty Name\nBorough Name\n\n\n\n\n36047\nKings County\nBrooklyn\n\n\n36005\nBronx County\nBronx\n\n\n36081\nQueens County\nQueens\n\n\n36085\nRichmond County\nStaten Island\n\n\n36061\nNew York County\nManhattan\n\n\n\nIn building a tracts call using tigris (to download tract geometries), we’ll need to specify at a minimum the state and the vintage of tract geometry we want to download. Let’s download 2020 tract data:\nLet’s use ggplot() to visualize so that we can confirm what we’ve downloaded is indeed all tracts in the state of New York:\n\n\nCode\nggplot() + \n  geom_sf(data=nyc_trt)\n\n\n\n\n\nThat looks roughly like the state of New York. Congratulations if this is your first map visualization in RStudio!\nWe have some things to unpack here. Out tigris call downloaded all tracts for the state of New York. We have tract properties, but then a list of lists in a field called “geometry”. What’s going on here?\nOur tract geographies were downloaded in what is known as simple features format. If you have dealt with prior formats for spatial data in R, you’ll understand that simple features really are much simpler! The list that you see here is a list of points that define the vertices, in this case, which represent each tract polygon. Embedding the geometry with data that represents the qualities of each tract is very useful.\nWe could query down to the counties that define New York City with the information we have in hand:\n\n\nCode\nnyc_trt &lt;- nyc_trt |&gt; \n  filter(COUNTYFP %in% c(\"047\", \"005\", \"081\", \"085\", \"061\"))\n\n\nAnd again, let’s visualize to confirm:\n\n\nCode\nggplot()+\n  geom_sf(data=nyc_trt)\n\n\n\n\n\nAgain, a lot going on here, but we definitely have a subgeography from the New York State data.\nWhile downloading tracts for the whole state and then querying out NYC counties is ok, tigris, can accept counties as inputs to its API call:\n\n\nCode\nnyc_trt &lt;- tracts(state = \"NY\", county = c(\"Bronx\", \"Kings\", \"New York\", \"Queens\", \"Richmond\"), year = 2020)\n\n\nHere we are just specifying counties by name - tigris looks them up and confirms which counties it has downloaded.\nBefore we get into modifying this geography, let’s transform it to an appropriate projection. The sf (simple features) package can handle this for us with st_transform()\n\n\nCode\nnyc_trt &lt;- nyc_trt |&gt; \n  st_transform(26918)\n\n\nThe string of numbers here refers to the particular transformation we’re doing - in this case to NAD 1983, a common coordinate reference system for the United States.\nOur NYC tracts look a little messy - they don’t really look like the shape of New York City. Let’s think about what role water might be playing here. In the tract data, there are fields called ALAND and AWATER which correspond to the tract area that is land and water (respectively). Let’s take a look to see if there are any tracts that are all water:\n\n\nCode\nnyc_trt |&gt; \n  filter(ALAND == 0)\n\n\nSimple feature collection with 3 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 562802.4 ymin: 4480943 xmax: 605316.4 ymax: 4495548\nProjected CRS: NAD83 / UTM zone 18N\n  STATEFP COUNTYFP TRACTCE       GEOID NAME          NAMELSAD MTFCC FUNCSTAT\n1      36      081  990100 36081990100 9901 Census Tract 9901 G5020        S\n2      36      047  990100 36047990100 9901 Census Tract 9901 G5020        S\n3      36      085  990100 36085990100 9901 Census Tract 9901 G5020        S\n  ALAND    AWATER    INTPTLAT     INTPTLON                       geometry\n1     0 130800077 +40.5380517 -073.8942020 MULTIPOLYGON (((581457.1 44...\n2     0  17793788 +40.5649788 -074.0148235 MULTIPOLYGON (((580899.6 44...\n3     0  81772671 +40.5249107 -074.1091816 MULTIPOLYGON (((562808.8 44...\n\n\nIt turns out we have three tracts with no land area and that are all water. Let’s remove them and see what our data looks like:\n\n\nCode\nnyc_trt &lt;- nyc_trt |&gt; \n  filter(ALAND &gt; 0)\n\nggplot() +\n  geom_sf(data=nyc_trt)\n\n\n\n\n\nA little better - we dropped a few tracts and can start to see more of the shape of the city. Let’s continue to diagnose what’s going on here by coloring each borough so we can see boundaries:\n\n\nCode\nggplot() +\n  geom_sf(data=nyc_trt, aes(fill = COUNTYFP))\n\n\n\n\n\nWe add an aesthetic mapping to our ggplot() call that assigns a unique color to each county. Referring back to our borough image of New York City, it looks like tracts are continuous with some portions covering parts of water bodies.\nCan we do any better? You know the answer is yes! tigris can use census hydrography to clip the portions of tracts that are water:\n\n\nCode\nnyc_trt &lt;- nyc_trt |&gt; \n  erase_water(area_threshold = .9, year = 2020)\n\n\nFetching area water data for your dataset's location...\n\n\nErasing water area...\nIf this is slow, try a larger area threshold value.\n\n\nYou will need to play with the threshold value to get something that looks good. In this case, we’re using a threshold of .9 which means that we are only removing the water areas that represent the largest 10 percent of all water geometries in the map.\nLet’s once again visualize to see what we have here:\n\n\nCode\nggplot() +\n  geom_sf(data=nyc_trt, aes(fill = COUNTYFP))\n\n\n\n\n\nWe have a bit more definition now.\nBefore we move on to adding census data to these tracts, let’s use them to create a new geometry - county boundaries. Yes, we could download them using tigris, but we actually have all the data we need in our tract files.\nOne very cool think about the implementation of simple features in R is that tidyverse commands like group_by() and summarise() work on them. What if we group tracts by their county identifier and summarise?\n\n\nCode\nnyc_co &lt;- nyc_trt |&gt; \n  group_by(COUNTYFP) |&gt; \n  summarise(tracts = n())\n\nggplot() +\n  geom_sf(data = nyc_co, aes(fill = COUNTYFP))\n\n\n\n\n\nWe have now created a custom set of county boundaries for New York City.\n\nStyling Maps\nI am a big proponent of making many bad maps so that you can progressively refine your visualization to something effective. Let’s think about some key controls for styling our maps.\nFirst, let’s start with our base tract data:\n\n\nCode\nggplot()+\n  geom_sf(data = nyc_trt)\n\n\n\n\n\nLet’s use a theme to remove some of the shading:\n\n\nCode\nggplot()+\n  geom_sf(data = nyc_trt) +\n  theme_minimal()\n\n\n\n\n\nIf you don’t want the graticules and latitude / longitude markings, you can either use a theme to remove them:\n\n\nCode\nggplot()+\n  geom_sf(data = nyc_trt) +\n  theme_void()\n\n\n\n\n\nOr you can manually remove them this way:\n\n\nCode\nggplot()+\n  geom_sf(data = nyc_trt) +\n  coord_sf(datum = NA)\n\n\n\n\n\nLet’s add counties as an overlay:\n\n\nCode\nggplot()+\n  geom_sf(data = nyc_trt) +\n  geom_sf(data = nyc_co) +\n  coord_sf(datum = NA)\n\n\n\n\n\nHmm - where’s our tracts? We can see our counties, but tracts are underneath! Let’s remove the fill from the counties file so we can see the tracts underneath:\n\n\nCode\nggplot()+\n  geom_sf(data = nyc_trt) +\n  geom_sf(data = nyc_co, fill=NA) +\n  coord_sf(datum = NA)\n\n\n\n\n\nNow we can see the tracts, but what about our counties? Let’s use color to differentiate?\n\n\nCode\nggplot()+\n  geom_sf(data = nyc_trt) +\n  geom_sf(data = nyc_co, fill = NA, color = \"blue\") +\n  coord_sf(datum = NA)\n\n\n\n\n\nOk - now we can see the counties. Why don’t we try to make the line weight heavier on the county outlines to create more hierarchy here.\n\n\nCode\nggplot()+\n  geom_sf(data = nyc_trt) +\n  geom_sf(data = nyc_co, fill = NA, color = \"blue\", cex = .7) +\n  coord_sf(datum = NA)\n\n\n\n\n\nA little thicker. Why don’t we also decrease the line weight of the tracts to help differentiate more?\n\n\nCode\nggplot()+\n  geom_sf(data = nyc_trt, cex = .1) + # Decrease the line weight to differentiate\n  geom_sf(data = nyc_co, fill = NA, color = \"blue\", cex = .7)+ # Increase the line weight to differentiate\n  coord_sf(datum = NA)\n\n\n\n\n\nThat’s starting to look better. Maybe now we can try again but using only gray lines:\n\n\nCode\nggplot()+\n  geom_sf(data = nyc_trt, cex = .1, color = \"gray70\", fill = NA) + # Decrease the line weight to differentiate\n  geom_sf(data = nyc_co, fill = NA, color = \"gray30\", cex = .6)+ # Increase the line weight to differentiate\n  coord_sf(datum = NA) +\n  theme_minimal()\n\n\n\n\n\nLet’s consider this “styled’ enough for now…"
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#downloading-census-data",
    "href": "assignments/labs/06_segregation/06_segregation.html#downloading-census-data",
    "title": "Measuring Residential Segregation",
    "section": "Downloading Census Data",
    "text": "Downloading Census Data\nFor the purposes of our example, we’ll examine racial segregation in New York City. Let’s start by downloading and preparing data on race for New York City.\n\nGet Tract Boundaries\nLet’s download 2020 census tract boundaries for New York City’s five Boroughs. While we’re at it, we’ll clean them up a little bit.\n\nNew York City Boroughs\n\n\nFIPS Code\nCounty Name\nBorough Name\n\n\n\n\n36047\nKings County\nBrooklyn\n\n\n36005\nBronx County\nBronx\n\n\n36081\nQueens County\nQueens\n\n\n36085\nRichmond County\nStaten Island\n\n\n36061\nNew York County\nManhattan\n\n\n\n\n\nCode\nnyc_trt &lt;- tracts(state = \"NY\", county = c(\"005\", \"047\", \"061\", \"081\", \"085\"), year = 2020) |&gt; \n  st_transform(26918) |&gt; \n  filter(ALAND != 0) |&gt; \n  erase_water(area_threshold = .9, year = 2020)\n\n\nThe code above is accomplishing the following things:\n\nDownload tract-level census geometries as a simple features object for Kings, Bronx, Queens, Richmond, and New York Counties for the year 2020.\nTransform these geometries to the NAD 1983 coordinate reference system.\nRemove those defined tract boundaries with no land area.\nErase the portion of tract boundaries that are water, focusing on the top 10 percent of largest defined water polygons, and using hydrography defined by the census in 2020.\n\nLet’s use ggplot() to visualize so that we can confirm what we’ve downloaded is indeed New York City’s boroughs:\n\n\nCode\nggplot()+\n  geom_sf(data=nyc_trt)\n\n\n\n\n\n\n\nGet ACS Data\nNext, we’ll download ACS data on race for New York City. We’ll focus on 2020 data from the 5-year ACS from Table B03002.\nTo keep things organized, let’s define the variables we want to download. Note that we’re defining variable names as well as values. To facilitate some of our later operations, let’s download these data in wide format rather than tidy format. In wide format, we’ll have one observation (row) for each census tract, with multiple columns containing estimates for each population subgroup and the total population.\n\n\nCode\ndl_vars &lt;- c(\n  White = \"B03002_003\",\n  Black = \"B03002_004\",\n  Native = \"B03002_005\",\n  Asian = \"B03002_006\",\n  HIPI = \"B03002_007\",\n  Hispanic = \"B03002_012\",\n  Poptot = \"B03002_001\"\n)\n\n# Our tidycensus download call:\n\nnyc_race &lt;- get_acs(\n  geography = \"tract\",\n  state = \"NY\",\n  county = c(\"Bronx\", \"Kings\", \"New York\", \"Queens\", \"Richmond\"),\n  variables = dl_vars,\n  year = 2020,\n  output = \"wide\"\n) \n\nnyc_race |&gt; \n  slice_head(n = 5) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      NAME\n      WhiteE\n      WhiteM\n      BlackE\n      BlackM\n      NativeE\n      NativeM\n      AsianE\n      AsianM\n      HIPIE\n      HIPIM\n      HispanicE\n      HispanicM\n      PoptotE\n      PoptotM\n    \n  \n  \n    36047009202\nCensus Tract 92.02, Kings County, New York\n328\n96\n162\n129\n0\n12\n1037\n293\n0\n12\n1904\n630\n3453\n653\n    36047009401\nCensus Tract 94.01, Kings County, New York\n132\n72\n173\n245\n0\n12\n1582\n389\n0\n12\n406\n283\n2293\n380\n    36047009402\nCensus Tract 94.02, Kings County, New York\n377\n198\n38\n57\n0\n12\n1903\n586\n0\n12\n271\n213\n2746\n553\n    36047009600\nCensus Tract 96, Kings County, New York\n1035\n313\n78\n60\n0\n17\n1993\n519\n0\n17\n2629\n629\n5858\n710\n    36047009800\nCensus Tract 98, Kings County, New York\n218\n83\n162\n160\n0\n17\n2306\n504\n0\n17\n3275\n740\n6021\n798\n  \n  \n  \n\n\n\n\nNote that this code defines the list of specific variables to download (rather than a whole table), asks tidycensus to download the data, and labels each column with the label we have defined (with E and M denoting estimates and margins of error because this is sample-based data).\nWhile we’re at it, let’s separate our tract name label into separate tract, county, and state values.\n\n\nCode\nnyc_race &lt;- nyc_race |&gt; \n  separate(NAME, into = c(\"Tract\", \"County\", \"State\"), sep = \", \")\n\n\nNote the use of sep = \", \" includes the space after the comma so we don’t have an extra space in our resulting county and state variables.\n\n\nCreate County Summaries\nBefore we further manipulate our data, let’s aggregate the data we have to form county population summaries. Because our tract data is hierarchical, we can sum up our population estimates from tracts to counties to produce county population totals. While we’re at it, we’ll also use the county FIPS codes to create a Borough identifier (we could also add county labels back as well).\n\n\nCode\nnyc_race_co &lt;-\n  nyc_race |&gt; \n  group_by(STCO = substr(GEOID, 0, 5)) |&gt; \n  summarise(\n    co_WhiteE = sum(WhiteE, na.rm = TRUE),\n    co_WhiteM = moe_sum(WhiteM, WhiteE),\n    co_BlackE = sum(BlackE, na.rm = TRUE),\n    co_BlackM = moe_sum(BlackM, BlackE),\n    co_NativeE = sum(NativeE, na.rm = TRUE),\n    co_NativeM = moe_sum(NativeM, NativeE),\n    co_AsianE = sum(AsianE, na.rm = TRUE),\n    co_AsianM = moe_sum(AsianM, AsianE),\n    co_HIPIE = sum(HIPIE, na.rm = TRUE),\n    co_HIPIM = moe_sum(HIPIM, HIPIE),\n    co_HispanicE = sum(HispanicE, na.rm = TRUE),\n    co_HispanicM = moe_sum(HispanicM, HispanicE),\n    co_PoptotE = sum(PoptotE, na.rm = TRUE),\n    co_PoptotM = moe_sum(PoptotM, PoptotE),\n  ) |&gt; \n  mutate(Borough =\n    case_when(\n      STCO == \"36005\" ~ \"Bronx\",\n      STCO == \"36047\" ~ \"Brooklyn\",\n      STCO == \"36061\" ~ \"Manhattan\",\n      STCO == \"36081\" ~ \"Queens\",\n      STCO == \"36085\" ~ \"Staten Island\")\n  )\n\n\nLet’s walk through this:\n\nWe are going to use group_by() to aggregate our tract-level data to counties. In this case, we are pulling out the first five digits from the combined state-county-tract FIPS code (substr(GEOID, 0, 5)) using the substr() command from the stringr package. This allows us to create a combined state and county FIPS code (the state is the first two digits - 36 in the case of New York - and the county is the next three digits).\nWe use summarise() to create summary values for counties. We use sum() on our population estimates to sum up populations for tracts into county totals. We use moe_sum() from the tidycensus() package to properly aggregate margins of error accounting for the size of the population estimate being aggregated. The census provides documentation on what’s implemented in tidycensus. We denote that these are county estimates by including co_ as a prefix - you’ll see why later.\nWe use case_when() construct borough labels from our combined state and county FIPS codes.\n\nAs a result, we have a table containing county populations and margins of error for the boroughs of New York City:\n\n\n\n\n\n\n  \n    \n    \n      STCO\n      co_WhiteE\n      co_WhiteM\n      co_BlackE\n      co_BlackM\n      co_NativeE\n      co_NativeM\n      co_AsianE\n      co_AsianM\n      co_HIPIE\n      co_HIPIM\n      co_HispanicE\n      co_HispanicM\n      co_PoptotE\n      co_PoptotM\n      Borough\n    \n  \n  \n    36005\n128,717\n3,742\n411,293\n8,800\n2,882\n727\n53,498\n3,097\n374\n198\n799,765\n12,002\n1,427,056\n14,216\nBronx\n    36047\n938,573\n10,493\n754,576\n10,307\n3,053\n638\n304,420\n6,367\n1,009\n480\n486,272\n9,181\n2,576,771\n17,237\nBrooklyn\n    36061\n763,202\n11,503\n199,377\n6,238\n1,960\n581\n196,097\n6,049\n430\n237\n418,442\n9,769\n1,629,153\n16,500\nManhattan\n    36081\n559,778\n8,090\n385,868\n7,374\n5,561\n948\n584,214\n9,369\n832\n271\n631,657\n10,976\n2,270,976\n16,687\nQueens\n    36085\n286,462\n6,247\n43,941\n2,970\n515\n243\n47,194\n2,905\n55\n127\n87,733\n4,193\n475,596\n7,858\nStaten Island\n  \n  \n  \n\n\n\n\nLet’s create a more polished table showing the population by race for each NYC Borough:\n\n\nCode\nnyc_race_co |&gt; \n  mutate(other = co_NativeE+co_HIPIE) |&gt; \n  select(Borough, co_WhiteE, co_BlackE, co_AsianE, co_HispanicE, other, co_PoptotE) |&gt; \n  gt() |&gt; \n  fmt_number(2:7, decimals = 0) |&gt; \n  cols_label(\n    co_WhiteE = \"White\",\n    co_BlackE = \"Black\",\n    co_AsianE = \"Asian\",\n    co_HispanicE = \"Hispanic\",\n    other = \"Other\",\n    co_PoptotE = \"Total\"\n  )\n\n\n\n\n\n\n  \n    \n    \n      Borough\n      White\n      Black\n      Asian\n      Hispanic\n      Other\n      Total\n    \n  \n  \n    Bronx\n128,717\n411,293\n53,498\n799,765\n3,256\n1,427,056\n    Brooklyn\n938,573\n754,576\n304,420\n486,272\n4,062\n2,576,771\n    Manhattan\n763,202\n199,377\n196,097\n418,442\n2,390\n1,629,153\n    Queens\n559,778\n385,868\n584,214\n631,657\n6,393\n2,270,976\n    Staten Island\n286,462\n43,941\n47,194\n87,733\n570\n475,596\n  \n  \n  \n\n\n\n\nNote the strategic use of an “Other” category which combines the Native American and Hawaiian Island and Pacific Islander categories.\n\n\nSetup Summary\nWe now have three resources available to us:\n\nnyc_race: ACS data on race for all tracts in New York City.\nnyc_race_co: ACS data on race for all counties in New York City.\nnyc_trt: Tract geometry data for all tracts in New York City.\n\nThese three pieces of information can form the basis for manipulation and analysis of dimensions of racial segregation in New York City.\nLet’s now turn towards the measures of segregation."
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#dissimilarity",
    "href": "assignments/labs/06_segregation/06_segregation.html#dissimilarity",
    "title": "Measuring Residential Segregation",
    "section": "Dissimilarity",
    "text": "Dissimilarity\nDissimilarity is a common measure of evenness between two populations - dissimilarity measures the distribution of a minority population within a majority population. Conceptually, dissimilarity measures the proportion of that minority or subgroup population that would need to move in order to be equally distributed with the majority population. A dissimilarity score of 0 would mean a completely even distribution of a minority population amongst a majority population (no segregation). A dissimilarity score of 100 would mean a completely segregated minority population (100 percent of the minority population would need to move to achieve an even distribution amongst the population. Dissimilarity is calculated as follows:\n\\(D = .5*\\sum_i |\\frac{b_i}{B}-\\frac{w_i}{W}|\\)\nwhere \\(b_i\\) is the number of blacks in tract i\n\\(B\\) is the number of blacks for the county\n\\(w_i\\) is the number of whites in tract i\n\\(W\\) is the number of whites for the county\nNow that we know the formula, we can start thinking about how to implement this in R with dplyr.\n\nBlack-White Dissimilarity\nThis looks complicated, but we can actually do this fairly simply with dplyr notation. Let’s start off by calculating Black-White dissimilarity.\nWe have our white and black tract population data already in our il_trt data, and we have our county data in a separate object. Let’s go ahead and join that county data to our tract data.\n\n\nCode\nnyc_race&lt;-left_join(nyc_race |&gt; mutate(STCO = substr(GEOID, 0,5)), nyc_race_co, by= \"STCO\")\n\n\nNotice that this join looks pretty standard with the exception of nyc_race |&gt; mutate(STCO = substr(GEOID, 0,5)). When we created our county-level summaries of the tract-level data, we created a new variable called STCO which is the combined city and county FIPS codes. In order to join those data to our tract data, we need to temporarily create the same code from our GEOID field in the tract data. The mutate statement temporarily creates the STCO column in the nyc_race data, and the join can be performed denoting the common column, STCO.\nNow that we’ve joined these together, we can start breaking down the dissimilarity formula into code-able pieces. Inside the absolute value we are calculating fractions of tract minority and majority populations compared to their county. We then subtract these from each other and use abs() to find the absolute value. After that, we need to sum these all up and multiply by .5. We are calculating these by county, so we can use group_by() to sum up only the data for each county to produce a series of final dissimilarity statistics for each county in New York City. For other regions, you might end up calculating dissimilarity for multiple counties within a region.\n\n\nCode\nnyc_race  |&gt;\n  mutate(dissim_wb = abs(BlackE / co_BlackE - WhiteE / co_WhiteE)) |&gt;  \n  group_by(Borough) |&gt;  \n  summarise(dissimilarity = .5*sum(dissim_wb)) |&gt; gt()\n\n\n\n\n\n\n  \n    \n    \n      Borough\n      dissimilarity\n    \n  \n  \n    Bronx\n0.7107378\n    Brooklyn\n0.7731939\n    Manhattan\n0.7057447\n    Queens\n0.8146201\n    Staten Island\n0.7226401\n  \n  \n  \n\n\n\n\nHow would you interpret these black-white dissimilarity values? Where is Black-White dissimilarity the highest within the region? Where is it the lowest?"
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#nonwhite-white-dissimilarity",
    "href": "assignments/labs/06_segregation/06_segregation.html#nonwhite-white-dissimilarity",
    "title": "Measuring Residential Segregation",
    "section": "Nonwhite-White Dissimilarity",
    "text": "Nonwhite-White Dissimilarity\n\n\nCode\nnyc_race |&gt;\n  mutate(dissim_nww = abs((PoptotE-WhiteE) / (co_PoptotE-co_WhiteE) - WhiteE / co_WhiteE)) |&gt; \n  group_by(Borough) |&gt; \n  summarise(dissimilarity = .5*sum(dissim_nww)) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      Borough\n      dissimilarity\n    \n  \n  \n    Bronx\n0.6311840\n    Brooklyn\n0.5502573\n    Manhattan\n0.5144714\n    Queens\n0.5274621\n    Staten Island\n0.4167665\n  \n  \n  \n\n\n\n\nCompare and contrast this with Black-White dissimilarity - what do you observe?"
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#latino-white-dissimilarity",
    "href": "assignments/labs/06_segregation/06_segregation.html#latino-white-dissimilarity",
    "title": "Measuring Residential Segregation",
    "section": "Latino-White Dissimilarity",
    "text": "Latino-White Dissimilarity\n\n\nCode\nnyc_race  |&gt; \n  mutate(dissim_lw = abs(HispanicE / co_HispanicE - WhiteE / co_WhiteE)) |&gt; \n  group_by(Borough) |&gt;  \n  summarise(dissimilarity = .5*sum(dissim_lw)) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      Borough\n      dissimilarity\n    \n  \n  \n    Bronx\n0.6244812\n    Brooklyn\n0.5451200\n    Manhattan\n0.6129532\n    Queens\n0.5070836\n    Staten Island\n0.4122438\n  \n  \n  \n\n\n\n\nCompare and contrast this with Black-White dissimilarity - what do you observe?"
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#nonwhite-isolation",
    "href": "assignments/labs/06_segregation/06_segregation.html#nonwhite-isolation",
    "title": "Measuring Residential Segregation",
    "section": "Nonwhite Isolation",
    "text": "Nonwhite Isolation\n\n\nCode\nnyc_race |&gt; \n  mutate(isolation_bw = ((PoptotE - WhiteE) / (co_PoptotE - co_WhiteE) * (PoptotE-WhiteE) / PoptotE)) |&gt; \n  group_by(Borough) |&gt; \n  summarise(isolation = sum(isolation_bw, na.rm=TRUE)) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      Borough\n      isolation\n    \n  \n  \n    Bronx\n0.9369248\n    Brooklyn\n0.7689225\n    Manhattan\n0.6803407\n    Queens\n0.8228511\n    Staten Island\n0.5476338"
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#latino-isolation",
    "href": "assignments/labs/06_segregation/06_segregation.html#latino-isolation",
    "title": "Measuring Residential Segregation",
    "section": "Latino Isolation",
    "text": "Latino Isolation\n\n\nCode\nnyc_race |&gt; \n  mutate(isolation_bw = (HispanicE / co_HispanicE * HispanicE / PoptotE)) |&gt;  \n  group_by(Borough) |&gt;\n  summarise(isolation = sum(isolation_bw, na.rm=TRUE)) |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      Borough\n      isolation\n    \n  \n  \n    Bronx\n0.6313472\n    Brooklyn\n0.3350622\n    Manhattan\n0.4678842\n    Queens\n0.4283512\n    Staten Island\n0.2666154\n  \n  \n  \n\n\n\n\nProvide your interpretation of how isolation differs for these groups and for each borough."
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#lab-evaluation",
    "href": "assignments/labs/06_segregation/06_segregation.html#lab-evaluation",
    "title": "Measuring Residential Segregation",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nProper use of tigris tidycensus to download tabular and spatial data from the census bureau.\nProduction of clearly-labelled and well-formatted tables.\nProduction of well-formatted maps of income inequality.\nDetailed reflection and diagnosis using measure of segregation.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/06_segregation/06_segregation.html#references",
    "href": "assignments/labs/06_segregation/06_segregation.html#references",
    "title": "Measuring Residential Segregation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html",
    "title": "Place Opportunity",
    "section": "",
    "text": "In this lab, we’ll extend our knowledge of opportunity maps, and more generally in standardizing and creating indexes from data. This strategy is in some ways an extension of the strategy we used to measure neighborhood change. Our measurement of neighborhood change, however, focused on a single dimension of change - income. In this case, we’ll produce a multidimensional measure of change.\nAs we discussed in class, opportunity maps and other indexes are commonly used to be able to illustrate the distribution of resources across space and over time. These types of indexes are designed to be multidimensional, and therefore require a strong theoretical framework connecting concepts related to opportunity to indicators or measures designed to proxy or represent these concepts."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#introduction",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#introduction",
    "title": "Place Opportunity",
    "section": "",
    "text": "In this lab, we’ll extend our knowledge of opportunity maps, and more generally in standardizing and creating indexes from data. This strategy is in some ways an extension of the strategy we used to measure neighborhood change. Our measurement of neighborhood change, however, focused on a single dimension of change - income. In this case, we’ll produce a multidimensional measure of change.\nAs we discussed in class, opportunity maps and other indexes are commonly used to be able to illustrate the distribution of resources across space and over time. These types of indexes are designed to be multidimensional, and therefore require a strong theoretical framework connecting concepts related to opportunity to indicators or measures designed to proxy or represent these concepts."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#goals",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#goals",
    "title": "Place Opportunity",
    "section": "Goals",
    "text": "Goals\nThis lab introduces you to the following:\n\nCommon methods for standardizing and constructing indexes from demographic data\nBasic concepts associated with relational joins between datasets\nReinforces and provides a means of practicing data visualization and presentation"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#core-concepts",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#core-concepts",
    "title": "Place Opportunity",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nR and Rstudio\n\nacross()\nreduce()\nrowwise()\nscale()\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#github-lab-repository",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#github-lab-repository",
    "title": "Place Opportunity",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#download-and-prepare-data-for-opportunity-mapping",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#download-and-prepare-data-for-opportunity-mapping",
    "title": "Place Opportunity",
    "section": "Download and Prepare Data for Opportunity Mapping",
    "text": "Download and Prepare Data for Opportunity Mapping\nThe National Housing Conference brief which you read for Tuesday outlines several suggested data sources for opportunity mapping. For the purpose of learning some of the methods associated with opportunity mapping, we’ll work with three primary data sources:\n\nU.S. Census Bureau - American Community Survey\nHUD Location Affordability Index (version 3.0)\nEPA EJSCREEN\n\nYou should independently explore the documentation for these datasets and their indicators - there is a treasure trove of documentation to look at. When possible, download data documentation to your docuemtnation project folder.\nWe’ll divide these data into a few conceptual categories commonly seen in opportunity maps:\n\nEducation\nHousing Characteristics\nSocial Capital\nPublic Health and Safety\nEmployment and Workforce\nTransportation and Mobility\n\n\n\n\nCategory\nIndicator\nData Source\n\n\n\n\nEducation\nPopulation with a High School Diploma or greater\nACS\n\n\nEducation\nPopulation with a Bachelor’s Degree or greater\nACS\n\n\nHousing Characteristics\nMedian Home Value\nACS\n\n\nHousing Characteristics\nMedian Gross Rent\nHUD LAI\n\n\nHousing Characteristics\nPercentage Single Family Housing Units\nHUD LAI\n\n\nHousing Characteristics\nGross Rent as Percentage of Income\nACS\n\n\nHousing Characteristics\nHousing Cost Burden\nACS\n\n\nHousing Characteristics\nResidential Vacancy Rate\nACS\n\n\nSocial Capital\nPopulation Age 25 - 44\nACS\n\n\nSocial Capital\nMedian Household Income\nACS\n\n\nSocial Capital\nPercent of Households in Poverty\nACS\n\n\nSocial Capital\nPercentage of Owner-Occupied Housing Units\nACS\n\n\nEmployment and Worforce\nJob Density\nHUD LAI\n\n\nEmployment and Worforce\nRetail Density\nHUD LAI\n\n\nTransportation and Mobility\nMedian Commute Time\nHUD LAI\n\n\nTransportation and Mobility\nPublic Transit Use (Journey to work)\nHUD LAI\n\n\nPublic Health and Safety\nNATA Cancer Risk Index\nEPA EJSCREEN (2022)\n\n\nPublic Health and Safety\nNATA Respiratory Hazard Index\nEPA EJSCREEN (2022)\n\n\nPublic Health and Safety\nTraffic Proximity and Volume\nEPA EJSCREEN (2022)\n\n\nPublic Health and Safety\nParticulate Matter\nEPA EJSCREEN(2022)\n\n\n\nI have provided download links for the EJSCREEN and LAI data. We will use tidycensus to load ACS data.\n\nACS Data\nLet’s start by loading some ACS data:\n\n\nCode\nDL_Year&lt;-2020\nsurvey &lt;- \"acs5\"\nstate&lt;-c(\"NY\")\nsource(\"scripts/1_Get_ACS.R\")\n\n\nThis should look familiar - we have a script kept in the scripts folder with general code to download selected variables from the American Community Survey. Rather than placing this code in our notebook, we can use the source() command to run the script in its entirety. We are defining the variables (api_key, DL_Year, survey, state) in our notebook, so that these can be referenced when the script is running.\nWe have downloaded a lot of ACS data here at the census tract level for all tracts in Illinois:\n\n\n\nTable\nDescription\n\n\n\n\nB01001\nAge and Population\n\n\nB02001\nRace\n\n\nB03001\nEthnicity\n\n\nB05002\nForeign Born\n\n\nB11001\nFemale Headed Household\n\n\nB17001\nPoverty Rate\n\n\nB19013\nMedian Household Income\n\n\nB25002\nResidential Vacancy\n\n\nB25003\nHousing Tenure\n\n\nB25077\nMedian Home Value\n\n\nB25106\nHousing Cost Burden\n\n\n\nI went ahead and turned this raw data into some selected indicators:\n\n\n\n\n\n\n\n\nTable\nLabel\nDescription\n\n\n\n\nB01001\nunder18\nProportion of Population under 18\n\n\nB01001\nover65\nProportion of Population over 65\n\n\nB01001\nP_Female\nProportion of Population female\n\n\nB01001\nPop\nTotal Population size\n\n\nB02001\nPWhite\nProportion Population White\n\n\nB02001\nPBlack\nProportion Population Black\n\n\nB02001\nPAIAN\nProportion Population AIAN\n\n\nB02001\nPAsian\nProportion Population Asian\n\n\nB02001\nPNonwhite\nProportion Population Nonwhite\n\n\nB03001\nPLatino\nProportion Population Latino Ethnicity (Of all Races)\n\n\nB05002\nPForeignborn\nProportion Population Foreign Born\n\n\nB19013\nMHHI\nMedian Household Income\n\n\nB11001\nP_FHHH\nProportion Female Headed Households\n\n\nB17001\nPov\nProportion Households Below Poverty\n\n\nB25003\nP_Own\nProportion Owner Occupied Housing Units\n\n\nB25077\nMHV\nMedian Home Value\n\n\nB25106\nCostBurden\nHousing Cost Burden\n\n\nB25002\nRvac\nResidential Vacancy Rate\n\n\n\n\nJoin ACS Data\nNow that we have these individual tables downloaded, we need to combine these together into a single dataset. We’ve done this many times in the past using left_join() but only with respect to two datasets at a time. Let’s introduce an alternative strategy here, using the purrr package’s reduce() function combined with left_join().\nFirst, here’s the way we might accomplish this as we have in the past:\n\n\nCode\nacs_data&lt;-left_join(B01001, B02001, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B03001, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B05002, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B11001, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B17001, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B19013, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B25002, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B25003, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B25077, by=\"GEOID\")\nacs_data&lt;-left_join(acs_data, B25106, by=\"GEOID\")\n\n\nThis works just fine, but there’s a lot of repition of code here with so many data frames.\nNow let’s look at how we can implement the same thing using reduce():\n\n\nCode\nacs_data &lt;- list(B01001, B02001, B03001, B05002, B11001, B17001, B19013, B25002, B25003, B25077, B25106) |&gt; \n  reduce(left_join, by = \"GEOID\")\n\n\nIn this case, we create a list containing each of the data frames we want to join together (note that we don’t put these in quotes since they’re objects and not names). We then use the reduce() command and tell reduce that we want to perform a left join and also supply the common variable name.\nBut what is reduce() really doing here? If you look at it’s documentation (?reduce), you’ll see that:\nreduce() is an operation that combines elements of a vector into a single value. The combination is driven by .f, a binary function, and takes two values and returns a single value: Reducing f over 1:3 computes the values f(f(1,2), 3).\nSo if we are doing a left join, this is essentially doing the following:\n\n\nCode\nacs_data &lt;- left_join(df1, df2)\nacs_data &lt;- left_join(acs_data, df3)\n\n\nAnd on and on until reduce has iterated through all elements of the list. This helps us cut down on repetitious code.\nInspecting our handiwork, our acs_data table (for which we downloaded tract-level data for New York state has 5,411 observations and 19 variables:\n\n\nCode\nstr(acs_data)\n\n\ntibble [5,411 × 19] (S3: tbl_df/tbl/data.frame)\n $ GEOID       : chr [1:5411] \"36011040600\" \"36011040700\" \"36011040800\" \"36011040900\" ...\n $ under18     : num [1:5411] 0.183 0.187 0.192 0.218 0.2 ...\n $ over65      : num [1:5411] 0.234 0.216 0.182 0.18 0.165 ...\n $ Pop         : num [1:5411] 3400 3633 4668 3683 3088 ...\n $ P_Female    : num [1:5411] 0.504 0.518 0.419 0.476 0.456 ...\n $ PWhite      : num [1:5411] 0.945 0.978 0.841 0.982 0.94 ...\n $ PBlack      : num [1:5411] 0.037647 0 0.113325 0.000272 0 ...\n $ PAIAN       : num [1:5411] 0 0 0 0.000815 0.000324 ...\n $ PAsian      : num [1:5411] 0.00235 0 0.00771 0.00434 0.0068 ...\n $ PNonwhite   : num [1:5411] 0.0547 0.022 0.1594 0.0185 0.0599 ...\n $ PLatino     : num [1:5411] 0.01647 0.00798 0.02571 0.02607 0.04955 ...\n $ PForeignborn: num [1:5411] 0.0176 0.0242 0.0176 0.0141 0.0385 ...\n $ P_FHHH      : num [1:5411] 0.0931 0.0486 0.089 0.0989 0.0638 ...\n $ Pov         : num [1:5411] 0.0507 0.049 0.0991 0.158 0.087 ...\n $ MHHI        : num [1:5411] 84330 93493 64811 66711 73182 ...\n $ Rvac        : num [1:5411] 0.0898 0.1612 0.3182 0.176 0.246 ...\n $ P_Own       : num [1:5411] 0.917 0.935 0.828 0.872 0.836 ...\n $ MHV         : num [1:5411] 186500 174900 147100 142700 147600 ...\n $ CostBurden  : num [1:5411] 0.224 0.206 0.283 0.246 0.172 ...\n\n\nWhile we’re at it, let’s do a little cleaning up. We have a lot of data frames which are now consolidated into acs_data. We can delete other objects from our environment using rm()\n\n\nCode\nrm(B01001, B02001, B03001, B05002, B11001, B17001, B19013, B25002, B25003, B25077, B25106)\n\n\n\n\n\nLocation Affordability Index Data\nLet’s download the HUD Location Affordability Index data from here. We can then load it directly.\n\n\nCode\nLAI &lt;- read_csv(\"data_raw/Location_Affordability_Index_v.3.csv\")\n\n\nRows: 73763 Columns: 444\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (7): GEOID, STATE, COUNTY, TRACT, CNTY_FIPS, STUSAB, area_type\ndbl (437): OBJECTID, households, owner_occupied_hu, renter_occupied_hu, pct_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s go ahead and select the following variables (our variables of interest) and overwrite the initial file:\n\nGEOID\npct_transit_j2w\nmedian_gross_rent\npct_hu_1_detatched\njob_density_simple\nretail_density_simple\nmedian_commute\n\n\n\nCode\nLAI&lt;-LAI |&gt;  select(GEOID, \n                    STUSAB,\n                    pct_transit_j2w,\n                    median_gross_rent,\n                    pct_hu_1_detached,\n                    job_density_simple,\n                    retail_density_simple,\n                    median_commute\n                    )\n\n\n\n\nEJSCREEN Data\nWe can download the EJSCREEN data here. Unlike the LAI data which we could read directly from the .csv file, the EJSCREEN data will need to be downloaded and unzipped before it can be used.\nWe’ll use the read_csv() function from the readr package to load raw EJSCREEN data, located in the “data” folder. Load these into an object called “ejscreen”:\nInspect the data. Let’s select a subset of the variables in the data which we’ll work more with. Based upon our framework, we’ll select the following variables and overwrite the original ejscreen object with our selection:\n\nID\nACSTOTPOP\nCANCER\nRESP\nPTRAF\nPM25\n\n\n\nCode\nejscreen&lt;-ejscreen |&gt; \n  select(ID,\n        ACSTOTPOP,\n        CANCER,\n        RESP,\n        PTRAF,\n        PM25)"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#joining-the-three-datasets",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#joining-the-three-datasets",
    "title": "Place Opportunity",
    "section": "Joining the Three Datasets",
    "text": "Joining the Three Datasets\nOk - we’re getting somewhere, I promise! We now have three datasets - acs_data, which contains tract-level data for New York state, LAI which contains tract-level data for the entire US, and ejscreen which contains block group level data for the US. We’re so close to joining these together and indexing the data!\n\nCreating Tract Data from Block Groups\nOne thing, though - we need to convert our ejscreen block groups data into tracts. Block groups are geographic subdivisions of tracts. Fortunately for us, Census FIPS codes are hierarchical - the combined state-county-tract-blockgroup variable called “ID” in ejscreen contains the tract FIPS code: - State (2 characters) - County (3 characters) - Tract (6 characters) - Block Group (1 character)\nOk, this is all well and good - how do we use this knowledge to combine things together? We could simply average the block group values together to approximate a tract-level average. At the same time, block groups are likely to have different populations. Given this potential heterogeneity, let’s use the population weighted average of those block group characteristics to constitute our tract characteristics.\nTo aggregate our block group data, we need to extract the tract characters from the ID column, and then we can group_by() and summarise our data based upon the tract FIPS code. Base R fortunately has a function called substr() (substring) which can handle the extraction for us:\n\n\nCode\nejscreen&lt;-ejscreen |&gt; \n  mutate(GEOID = substr(ID, 0, 11))\n\n\nWhat happened here? We’re creating a new column called GEOID, and then invoking substr. We tell that function that we want a substring from the ID field, and then we say 0, 11, where 0 is the first position if we were counting from the left of the first character in the ID variable, and 11 is the last character that we want (remember, the combined FIPS code is 12 characters long, and we want all but the last character).\nAnd now we can use group_by() and summarise() to group together the data into census tract level aggregates. We can use weighted.mean() to calculate… weight for it… the weighted average (professor humor)! GEOID contains our combined state-county-tract FIPS code, ACSTOTPOP contains out population for the purpose of weighting, and we want to calculate averages for CANCER, RESP, PTRAF, and PM25. Figure out how to get that done.\nSneak preview - you’re going to hit an error - think about what rows might need to be removed to calculate a weighted average:\n\n\nCode\nejscreen&lt;-ejscreen |&gt;  \n  filter(ACSTOTPOP != 0, CANCER != \"None\") |&gt; \n  group_by(GEOID) |&gt;  \n  summarise(\n  CANCER = weighted.mean(as.numeric(CANCER), ACSTOTPOP),\n  RESP = weighted.mean(as.numeric(RESP), ACSTOTPOP),\n  PTRAF = weighted.mean(PTRAF, ACSTOTPOP),\n  PM25 = weighted.mean(as.numeric(PM25), ACSTOTPOP))\n\n\nOk - now we have around 73,000 tract-level observations for the EPA data - not too far off from what we have for the LAI data. Both of these datasets include observations for the entire US, while our ACS data only contains observations for New York State. Using your newfound knowledge of how to join data together based upon a common column, create a new object named dataset which contains combined ACS, ejscreen, and LAI data for Illinois census tracts:\n\n\nCode\ndataset&lt;- list(acs_data,  LAI, ejscreen) |&gt; reduce(left_join, by=\"GEOID\")\n\ndataset |&gt; \n  head() |&gt; \n  gt()\n\n\n\n\n\n\n  \n    \n    \n      GEOID\n      under18\n      over65\n      Pop\n      P_Female\n      PWhite\n      PBlack\n      PAIAN\n      PAsian\n      PNonwhite\n      PLatino\n      PForeignborn\n      P_FHHH\n      Pov\n      MHHI\n      Rvac\n      P_Own\n      MHV\n      CostBurden\n      STUSAB\n      pct_transit_j2w\n      median_gross_rent\n      pct_hu_1_detached\n      job_density_simple\n      retail_density_simple\n      median_commute\n      CANCER\n      RESP\n      PTRAF\n      PM25\n    \n  \n  \n    36011040600\n0.1829412\n0.2341176\n3400\n0.5044118\n0.9452941\n0.0376470588\n0.0000000000\n0.002352941\n0.05470588\n0.016470588\n0.01764706\n0.09307876\n0.05068672\n84330\n0.08979001\n0.9172633\n186500\n0.2235481\nNY\n0.7344093\n869\n92.82640\n0.071023683\n0.006993935\n16.00\n19.70603\n0.2238431\n76.199249\n6.211746\n    36011040700\n0.1871731\n0.2163501\n3633\n0.5183044\n0.9779796\n0.0000000000\n0.0000000000\n0.000000000\n0.02202037\n0.007982384\n0.02422241\n0.04855024\n0.04899532\n93493\n0.16119910\n0.9352664\n174900\n0.2063385\nNY\n0.0000000\n787\n94.71613\n0.084031686\n0.000897934\n13.95\n19.58912\n0.2248577\n8.835063\n6.179412\n    36011040800\n0.1919452\n0.1820908\n4668\n0.4190231\n0.8406170\n0.1133247644\n0.0000000000\n0.007712082\n0.15938303\n0.025706941\n0.01756641\n0.08900524\n0.09908915\n64811\n0.31816154\n0.8278796\n147100\n0.2827225\nNY\n0.1046997\n621\n75.63869\n0.021437169\n0.003085479\n20.63\n17.56237\n0.1972023\nNA\n6.092557\n    36011040900\n0.2180288\n0.1802878\n3683\n0.4759707\n0.9815368\n0.0002715178\n0.0008145534\n0.004344285\n0.01846321\n0.026065707\n0.01411892\n0.09890110\n0.15799510\n66711\n0.17600453\n0.8722527\n142700\n0.2458791\nNY\n0.1057314\n733\n70.25913\n0.004264647\n0.000176875\n18.12\n17.22243\n0.1934044\nNA\n6.108516\n    36011041001\n0.2001295\n0.1648316\n3088\n0.4562824\n0.9400907\n0.0000000000\n0.0003238342\n0.006800518\n0.05990933\n0.049546632\n0.03853627\n0.06375839\n0.08699902\n73182\n0.24604681\n0.8355705\n147600\n0.1719799\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n    36011041002\n0.1115152\n0.1921212\n1650\n0.4678788\n0.8909091\n0.0151515152\n0.0042424242\n0.013333333\n0.10909091\n0.065454545\n0.02424242\n0.04838710\n0.09794239\n79167\n0.35416667\n0.8440860\n169900\n0.1487455\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n  \n  \n  \n\n\n\n\nNow that we have our base dataset together, we can filter to our area of interest, in this case, the boroughs that make up New York City:\n\n\n\nFIPS Code\nCounty Name\nBorough Name\n\n\n\n\n36047\nKings County\nBrooklyn\n\n\n36005\nBronx County\nBronx\n\n\n36081\nQueens County\nQueens\n\n\n36085\nRichmond County\nStaten Island\n\n\n36061\nNew York County\nManhattan\n\n\n\n\n\nCode\ndataset &lt;- dataset |&gt; \n  filter(substr(GEOID, 0, 5) %in% c(\"36047\", \"36005\", \"36081\", \"36085\", \"36061\"))\n\n\nThe above code uses substr() to pull out the first 5 characters from the GEOID field (the combined state and county FIPS codes) and then matches them against the FIPS codes for the five boroughs of New York City."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#create-standardized-scores",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#create-standardized-scores",
    "title": "Place Opportunity",
    "section": "Create Standardized Scores",
    "text": "Create Standardized Scores\nRemembering back to your high school or college statistics class, a Z-score (or standardized score) can be be calculated by subtracting from a given observation the mean of all observations and then dividing by the standard deviation of all observations.\nRecall:\n\\(z = \\frac{x-\\mu}{\\sigma}\\) where: \\(x\\) is the individual observation we want to standardize \\(\\mu\\) is the population mean \\(\\sigma\\) is the population standard deviation\nFind the mean and standard deviation for a variable in our dataset and manually calculate a z-score (just for fun).\n\n\nCode\ndataset |&gt; \n  summarise(\n  under18_mean = mean(under18, na.rm=TRUE), \n  under18_sd = sd(under18, na.rm=TRUE),\n  z_under18 = (under18 - under18_mean)/under18_sd\n  )\n\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n# A tibble: 2,327 × 3\n   under18_mean under18_sd z_under18\n          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1        0.205     0.0773   0.960  \n 2        0.205     0.0773   1.98   \n 3        0.205     0.0773   0.572  \n 4        0.205     0.0773  -0.254  \n 5        0.205     0.0773   0.980  \n 6        0.205     0.0773   0.706  \n 7        0.205     0.0773  -0.253  \n 8        0.205     0.0773   0.865  \n 9        0.205     0.0773  -0.00949\n10        0.205     0.0773   0.539  \n# ℹ 2,317 more rows\n\n\nFor the first observation, the value for Under 18 is 27.9 percent. The average value for tracts in Illinois is 27.9 percent, and the standard deviation is 7.73. The standardized score of .960 indicates that the observation is .96 of a standard deviation above the mean. There’s absolutely nothing wrong with doing this manually, however, we can use the scale() function to do the same thing:\n\n\nCode\ndataset |&gt;  \n  mutate(under18 = scale(under18))\n\n\n# A tibble: 2,327 × 30\n   GEOID       under18[,1] over65   Pop P_Female PWhite PBlack   PAIAN PAsian\n   &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 36047009202     0.960   0.0747  3453    0.514 0.296  0.0568 0.00782 0.300 \n 2 36047009401     1.98    0.0763  2293    0.515 0.0754 0.0754 0       0.690 \n 3 36047009402     0.572   0.0885  2746    0.548 0.172  0.0138 0       0.695 \n 4 36047009600    -0.254   0.132   5858    0.570 0.362  0.0446 0.00751 0.340 \n 5 36047009800     0.980   0.0864  6021    0.498 0.153  0.0485 0       0.383 \n 6 36047010000     0.706   0.128   5978    0.541 0.264  0.0238 0.0110  0.423 \n 7 36047010100    -0.253   0.0842  3944    0.508 0.510  0.0418 0.0327  0.0903\n 8 36047010200     0.865   0.126   4844    0.515 0.124  0.0142 0       0.705 \n 9 36047010401    -0.00949 0.125   2152    0.487 0.106  0.0279 0       0.838 \n10 36047010402     0.539   0.148   2256    0.523 0.0638 0      0       0.879 \n# ℹ 2,317 more rows\n# ℹ 21 more variables: PNonwhite &lt;dbl&gt;, PLatino &lt;dbl&gt;, PForeignborn &lt;dbl&gt;,\n#   P_FHHH &lt;dbl&gt;, Pov &lt;dbl&gt;, MHHI &lt;dbl&gt;, Rvac &lt;dbl&gt;, P_Own &lt;dbl&gt;, MHV &lt;dbl&gt;,\n#   CostBurden &lt;dbl&gt;, STUSAB &lt;chr&gt;, pct_transit_j2w &lt;dbl&gt;,\n#   median_gross_rent &lt;dbl&gt;, pct_hu_1_detached &lt;dbl&gt;, job_density_simple &lt;dbl&gt;,\n#   retail_density_simple &lt;dbl&gt;, median_commute &lt;dbl&gt;, CANCER &lt;dbl&gt;,\n#   RESP &lt;dbl&gt;, PTRAF &lt;dbl&gt;, PM25 &lt;dbl&gt;\n\n\nThis brings up an important question - what should the reference geography be for our opportunity measures? We narrowed down our dataset to New York City, however, we could have just as easily used the state or the nation. It’s important to keep in mind that once we construct Z-scores for our indicators, they are with reference to the distribiotn of values we choose to include. We are using the city of New York City as our point of reference. Using the state as a point of reference may be relevant and useful, especially if we want to make some comparisons across the state. At the same time, the characteristics of New York City and other metropolitan areas are likely to be very different than the balance of the state, and so comparisons for the purpose of standardizing our data may be a bit distorted. Now that we have our reference region selected, we can move forward with standardizing each indicator value.\nBefore we do this, it can be useful to provide a more intuitive description of each variable (since most people do not think about indicators in standardized terms). You may recall us previously using commands like summary() for basic descriptives.\nWe’re going to use a variant of the summarise command which you’ve used in the past. summarise_at() allows us to select variables to summarise by their name, and will then apply the same function across all of those variables. Let’s find the mean for each of the indicator variables in our compiled dataset:\n\n\nCode\ndataset |&gt; \n  summarise_at(vars(under18:median_commute), mean, na.rm=TRUE)\n\n\n# A tibble: 1 × 25\n  under18 over65   Pop P_Female PWhite PBlack   PAIAN PAsian PNonwhite PLatino\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1   0.205  0.151 3601.    0.519  0.410  0.250 0.00443  0.148     0.590   0.271\n# ℹ 15 more variables: PForeignborn &lt;dbl&gt;, P_FHHH &lt;dbl&gt;, Pov &lt;dbl&gt;, MHHI &lt;dbl&gt;,\n#   Rvac &lt;dbl&gt;, P_Own &lt;dbl&gt;, MHV &lt;dbl&gt;, CostBurden &lt;dbl&gt;, STUSAB &lt;dbl&gt;,\n#   pct_transit_j2w &lt;dbl&gt;, median_gross_rent &lt;dbl&gt;, pct_hu_1_detached &lt;dbl&gt;,\n#   job_density_simple &lt;dbl&gt;, retail_density_simple &lt;dbl&gt;, median_commute &lt;dbl&gt;\n\n\nWe use vars() to specify the variables to summarize by their name, using the colon to specify through (e.g. under18 through median_commute). We specify that we want our summary statistic to be the mean, and we specify that we want to remove NAs so we get a usable statistic out. This gives us some usable statistics for the entire region. We might also want to derive summaries for each of the counties in the region - how could we do this using your knowledge of group_by(), substr(), and summarise_at()?\n\n\nCode\ndataset |&gt; \n  group_by(substr(GEOID, 0, 5)) |&gt; \n  summarise_at(vars(under18:median_commute), mean, na.rm=TRUE)\n\n\nWarning: There were 5 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `STUSAB = (function (x, ...) ...`.\nℹ In group 1: `substr(GEOID, 0, 5) = \"36005\"`.\nCaused by warning in `mean.default()`:\n! argument is not numeric or logical: returning NA\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\n\n# A tibble: 5 × 26\n  `substr(GEOID, 0, 5)` under18 over65   Pop P_Female PWhite PBlack   PAIAN\n  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 36005                   0.241  0.134 3953.    0.528  0.235  0.344 0.00712\n2 36047                   0.224  0.144 3201.    0.524  0.430  0.313 0.00320\n3 36061                   0.134  0.167 5255.    0.520  0.570  0.142 0.00342\n4 36081                   0.195  0.160 3132.    0.509  0.355  0.200 0.00528\n5 36085                   0.215  0.160 3775.    0.513  0.688  0.124 0.00231\n# ℹ 18 more variables: PAsian &lt;dbl&gt;, PNonwhite &lt;dbl&gt;, PLatino &lt;dbl&gt;,\n#   PForeignborn &lt;dbl&gt;, P_FHHH &lt;dbl&gt;, Pov &lt;dbl&gt;, MHHI &lt;dbl&gt;, Rvac &lt;dbl&gt;,\n#   P_Own &lt;dbl&gt;, MHV &lt;dbl&gt;, CostBurden &lt;dbl&gt;, STUSAB &lt;dbl&gt;,\n#   pct_transit_j2w &lt;dbl&gt;, median_gross_rent &lt;dbl&gt;, pct_hu_1_detached &lt;dbl&gt;,\n#   job_density_simple &lt;dbl&gt;, retail_density_simple &lt;dbl&gt;, median_commute &lt;dbl&gt;\n\n\nYou can start to see some of the differences that exist between distributions at the borough (county) level. Do any stick out to you at this point?\nOk, we have created a numeric summary table that may be useful for reporting out. Now we can produce our standardized scores which we can use for index making.\nNow that you have defined your region, described the data for your region, and know how to standardize the values, you can create a scaled version of your data. Earlier, we learned how to use summarise_at() to select variables to summarize using a specific function. There’s also a mutate_at() function which allows you to perform the same alteration upon all of the variables you select. Take a look at the documentation for mutate_at(). You’ll need to supply the variables you want to mutate, as well as the function (read closely the documentation and examples before proceeding).\nNow let’s go ahead and create a separate dataset called dataset_scaled which contains the scaled values (there may a few fields which either do not need scaling or who you can remove):\n\n\nCode\ndataset_scaled&lt;-dataset |&gt;  \n  mutate_at(vars(under18, over65, P_Female, PNonwhite, PForeignborn, P_FHHH, Pov, MHHI, P_Own, MHV, CostBurden, CANCER, RESP, PTRAF, PM25, pct_transit_j2w, median_gross_rent, pct_hu_1_detached, job_density_simple, retail_density_simple, median_commute), list(scale=scale))\n\n\nWe are in the home stretch! We are very close to being able to make our index. We need to determine how we think each variable is related to opportunity. In some cases, higher values are “good” (more favorable), and in some cases, lower values are “good”. We need to make sure that those values are all moving in the same direction so that when we combine them they do not counter-act each other.\nBelow is a table proposing potential directions for each variable.\n\nNegative: For a variable labeled Negative, higher values are likely to indicate lower levels of opportunity (less favorable).\nPositive: Far a variable labeled Positive, higher values are likely to indicate higher levels of opportunity (more favorable).\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nRelationship to Opportunity\nCategory\n\n\n\n\nunder18\nProportion of Population under 18\nNegative\nDemographic Structure\n\n\nover65\nProportion of Population over 65\nNegative\nDemographic Structure\n\n\nPNonwhite\nProportion Population Nonwhite\nNegative\nDemographic Structure\n\n\nPForeignborn\nProportion Population Foreign Born\nNegative\nDemographic Structure\n\n\nMHHI\nMedian Household Income\nPositive\nEmployment and Economy\n\n\nP_FHHH\nProportion Female Headed Households\nNegative\nDemographic Structure\n\n\nPov\nProportion Households Below Poverty\nNegative\nDemographic Structure\n\n\nP_Own\nProportion Owner Occupied Housing Units\nPositive\nHousing\n\n\nMHV\nMedian Home Value\nPositive\nHousing\n\n\nCostBurden\nHousing Cost Burden\nNegative\nHousing\n\n\nCANCER\nCancer Risk Index\nNegative\nEnvironmental Health\n\n\nRESP\nRespiratory Hazard Index\nNegative\nEnvironmental Health\n\n\nPTRAF\nTraffic Proximity Index\nNegative\nEnvironmental Health\n\n\nPM25\nParticulate Matter Index\nNegative\nEnvironmental Health\n\n\npct_transit_j2w\nCommuting by Public Transportation\nPositive\nTransportation\n\n\nmedian_gross_rent\nMedian Gross Rent\nNegative\nHousing\n\n\npct_hu_1_detatched\nDetatched Housing Units\nPositive\nHousing\n\n\njob_density_simple\nJob Density\nPositive\nEmployment and Economy\n\n\nretail_density_simple\nRetail Density\nPositive\nEmployment and Economy\n\n\nmedian_commute\nMedian Commute Time\nNegative\nTransportation\n\n\n\nWe are going to create an **additive* index, where standardized values are simply added together. We can transform values so that they are moving in the same direction by simply switching the sign on values that need to be reversed (e.g. multiply by -1). Based upon the above table, Here’s what that would look like:\n\n\nCode\ndataset_scaled&lt;-dataset_scaled |&gt;  mutate(\n  under18_scale = under18_scale*-1,\n  over65_scale = over65_scale*-1,\n  PNonwhite_scale = PNonwhite_scale*-1,\n  PForeignborn_scale = PForeignborn_scale*-1,\n  P_FHHH_scale = P_FHHH_scale*-1,\n  Pov_scale = Pov_scale*-1,\n  CostBurden_scale = CostBurden_scale *-1,\n  CANCER_scale = CANCER_scale*-1,\n  RESP_scale = RESP_scale*-1,\n  PTRAF_scale = PTRAF_scale*-1,\n  PM25_scale = PM25_scale*-1,\n  median_gross_rent_scale = median_gross_rent_scale*-1,\n  median_commute_scale = median_commute_scale*-1\n)\n\n\nBy making these changes, once we add up our indicator values, we are essentially saying that larger values are indicative of greater opportunity and smaller values are indicative of less opportunity. Let’s do two things here - we can add up all of our values to get an overall opportunity score, but let’s also create sub-indicators for each category, as this may be useful information for us to observe.\nTo do this, add several new variables to dataset_scaled that sum together each of the index measures by category, and then create a separate indicator that sums up those category columns into an overall opportunity measure:\nUse the following labels:\n\ndem_index = Demographic Structure\nemp_index = Employment and Economy\nhou_index = Housing\nenv_index = Environmental Health\ntra_index = Transportation\ntot_index = Combined Total Index\n\n\n\nCode\ndataset_scaled&lt;-dataset_scaled |&gt; \n  rowwise() |&gt; \n  mutate(\n  dem_index = sum(under18_scale, over65_scale, PNonwhite_scale, PForeignborn_scale, P_FHHH_scale, Pov_scale, na.rm=TRUE),\n  emp_index = sum(MHHI_scale, job_density_simple_scale, retail_density_simple_scale, na.rm=TRUE),\n  hou_index = sum(P_Own_scale, MHV_scale, CostBurden_scale, median_gross_rent_scale, pct_hu_1_detached_scale, na.rm=TRUE),\n  env_index = sum(CANCER_scale, RESP_scale, PTRAF_scale, PM25_scale, na.rm=TRUE),\n  tra_index = sum(pct_transit_j2w_scale, median_commute_scale,na.rm=TRUE),\n  tot_index = dem_index + emp_index + hou_index + env_index + tra_index\n) \n\n\nSomething’s a little different here that makes a big difference. Did you notice rowwise()? Typically, if we were to ask dplyr to mutate by providing a sum, it would do so by column. rowwise() modifies this and asks for something to happen across a data observation (row) instead of by column. Therefore, when we ask in a mutate statement for the sum of values, we are asking R to add up the values in the same row and then insert the product into a new variable (in this case our named indexes). Pretty cool, right?\nOk - now we have subindex values as well as a total index value. We could analyze these and interpret them as is, but there’s one more thing that we might do to make these easier to interpret. Let’s quickly look at some descriptive stats for our index values and think about what may be challenging for us to interpret:\n\n\nCode\ndataset_scaled |&gt; \n  select(dem_index, emp_index, hou_index, env_index, tra_index, tot_index) |&gt; \n  summary()\n\n\n   dem_index          emp_index         hou_index         env_index      \n Min.   :-10.7759   Min.   :-2.4750   Min.   :-7.3245   Min.   :-14.084  \n 1st Qu.: -2.1811   1st Qu.:-0.8973   1st Qu.:-1.5020   1st Qu.: -1.639  \n Median : -0.1554   Median :-0.3840   Median :-0.1535   Median :  0.000  \n Mean   :  0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   :  0.000  \n 3rd Qu.:  2.1316   3rd Qu.: 0.1484   3rd Qu.: 1.1933   3rd Qu.:  1.443  \n Max.   : 11.8808   Max.   :31.0470   Max.   :10.5857   Max.   :  9.302  \n   tra_index         tot_index      \n Min.   :-6.7759   Min.   :-19.994  \n 1st Qu.:-0.8376   1st Qu.: -3.762  \n Median : 0.0000   Median :  0.000  \n Mean   : 0.0000   Mean   :  0.000  \n 3rd Qu.: 1.1383   3rd Qu.:  3.605  \n Max.   : 3.3514   Max.   : 35.604  \n\n\nLooking at this, there are a few challenges:\n\nSince each subindex component consists of between two and six values, they end up having very different scales. This means that we can’t interpret anything about the magnitude by comparing these values to each other.\nWe have both positive and negative index values. Negative values don’t necessarily mean anything other than that the sum of index values is negative.\n\nOne strategy for making these values more interpretable is to rescale them. Do not confuse rescaling with the type of standardizing which we previously performed by converting our indicator values into z scores. Rescaling will take the data from an existing range and convert it to a new range of values.\nRescaling converts our data from its existing range of values to a new range of values, preserving the magnitude of difference that exists between the values:\nIf we want to convert our data to the range [0,1] here’s what we’d do: \\(x_{rescaled} = \\frac{x-min(x)}{max(x)-min(x)}\\)\nWe can also convert our data to an arbitrary range. Let’s try 0,100: \\(x_{rescaled} = a+\\frac{x-min(x)(b-a)}{max(x) -min(x)}\\)\nFortunately, R has us covered here too - the rescale function in the scales package will rescale to whatever range we wish:\n\n\nCode\nlibrary(scales)\n\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\ndataset_scaled &lt;-dataset_scaled |&gt; \n  ungroup() |&gt; \n  mutate(dem_index = rescale(dem_index, to = c(0,100)),\n         emp_index = rescale(emp_index, to = c(0,100)),\n         hou_index = rescale(hou_index, to = c(0,100)),\n         env_index = rescale(env_index, to = c(0,100)),\n         tra_index = rescale(tra_index, to = c(0,100)),\n         tot_index = rescale(tot_index, to = c(0,100))\n  )\n\ndataset_scaled |&gt;  \n  select(dem_index, emp_index, hou_index, env_index, tra_index, tot_index) |&gt; \n  summary()\n\n\n   dem_index        emp_index         hou_index        env_index     \n Min.   :  0.00   Min.   :  0.000   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.: 37.93   1st Qu.:  4.707   1st Qu.: 32.51   1st Qu.: 53.22  \n Median : 46.88   Median :  6.238   Median : 40.04   Median : 60.23  \n Mean   : 47.56   Mean   :  7.383   Mean   : 40.90   Mean   : 60.23  \n 3rd Qu.: 56.97   3rd Qu.:  7.826   3rd Qu.: 47.56   3rd Qu.: 66.40  \n Max.   :100.00   Max.   :100.000   Max.   :100.00   Max.   :100.00  \n   tra_index        tot_index     \n Min.   :  0.00   Min.   :  0.00  \n 1st Qu.: 58.64   1st Qu.: 29.20  \n Median : 66.91   Median : 35.96  \n Mean   : 66.91   Mean   : 35.96  \n 3rd Qu.: 78.15   3rd Qu.: 42.45  \n Max.   :100.00   Max.   :100.00  \n\n\nThe values are all rescaled so that the minimum is now zero and the maximum value is 100. We use ungroup() here so that if we have used group_by() in the past, R ignores those past groupings when we rescale. Looking at the descriptive statistics for these variables, there’s still some skew in many cases. We might want to try visualizing these distributions so that we can better understand their implications."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#demographics-by-opportunity-levels",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#demographics-by-opportunity-levels",
    "title": "Place Opportunity",
    "section": "Demographics by Opportunity Levels",
    "text": "Demographics by Opportunity Levels\nLet’s take a look at our overall opportunity index again:\n\n\nCode\nggplot(data=dataset_scaled, aes(x=tot_index))+geom_histogram(bins=100)+\n  labs(title = \"Opportunity Index: Histogram\", x=\"Opportunity Index\", y=\"Count\")+\n  theme_classic()\n\n\n\n\n\nHow might we start to think about some of the demographic correlates of who lives in places with higher aggregate opportunity and lower aggregate opportunity? You’ll recall from when we initially created our index that we had downloaded some information on race and ethnicity which was not incorporated into our opportunity measures. We might be interested to see how our composite measure of low and high opportunity are related to the racial and ethnic composition of the census tracts they are from.\nWe might turn towards a some bivariate visualizations to help us here. Let’s start with a simple scatterplot:\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PWhite, y =tot_index))+geom_point()\n\n\n\n\n\nHow would you describe the relationship visualized here (note that I shifted the index values to the y scale and placed percent white on the x scale)? To make it easier, let’s add another geometry to the same plot, in this case, by fitting a line to our data:\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PWhite, y =tot_index))+geom_point()+geom_smooth(method = \"lm\")\n\n\n\n\n\nHow would you describe the relationship between the tract-level concentration of the white population and the overall index score?\nWe might want to polish things up a little more - here are some suggested additions to think about.\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PWhite, y =tot_index))+\n  geom_point(alpha = .6, cex = .75)+ # Use alpha to control the transparency of the points and cex to control their size\n  geom_smooth(method = \"lm\")+\n  labs(x=\"White (%)\", y = \"Combined Opportunity Index\")+ # Add labels to the x and y axes\n  scale_x_continuous(labels = scales::percent)+ # Use the scales package to convert X-axis to percentages\n  theme_minimal() #Use themes to eliminate extra shading on plots\n\n\n\n\n\nYour turn - produce similar plots for the Black, Asian, and Latino populations:\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PBlack, y =tot_index))+geom_point()+geom_smooth(method = \"lm\")\n\n\n\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PAsian, y =tot_index))+geom_point()+geom_smooth(method = \"lm\")\n\n\n\n\n\nCode\nggplot(data=dataset_scaled, aes(x=PLatino, y =tot_index))+geom_point()+geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#extending-your-analysis",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#extending-your-analysis",
    "title": "Place Opportunity",
    "section": "Extending your Analysis",
    "text": "Extending your Analysis\nThe most standard next step in opportunity mapping is…\n\nMaking a map. Drawing from your index data, make a map of your overall index score for your place or region, as well as for index sub-components. What spatial patterns emerge? Map out key demographic information - how do these seem to be related to your overall measures of opportunity?\nCreate summary visualizations and tables that display where the highest and lowest levels of opportunity are within the region? Who lives in these places?\nConsider connecting your opportunity indexes to other data (examples might include public health statistics, transportation costs, presence of public or subsidized housing, etc.) How does a multidimensional opportunity index help you tell stories about these places?\nCreate a journalistic narrative about opportunity in your place or for another region. What makes your story powerful, and what story is it telling?"
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#lab-evaluation",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#lab-evaluation",
    "title": "Place Opportunity",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nIn evaluating your lab submission, we’ll be paying attention to the following:\n\nA demonstrated conceptual understanding of the opportunity concept and the way it becomes operationalized with quantitative demographic data.\nCorrect re-deployment of the code based used in this lab for your own place or region.\nA clear narrative analysis of opportunity for your place.\nProper formatting of tables, figures, and visualizations.\n\nAs you get into the lab, please feel welcome to ask us questions, and please share where you’re struggling with us and with others in the class."
  },
  {
    "objectID": "assignments/labs/08_place_opportunity/08_placeopportunity.html#references",
    "href": "assignments/labs/08_place_opportunity/08_placeopportunity.html#references",
    "title": "Place Opportunity",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "assignments/labs/10_health_equity/10_health_equity.html",
    "href": "assignments/labs/10_health_equity/10_health_equity.html",
    "title": "Health Equity",
    "section": "",
    "text": "A growing public conversation recognizes that individual health and collective health of populations is determined by a range of factors, including many that are social and rooted in social relationships. As this conversation grows and evolves, planners have an important role to play helping to disentangle and describe how places matter and influence the health of populations.\nThis week, we will explore a case study strategy for linking social determinants measures rooted in neighborhood indicators with measures of health outcomes. Rather than focus on building out new methodological or analytical techniques, our focus will be on storytelling and in thinking about how to engage communities in further conversations about their health and wellbeing."
  },
  {
    "objectID": "assignments/labs/10_health_equity/10_health_equity.html#introduction",
    "href": "assignments/labs/10_health_equity/10_health_equity.html#introduction",
    "title": "Health Equity",
    "section": "",
    "text": "A growing public conversation recognizes that individual health and collective health of populations is determined by a range of factors, including many that are social and rooted in social relationships. As this conversation grows and evolves, planners have an important role to play helping to disentangle and describe how places matter and influence the health of populations.\nThis week, we will explore a case study strategy for linking social determinants measures rooted in neighborhood indicators with measures of health outcomes. Rather than focus on building out new methodological or analytical techniques, our focus will be on storytelling and in thinking about how to engage communities in further conversations about their health and wellbeing."
  },
  {
    "objectID": "assignments/labs/10_health_equity/10_health_equity.html#goals",
    "href": "assignments/labs/10_health_equity/10_health_equity.html#goals",
    "title": "Health Equity",
    "section": "Goals",
    "text": "Goals\n\nThink critically about how to translate conceptual arguments and data into coherent stories.\n\nLet’s get going…"
  },
  {
    "objectID": "assignments/labs/10_health_equity/10_health_equity.html#github-lab-repository",
    "href": "assignments/labs/10_health_equity/10_health_equity.html#github-lab-repository",
    "title": "Health Equity",
    "section": "Github Lab Repository",
    "text": "Github Lab Repository\nIf you have not already done so, follow this link to accept the lab Github Classroom assignment repository."
  },
  {
    "objectID": "assignments/labs/10_health_equity/10_health_equity.html#conceptualizing-structural-racism-at-the-neighborhood-level",
    "href": "assignments/labs/10_health_equity/10_health_equity.html#conceptualizing-structural-racism-at-the-neighborhood-level",
    "title": "Health Equity",
    "section": "Conceptualizing Structural Racism at the Neighborhood Level",
    "text": "Conceptualizing Structural Racism at the Neighborhood Level\nReading Dougherty, et al. (2020), we get a very nice and compact understanding of how a complex theoretical concept like structural racism has been measured in the past, and how specifically these measures have been applied to understanding outcomes related to health:\n\nStructural racism is frequently thought of as a construct involving discriminatory practices or racial inequities in multiple domains, often including housing, education, health, criminal justice, and employment. To date, research on adiposity has focused on individual components of structural racism rather than on measures that account for the complex, multidimensional nature of structural racism.\n\nThe paper then goes on to itemize five domains of structural racism measured in the literature:\n\nHousing\nEducation\nEmployment\nHealth Care\nCriminal Justice\n\nPart of the argument being made here is that while other research may have focused on one of these domains or a single measure proxying structural racism that the approach adopted by the authors involves thinking about structural racism as being multidimensional in nature with multiple overlapping domains that need to be measured at the same time.\nAt this point in our class, this observation and approach should sound pretty familiar - we’ve learned how to construct multidimensional measures of neighborhood conditions in some of our prior labs, just with other conceptual frameworks and measures proxying things other than structural racism.\nKeeping our approach to these other neighborhood-level indexes will be important for thinking about how we might approach creating a tract-level multidimensional index of structural racism. Let’s think a bit about how we might adapt Dougherty’s methods to operationalize structural racism at the neighborhood level."
  },
  {
    "objectID": "assignments/labs/10_health_equity/10_health_equity.html#operationalizing-structural-racism-at-the-neighborhrood-level",
    "href": "assignments/labs/10_health_equity/10_health_equity.html#operationalizing-structural-racism-at-the-neighborhrood-level",
    "title": "Health Equity",
    "section": "Operationalizing Structural Racism at the Neighborhrood Level",
    "text": "Operationalizing Structural Racism at the Neighborhrood Level\nThe Dougherty Appendix document describes how a range of candidate indicators were selected to proxy five conceptual domains of structural racism and then confirmatory factor analysis was used to identify the strongest indicators across each domain:\n\nWe can turn to the methods section of the Dougherty paper to think more about how to operationalize structural racism at the neighborhood level:\n\nMultiple indicators of differential treatment by race were evaluated, with modeling diagnostics guiding selection of the most informative indicators. These indicators with few exceptions, are prevalence ratios… Derivation of the CSR scale relied primarily on U.S. Census Bureau survey data and community-level data collected for administrative reasons.\n\nIn this passage, the authors describe prevalence ratios as follows: \\[P_{w=1} / P_{b=1}\\] where \\[P_{w=1}\\] is the proportion of whites in a county experiencing an event and \\[P_{b=1}\\] is the proportion of blacks in a county experiencing the event.\nWhile the authors focus on calculating these ratios across five domains at the county level, we will think about how to construct several domains at the census tract level to proxy neighborhood-level structural racism.\nTo keep things simple for the purpose of our lab, we are going to focus on calculating a few measures from ACS data only. Dougherty relies upon a range of other administrative data sources, and if we were going to develop a fully-functioning neighborhood structural racism scale, we would need to think about developing appropriate proxies for all dimensions at the neighborhood level.\nWe will focus on translating proxies for housing, education, employment to the tract level.\n\nHousing\nDougherty’s housing measures tend to focus on measuring dimensions of segregation and spatial unevenness at the county level. While we might be able to construct similar measures of segregation at the tract level, we will instead employ a simple measure of differences in housing tenure (owned versus rented) by race.\nACS Table B25003A and B25003B contain data on tenure for white and black heads of household. We will use these to calculate prevalence ratios:\n\n\nCode\nacs_vars &lt;- c(\"B25003H_001\", \"B25003H_002\", \"B25003B_001\", \"B25003B_002\")\n\npr_own &lt;- get_acs(geography = \"tract\", variables = acs_vars, state = \"MI\", output = \"wide\", year = 2021) |&gt; \n  mutate(\n    B25003H_001E = ifelse(B25003H_001E == 0, .01, B25003H_001E),\n    B25003H_002E = ifelse(B25003H_002E == 0, .01, B25003H_002E),\n        B25003B_001E = ifelse(B25003B_001E == 0, .01, B25003B_001E),\n    B25003B_002E = ifelse(B25003B_002E == 0, .01, B25003B_002E),\n    pr_housing = (B25003H_002E/B25003H_001E)/(B25003B_002E/B25003B_001E)) |&gt; \n  select(GEOID, pr_housing)\n\n\nGetting data from the 2017-2021 5-year ACS\n\n\n\n\nEducation\nFor education, let’s look at the proportion of adults over age 25 who have not completed a high school education (or equivalent). As with our prior measures, we’ll calculate the prevalence ratio of white to black adults with no high school degree using data from tables C15002H and C15002B.\n\n\nCode\nacs_vars &lt;- c(\n\"C15002H_001\", \n\"C15002H_003\",\n\"C15002H_008\",\n\"C15002B_001\",\n\"C15002B_003\",\n\"C15002B_008\"\n)\n\npr_ed &lt;- get_acs(geography = \"tract\", variables = acs_vars, state = \"MI\", output = \"wide\", year = 2021) |&gt; \n  mutate(\nC15002H_001E = ifelse(C15002H_001E == 0, .01, C15002H_001E), \nC15002H_003E = ifelse(C15002H_003E == 0, .01, C15002H_003E) ,\nC15002H_008E = ifelse(C15002H_008E == 0, .01, C15002H_008E),\nC15002B_001E = ifelse(C15002B_001E == 0, .01, C15002B_001E),\nC15002B_003E = ifelse(C15002B_003E == 0, .01, C15002B_003E),\nC15002B_008E = ifelse(C15002B_008E == 0, .01, C15002B_008E),\n    pr_ed = ((C15002H_003E+C15002H_008E)/C15002H_001E)/(C15002B_003E+C15002B_008E)/C15002B_001E) |&gt; \n  select(GEOID, pr_ed)\n\n\nGetting data from the 2017-2021 5-year ACS\n\n\n\n\nEmployment\nAs a proxy for employment, we’ll calculate the prevalence ratio for white and black poverty drawing from tables B17001H and B17001B\n\n\nCode\nacs_vars &lt;- c(\n  \"B17001H_001\",\n  \"B17001H_002\",\n  \"B17001B_001\",\n  \"B17001B_002\"\n)\n\npr_pov &lt;- get_acs(geography = \"tract\", variables = acs_vars, state = \"MI\", output = \"wide\", year = 2021) |&gt; \n  mutate(\n    B17001H_001E = ifelse(B17001H_001E == 0, .01, B17001H_001E),\n    B17001H_002E = ifelse(B17001H_002E == 0, .01, B17001H_002E),\n    B17001B_001E = ifelse(B17001B_001E == 0, .01, B17001B_001E),\n    B17001B_002E = ifelse(B17001B_002E == 0, .01, B17001B_002E),\n    pr_pov = ((B17001H_002E/B17001H_001E)/(B17001B_002E/B17001B_001E))\n  ) |&gt; \n  select(GEOID, pr_pov)\n\n\nGetting data from the 2017-2021 5-year ACS\n\n\n\n\nJoining Together Census Data\nWith these three measures in hand, we can joini together our three prevalence ratios:\n\n\nCode\ndataset &lt;- list(pr_own, pr_ed, pr_pov) |&gt; reduce(left_join, by=\"GEOID\")\n\n\nLet’s next pull out data for the detroit region:\n\n\nCode\ndataset &lt;- dataset |&gt; \n  mutate(\n  MSA_Flag = case_when(substr(GEOID, 3, 5) %in% c(\"087\", \"093\", \"147\", \"099\", \"125\", \"163\") ~ 1, TRUE ~ 0),\n  Tri_County_Flag = case_when(substr(GEOID, 3, 5) %in% c(\"099\", \"125\", \"163\") ~ 1, TRUE ~ 0))\n\ndetroit &lt;- dataset |&gt; filter(MSA_Flag == 1)\n\n\nNow that we have specific data on three tract-level measures of structural racism for the Detroit metropolitan area, let’s connect these data to information on health outcomes."
  },
  {
    "objectID": "assignments/labs/10_health_equity/10_health_equity.html#identifying-a-connection-to-health-outcomes",
    "href": "assignments/labs/10_health_equity/10_health_equity.html#identifying-a-connection-to-health-outcomes",
    "title": "Health Equity",
    "section": "Identifying a Connection to Health Outcomes",
    "text": "Identifying a Connection to Health Outcomes\nWe are going to draw information on health outcomes from the U.S. Center for Disease Control’s PLACES dataset. Specifically, we’ll use the 2022 PLACES tract-level dataset. Let’s download and prepare those data for joining to our tract-level prevalence ratios:\n\n\nCode\ncdc_places &lt;- read_csv(\"https://chronicdata.cdc.gov/api/views/cwsq-ngmh/rows.csv\")\n\n\nRows: 2555113 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): StateAbbr, StateDesc, CountyName, CountyFIPS, LocationName, DataSo...\ndbl  (5): Year, Data_Value, Low_Confidence_Limit, High_Confidence_Limit, Tot...\nlgl  (2): Data_Value_Footnote_Symbol, Data_Value_Footnote\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ncdc_places1 &lt;- cdc_places |&gt; pivot_wider(\n  id_cols = LocationName,\n  names_from = MeasureId, values_from = Data_Value)\n\n\nWhat I did here was to download these data as a .csv directly from the PLACES website into our R session using read_csv(). Because the data are in a tidy format, I use pivot_wider() to convert from one row per observation to one row per census tract for all indicators.\nThe dataset also has fairly good definitions present for each of the health indicators. Here’s a table summarizing them for you (this will be important as you get into the work of this lab):\n\n\nCode\ncdc_places |&gt; select(MeasureId, Measure, Data_Value_Type, Data_Value_Unit) |&gt; distinct() |&gt; gt()\n\n\n\n\n\n\n  \n    \n    \n      MeasureId\n      Measure\n      Data_Value_Type\n      Data_Value_Unit\n    \n  \n  \n    OBESITY\nObesity among adults aged &gt;=18 years\nCrude prevalence\n%\n    STROKE\nStroke among adults aged &gt;=18 years\nCrude prevalence\n%\n    ARTHRITIS\nArthritis among adults aged &gt;=18 years\nCrude prevalence\n%\n    MAMMOUSE\nMammography use among women aged 50-74 years\nCrude prevalence\n%\n    DEPRESSION\nDepression among adults aged &gt;=18 years\nCrude prevalence\n%\n    VISION\nVision disability among adults aged &gt;=18 years\nCrude prevalence\n%\n    COPD\nChronic obstructive pulmonary disease among adults aged &gt;=18 years\nCrude prevalence\n%\n    DENTAL\nVisits to dentist or dental clinic among adults aged &gt;=18 years\nCrude prevalence\n%\n    HIGHCHOL\nHigh cholesterol among adults aged &gt;=18 years who have been screened in the past 5 years\nCrude prevalence\n%\n    CSMOKING\nCurrent smoking among adults aged &gt;=18 years\nCrude prevalence\n%\n    MOBILITY\nMobility disability among adults aged &gt;=18 years\nCrude prevalence\n%\n    BPMED\nTaking medicine for high blood pressure control among adults aged &gt;=18 years with high blood pressure\nCrude prevalence\n%\n    SELFCARE\nSelf-care disability among adults aged &gt;=18 years\nCrude prevalence\n%\n    CHOLSCREEN\nCholesterol screening among adults aged &gt;=18 years\nCrude prevalence\n%\n    DISABILITY\nAny disability among adults aged &gt;=18 years\nCrude prevalence\n%\n    COLON_SCREEN\nFecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years\nCrude prevalence\n%\n    TEETHLOST\nAll teeth lost among adults aged &gt;=65 years\nCrude prevalence\n%\n    LPA\nNo leisure-time physical activity among adults aged &gt;=18 years\nCrude prevalence\n%\n    COREW\nOlder adult women aged &gt;=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening, and Mammogram past 2 years\nCrude prevalence\n%\n    SLEEP\nSleeping less than 7 hours among adults aged &gt;=18 years\nCrude prevalence\n%\n    INDEPLIVE\nIndependent living disability among adults aged &gt;=18 years\nCrude prevalence\n%\n    ACCESS2\nCurrent lack of health insurance among adults aged 18-64 years\nCrude prevalence\n%\n    CANCER\nCancer (excluding skin cancer) among adults aged &gt;=18 years\nCrude prevalence\n%\n    KIDNEY\nChronic kidney disease among adults aged &gt;=18 years\nCrude prevalence\n%\n    BPHIGH\nHigh blood pressure among adults aged &gt;=18 years\nCrude prevalence\n%\n    MHLTH\nMental health not good for &gt;=14 days among adults aged &gt;=18 years\nCrude prevalence\n%\n    CERVICAL\nCervical cancer screening among adult women aged 21-65 years\nCrude prevalence\n%\n    BINGE\nBinge drinking among adults aged &gt;=18 years\nCrude prevalence\n%\n    GHLTH\nFair or poor self-rated health status among adults aged &gt;=18 years\nCrude prevalence\n%\n    CASTHMA\nCurrent asthma among adults aged &gt;=18 years\nCrude prevalence\n%\n    CHECKUP\nVisits to doctor for routine checkup within the past year among adults aged &gt;=18 years\nCrude prevalence\n%\n    COGNITION\nCognitive disability among adults ages &gt;=18 years\nCrude prevalence\n%\n    HEARING\nHearing disability among adults aged &gt;=18 years\nCrude prevalence\n%\n    DIABETES\nDiagnosed diabetes among adults aged &gt;=18 years\nCrude prevalence\n%\n    COREM\nOlder adult men aged &gt;=65 years who are up to date on a core set of clinical preventive services: Flu shot past year, PPV shot ever, Colorectal cancer screening\nCrude prevalence\n%\n    PHLTH\nPhysical health not good for &gt;=14 days among adults aged &gt;=18 years\nCrude prevalence\n%\n    CHD\nCoronary heart disease among adults aged &gt;=18 years\nCrude prevalence\n%\n  \n  \n  \n\n\n\n\nYou can also find definitions by category from the CDC here.\nFinally, we’ll join to our detroit data so that we have a dataset consisting of data for the Detroit metro area that includes our prevalence measures as well as health behaviors and outcomes as proxied by the CDC data.\n\n\nCode\ndetroit &lt;- left_join(detroit, cdc_places1, by= c(\"GEOID\" = \"LocationName\"))\n\n\n\nCategorizing Structural Racism by Tracts\nWe are getting close to being able to do some storytelling! In order to make it easier to tell stories from our Detroit metro dataset, let’s categorize census tracts based upon the prevalence ratios related to housing, education, and employment. To keep things simple, we’ll find the percentile rank associated with each observation, and then label the values below or equal to 25% “Low”, the values between 25% and 75% “Moderate”, and the values above 75% “High”. Keep in mind that we haven’t scaled or reversed our prevalence ratios - this will be important when you attempt to interpret what Low, Moderate, or High ratios might mean.\n\n\nCode\ndetroit &lt;-detroit |&gt; \n  mutate(\n    percentile_housing = percent_rank(pr_housing),\n    percentile_education = percent_rank(pr_ed),\n    percentile_poverty = percent_rank(pr_pov),\n    cat_housing = case_when(\n      percentile_housing &lt; .25 ~ \"Low\",\n      percentile_housing &gt; .75 ~ \"High\",\n      .default = \"Moderate\"),\n    cat_education = case_when(\n      percentile_education &lt; .25 ~ \"Low\",\n      percentile_education &gt; .75 ~ \"High\",\n      .default = \"Moderate\"),\n    cat_poverty = case_when(\n      percentile_poverty &lt; .25 ~ \"Low\",\n      percentile_poverty &gt; .75 ~ \"High\",\n      .default = \"Moderate\"))"
  },
  {
    "objectID": "assignments/labs/10_health_equity/10_health_equity.html#lab-tasks",
    "href": "assignments/labs/10_health_equity/10_health_equity.html#lab-tasks",
    "title": "Health Equity",
    "section": "Lab Tasks",
    "text": "Lab Tasks\nIn our class session, split into three groups. Each group will focus on one dimension of structural racism (housing, education, employment). Working in your group, explore the relationship between your structural racism domain and the CDC health outcomes.\nTo get you started, I’ve prepared a summary table for each domain. Recall that the Low, Moderate, and High are categorical splits of the prevalence ratios based upon their distribution in the data. The values for each of the indicators are typically the percentage of individuals experiencing that condition at the tract level.\n\nArticulate what a low, moderate, or high prevalence ratio means based upon the structural racism indicator you’re looking at.\nReferring to CDC’s measure definitions, identify one health outcome, health risk behavior, prevention indicator, and health status indicator, and focus on telling a story about the Detroit region relating your four health indicators back to your assigned structural racism indicator.\nDevelop one (or more) key visuals that help to substantiate your story. Prepare to share this key visual during the last 15 minutes of class.\nIdentify key questions and next steps to take based upon the story and evidence you’ve analyzed.\n\n\nHousing\n\n\nCode\ndetroit |&gt; \n  group_by(cat_housing) |&gt; \n  summarise(\n    COPD = mean(COPD, na.rm=TRUE),\n    OBESITY = mean(OBESITY, na.rm=TRUE),\n    STROKE = mean(STROKE, na.rm=TRUE),\n    DEPRESSION = mean(DEPRESSION, na.rm=TRUE),\n    LPA = mean(LPA, na.rm=TRUE),\n    CASTHMA = mean(CASTHMA, na.rm=TRUE),\n    MAMMOUSE = mean(MAMMOUSE, na.rm=TRUE),\n    TEETHLOST = mean(TEETHLOST, na.rm=TRUE),\n    ARTHRITIS = mean(ARTHRITIS, na.rm=TRUE),\n    COREM = mean(COREM, na.rm=TRUE),\n    DIABETES = mean(DIABETES, na.rm=TRUE),\n    BINGE = mean(BINGE, na.rm=TRUE),\n    SLEEP = mean(SLEEP, na.rm=TRUE),\n    ACCESS2 = mean(ACCESS2, na.rm=TRUE),\n    BPMED = mean(BPMED, na.rm=TRUE),\n    PHLTH = mean(PHLTH, na.rm=TRUE),\n    DENTAL = mean(DENTAL, na.rm=TRUE),\n    MHLTH = mean(MHLTH, na.rm=TRUE),\n    BPHIGH = mean(BPHIGH, na.rm=TRUE),\n    CANCER = mean(CANCER, na.rm=TRUE),\n    CHOLSCREEN = mean(CHOLSCREEN, na.rm=TRUE),\n    CHD = mean(CHD, na.rm=TRUE),\n    GHLTH = mean(GHLTH, na.rm=TRUE),\n    CHECKUP = mean(CHECKUP, na.rm=TRUE),\n    CSMOKING = mean(CSMOKING, na.rm=TRUE),\n    CERVICAL = mean(CERVICAL, na.rm=TRUE),\n    COLON_SCREEN = mean(COLON_SCREEN, na.rm=TRUE),\n    KIDNEY = mean(KIDNEY, na.rm=TRUE),\n    HIGHCOL = mean(HIGHCHOL, na.rm=TRUE),\n    COREW = mean(COREW, na.rm=TRUE)\n  ) |&gt; \n  pivot_longer(cols = -1) |&gt; \n  pivot_wider(names_from = \"cat_housing\", values_from = \"value\") |&gt; \n  select(Name = name, Low, Moderate, High) |&gt; \n  gt() |&gt; \n  fmt_percent(2:4, decimals = 2, scale_values = FALSE)\n\n\n\n\n\n\n  \n    \n    \n      Name\n      Low\n      Moderate\n      High\n    \n  \n  \n    COPD\n7.63%\n7.63%\n7.78%\n    OBESITY\n35.55%\n36.56%\n35.30%\n    STROKE\n3.56%\n3.72%\n3.33%\n    DEPRESSION\n22.51%\n21.54%\n23.37%\n    LPA\n24.27%\n25.66%\n24.96%\n    CASTHMA\n11.49%\n11.86%\n11.61%\n    MAMMOUSE\n76.37%\n77.50%\n76.02%\n    TEETHLOST\n13.68%\n14.54%\n13.96%\n    ARTHRITIS\n27.90%\n27.68%\n26.96%\n    COREM\n43.48%\n41.57%\n41.93%\n    DIABETES\n10.89%\n11.61%\n10.25%\n    BINGE\n16.91%\n15.92%\n17.00%\n    SLEEP\n35.62%\n37.32%\n35.82%\n    ACCESS2\n7.70%\n8.20%\n8.60%\n    BPMED\n77.43%\n77.55%\n75.99%\n    PHLTH\n12.00%\n12.23%\n12.18%\n    DENTAL\n68.27%\n66.22%\n66.54%\n    MHLTH\n16.33%\n16.62%\n17.29%\n    BPHIGH\n35.30%\n36.67%\n33.76%\n    CANCER\n7.02%\n6.74%\n6.67%\n    CHOLSCREEN\n87.01%\n86.97%\n85.55%\n    CHD\n6.25%\n6.21%\n6.09%\n    GHLTH\n17.46%\n18.66%\n17.68%\n    CHECKUP\n76.49%\n77.66%\n75.86%\n    CSMOKING\n19.03%\n19.65%\n20.01%\n    CERVICAL\n85.46%\n85.53%\n84.41%\n    COLON_SCREEN\n72.95%\n72.56%\n71.64%\n    KIDNEY\n3.27%\n3.38%\n3.12%\n    HIGHCOL\n35.05%\n34.39%\n34.15%\n    COREW\n39.09%\n38.05%\n38.39%\n  \n  \n  \n\n\n\n\n\n\nPoverty\n\n\nCode\ndetroit |&gt; \n  group_by(cat_poverty) |&gt; \n  summarise(\n    COPD = mean(COPD, na.rm=TRUE),\n    OBESITY = mean(OBESITY, na.rm=TRUE),\n    STROKE = mean(STROKE, na.rm=TRUE),\n    DEPRESSION = mean(DEPRESSION, na.rm=TRUE),\n    LPA = mean(LPA, na.rm=TRUE),\n    CASTHMA = mean(CASTHMA, na.rm=TRUE),\n    MAMMOUSE = mean(MAMMOUSE, na.rm=TRUE),\n    TEETHLOST = mean(TEETHLOST, na.rm=TRUE),\n    ARTHRITIS = mean(ARTHRITIS, na.rm=TRUE),\n    COREM = mean(COREM, na.rm=TRUE),\n    DIABETES = mean(DIABETES, na.rm=TRUE),\n    BINGE = mean(BINGE, na.rm=TRUE),\n    SLEEP = mean(SLEEP, na.rm=TRUE),\n    ACCESS2 = mean(ACCESS2, na.rm=TRUE),\n    BPMED = mean(BPMED, na.rm=TRUE),\n    PHLTH = mean(PHLTH, na.rm=TRUE),\n    DENTAL = mean(DENTAL, na.rm=TRUE),\n    MHLTH = mean(MHLTH, na.rm=TRUE),\n    BPHIGH = mean(BPHIGH, na.rm=TRUE),\n    CANCER = mean(CANCER, na.rm=TRUE),\n    CHOLSCREEN = mean(CHOLSCREEN, na.rm=TRUE),\n    CHD = mean(CHD, na.rm=TRUE),\n    GHLTH = mean(GHLTH, na.rm=TRUE),\n    CHECKUP = mean(CHECKUP, na.rm=TRUE),\n    CSMOKING = mean(CSMOKING, na.rm=TRUE),\n    CERVICAL = mean(CERVICAL, na.rm=TRUE),\n    COLON_SCREEN = mean(COLON_SCREEN, na.rm=TRUE),\n    KIDNEY = mean(KIDNEY, na.rm=TRUE),\n    HIGHCOL = mean(HIGHCHOL, na.rm=TRUE),\n    COREW = mean(COREW, na.rm=TRUE)\n  ) |&gt; \n  pivot_longer(cols = -1) |&gt; \n  pivot_wider(names_from = \"cat_poverty\", values_from = \"value\") |&gt; \n  select(Name = name, Low, Moderate, High) |&gt; \n  gt() |&gt; \n  fmt_percent(2:4, decimals = 2, scale_values = FALSE)\n\n\n\n\n\n\n  \n    \n    \n      Name\n      Low\n      Moderate\n      High\n    \n  \n  \n    COPD\n7.42%\n8.57%\n6.29%\n    OBESITY\n35.65%\n38.42%\n31.88%\n    STROKE\n3.51%\n4.04%\n2.80%\n    DEPRESSION\n22.31%\n22.41%\n21.97%\n    LPA\n24.05%\n28.55%\n20.03%\n    CASTHMA\n11.52%\n12.43%\n10.55%\n    MAMMOUSE\n76.75%\n77.46%\n75.74%\n    TEETHLOST\n13.26%\n17.23%\n9.54%\n    ARTHRITIS\n27.68%\n28.19%\n26.24%\n    COREM\n43.80%\n39.30%\n45.63%\n    DIABETES\n10.86%\n12.66%\n8.41%\n    BINGE\n16.83%\n15.49%\n17.81%\n    SLEEP\n35.82%\n38.85%\n32.93%\n    ACCESS2\n7.50%\n9.65%\n6.19%\n    BPMED\n77.30%\n77.10%\n76.94%\n    PHLTH\n11.82%\n13.44%\n10.17%\n    DENTAL\n68.48%\n62.14%\n73.59%\n    MHLTH\n16.26%\n18.07%\n14.77%\n    BPHIGH\n35.36%\n38.04%\n31.27%\n    CANCER\n6.93%\n6.48%\n7.20%\n    CHOLSCREEN\n87.20%\n85.44%\n88.12%\n    CHD\n6.10%\n6.61%\n5.53%\n    GHLTH\n17.26%\n21.23%\n13.30%\n    CHECKUP\n76.80%\n77.66%\n75.60%\n    CSMOKING\n18.78%\n22.35%\n15.42%\n    CERVICAL\n85.73%\n84.45%\n86.07%\n    COLON_SCREEN\n73.43%\n71.30%\n73.41%\n    KIDNEY\n3.24%\n3.57%\n2.82%\n    HIGHCOL\n34.90%\n34.37%\n34.30%\n    COREW\n39.22%\n35.39%\n43.02%\n  \n  \n  \n\n\n\n\n\n\nEducation\n\n\nCode\ndetroit |&gt; \n  group_by(cat_education) |&gt; \n  summarise(\n    COPD = mean(COPD, na.rm=TRUE),\n    OBESITY = mean(OBESITY, na.rm=TRUE),\n    STROKE = mean(STROKE, na.rm=TRUE),\n    DEPRESSION = mean(DEPRESSION, na.rm=TRUE),\n    LPA = mean(LPA, na.rm=TRUE),\n    CASTHMA = mean(CASTHMA, na.rm=TRUE),\n    MAMMOUSE = mean(MAMMOUSE, na.rm=TRUE),\n    TEETHLOST = mean(TEETHLOST, na.rm=TRUE),\n    ARTHRITIS = mean(ARTHRITIS, na.rm=TRUE),\n    COREM = mean(COREM, na.rm=TRUE),\n    DIABETES = mean(DIABETES, na.rm=TRUE),\n    BINGE = mean(BINGE, na.rm=TRUE),\n    SLEEP = mean(SLEEP, na.rm=TRUE),\n    ACCESS2 = mean(ACCESS2, na.rm=TRUE),\n    BPMED = mean(BPMED, na.rm=TRUE),\n    PHLTH = mean(PHLTH, na.rm=TRUE),\n    DENTAL = mean(DENTAL, na.rm=TRUE),\n    MHLTH = mean(MHLTH, na.rm=TRUE),\n    BPHIGH = mean(BPHIGH, na.rm=TRUE),\n    CANCER = mean(CANCER, na.rm=TRUE),\n    CHOLSCREEN = mean(CHOLSCREEN, na.rm=TRUE),\n    CHD = mean(CHD, na.rm=TRUE),\n    GHLTH = mean(GHLTH, na.rm=TRUE),\n    CHECKUP = mean(CHECKUP, na.rm=TRUE),\n    CSMOKING = mean(CSMOKING, na.rm=TRUE),\n    CERVICAL = mean(CERVICAL, na.rm=TRUE),\n    COLON_SCREEN = mean(COLON_SCREEN, na.rm=TRUE),\n    KIDNEY = mean(KIDNEY, na.rm=TRUE),\n    HIGHCOL = mean(HIGHCHOL, na.rm=TRUE),\n    COREW = mean(COREW, na.rm=TRUE)\n  ) |&gt; \n  pivot_longer(cols = -1) |&gt; \n  pivot_wider(names_from = \"cat_education\", values_from = \"value\") |&gt; \n  select(Name = name, Low, Moderate, High) |&gt; \n  gt() |&gt; \n  fmt_percent(2:4, decimals = 2, scale_values = FALSE)\n\n\n\n\n\n\n  \n    \n    \n      Name\n      Low\n      Moderate\n      High\n    \n  \n  \n    COPD\n9.33%\n7.15%\n6.92%\n    OBESITY\n43.00%\n33.66%\n33.09%\n    STROKE\n5.03%\n3.14%\n2.87%\n    DEPRESSION\n20.76%\n22.51%\n23.49%\n    LPA\n33.10%\n22.88%\n20.94%\n    CASTHMA\n13.72%\n11.10%\n10.72%\n    MAMMOUSE\n80.13%\n75.92%\n75.02%\n    TEETHLOST\n21.60%\n12.00%\n10.45%\n    ARTHRITIS\n29.20%\n26.93%\n27.04%\n    COREM\n35.60%\n43.59%\n46.54%\n    DIABETES\n16.04%\n9.54%\n8.76%\n    BINGE\n13.55%\n17.17%\n18.20%\n    SLEEP\n43.45%\n34.56%\n32.82%\n    ACCESS2\n10.96%\n7.43%\n6.63%\n    BPMED\n78.13%\n76.74%\n76.78%\n    PHLTH\n14.91%\n11.30%\n10.90%\n    DENTAL\n56.24%\n69.78%\n72.41%\n    MHLTH\n19.13%\n16.01%\n15.58%\n    BPHIGH\n43.66%\n33.05%\n31.86%\n    CANCER\n5.95%\n7.03%\n7.22%\n    CHOLSCREEN\n85.35%\n87.01%\n87.16%\n    CHD\n7.05%\n5.92%\n5.79%\n    GHLTH\n26.00%\n15.78%\n14.20%\n    CHECKUP\n80.25%\n76.06%\n74.87%\n    CSMOKING\n25.02%\n17.93%\n17.05%\n    CERVICAL\n84.87%\n85.30%\n85.44%\n    COLON_SCREEN\n71.22%\n72.57%\n73.50%\n    KIDNEY\n4.15%\n3.04%\n2.85%\n    HIGHCOL\n33.82%\n34.46%\n35.36%\n    COREW\n31.33%\n40.41%\n42.06%"
  },
  {
    "objectID": "assignments/labs/10_health_equity/10_health_equity.html#lab-evaluation",
    "href": "assignments/labs/10_health_equity/10_health_equity.html#lab-evaluation",
    "title": "Health Equity",
    "section": "Lab Evaluation",
    "text": "Lab Evaluation\nYour group will present your narrative at the end of our class session. After class, please complete your lab reflection and push this to Github. There will be no other lab content or work for this week other than completing your own individual reflection on what we’ve done in small groups in class."
  },
  {
    "objectID": "assignments/labs/10_health_equity/10_health_equity.html#references",
    "href": "assignments/labs/10_health_equity/10_health_equity.html#references",
    "title": "Health Equity",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "howto/getstarted.html",
    "href": "howto/getstarted.html",
    "title": "Get Started",
    "section": "",
    "text": "Here you will find a range of “how to” resources and code base to help you advance tasks in Neighborhood Analysis."
  },
  {
    "objectID": "howto/rbasics_01.html#getting-familiar",
    "href": "howto/rbasics_01.html#getting-familiar",
    "title": "Lesson 1: Basic Principles",
    "section": "Getting Familiar",
    "text": "Getting Familiar\nLet’s start off by simply getting familiar with the R console. As you learned in the introduction, R is command line software focused on statistical computing, and RStudio is a user interface which enhances your ability to interact with R.\nFor most new useRs, R may be intimidating because of the need to learn a basic language to interact with it. When most of our interaction with computers happens through “point and click” graphical interfaces, language-driven approaches seem less intuitive.\nAt the same time, there are some really good reasons to gain familiarity with R and RStudio, particularly within the context of urban planning analysis.\n\nR and RStudio are open source tools, and are therefore available to be downloaded and used without cost (this does not negate questions regarding accessibility of the language, given its steep learning curve).\nR and RStudio are supported by a wide range of users who develop packages for specific use cases, including those that are useful for urban planning analysis.\nR and RStudio provide a framework for reproducible data manipulation and analysis - rather than sharing output with others, we can share raw data and code and they can reproduce our output.\n\nThinking about the type of analysis we will do in this class, there are some additional rationales for learning and working in R and RStudio:\n\nR has a powerful set of functions for aggregating and manipulating many data records into a smaller number of summary records - we use these types of functions frequently to summarize neighborhood characteristics\nR can natively read from and write to many types of data sources - this allows us to perform most or all of our analysis within a single application rather than passing data between applications for different types of manipulation or analysis\nR can help us to automate elements of data visualization, which can be useful when we need to reproduce forms of analysis for different places or other data categories\nLooking beyond the reasons to use R and RStudio as a platform for analysis, these tools represent one of several programming languages frequently used for data science (the other main language being Python) - learning these languages prepares you for future interface with other data science tools and strategies"
  },
  {
    "objectID": "howto/rbasics_01.html#lesson-goals",
    "href": "howto/rbasics_01.html#lesson-goals",
    "title": "Lesson 1: Basic Principles",
    "section": "Lesson Goals",
    "text": "Lesson Goals\nBy the end of this lesson, you should be familiar with:\n\nThe RStudio console and R language\nR data types and structures\nBasic data manipulation and querying"
  },
  {
    "objectID": "howto/rbasics_01.html#r-basics",
    "href": "howto/rbasics_01.html#r-basics",
    "title": "Lesson 1: Basic Principles",
    "section": "R Basics",
    "text": "R Basics\nAs a programming language, R was initially designed to be run in a terminal console. You can still run R in this fashion, if you wish. RStudio is an integrated development environment (IDE) for R - in addition to providing us with a terminal window in which we could run commands, it also provides additional windows for viewing data and visualizations.\nIt is important that we keep these things in mind, because they will help us to understand what its like to interact with R via RStudio. Let’s start with a very basic way of interacting with R. Here is what the basic RStudio interface looks like:\n\nThe Source pane is where you will write and view scripts that provide instructions to the console.\nThe Console / Terminal pane is the where you can view the execution of commands. You can directly type commands here and then click control / command + enter to run. More typically, you’ll execute lines of code from within your scripts or the source pane.\nTHe Environment pane allows you to view datasets loaded into memory and other objects which you have defined. You can also see a few basic characteristics of existing R objects.\nThe Auxiliary pane which has multiple tabs. You will most often use this to view output, or load help documentation."
  },
  {
    "objectID": "howto/rbasics_01.html#r-scripts-vs-notebooks",
    "href": "howto/rbasics_01.html#r-scripts-vs-notebooks",
    "title": "Lesson 1: Basic Principles",
    "section": "R Scripts vs Notebooks",
    "text": "R Scripts vs Notebooks\nThere’s two main strategies for creating reproducible commands (code) to tell R what to do. Scripts are text files where the majority of language used will be R commands. Everything in a script is interpreted as code unless you explicitly denote that it is a comment. Notebooks are powerful tools in that they allow you to blend together text that’s meant to be read by humans with code chunks that are meant to be read and interpreted by R (or another computing language). Notebooks are particularly powerful because they allow you to format and publish your text while embedding chunks of analysis at strategically appropriate places.\nThe benefit of working in a notebook is that you can run code in line with your text, and see the results integrated with your writing kind of like a scientific lab notebook. Some people will use R Notebooks to write reports, since they can render tables and figures in line with their text, and these can easily be updated if new data or parameters are supplied.\nCode Chunks\nThis is what a code chunk looks like:\n\n# Code Chunk\n\nAny content inside of this code chunk will be interpreted by your R session in the terminal when you hit the green play button to the right. You can also step through each line of code by putting your cursor to the right of it and hitting command+enter (Mac) or control+enter (PC) - I strongly recommend you get into the habit of running code this way at first.\nYou can create new code chunks by pressing control+option+I (Mac) or control+alt+I (PC).\nComments\nIn the above code chunk, we have some text preceded by a hashtag (#). Any content to the right of a hashtag (# groundbreaking insightful comment) will be considered a comment and will not be interpreted as code. Comments are a great way to make short notes to remind yourself or others of what you’re doing:\n\n# This is a comment\n\n1+2 # This is a comment in line with some active code\n\n[1] 3"
  },
  {
    "objectID": "howto/rbasics_01.html#your-first-commands",
    "href": "howto/rbasics_01.html#your-first-commands",
    "title": "Lesson 1: Basic Principles",
    "section": "Your First Commands",
    "text": "Your First Commands\nLet’s start by entering a simple command - let’s add together 2 and 2 in the console and ask R to return the product.\n\n2+2\n\n[1] 4\n\n\nEntering 2+2 into our console window and then hitting command/control+enter asks R to process the request we have given it - it then gives us back an answer to our request. We could of course do the same thing with other simple numeric operators:\n\n\n+ Addition\n\n- Subtraction\n\n* Multiplication\n\n/ Division\n\n^ Exponents (e.g. 2^3 = 8)\n\n() Parentheses - to control order of operations (e.g. ```(2+3)/5 = 1)\n\nWe can do basic math in a console - not terribly exciting, but at least this helps you to see how R will respond to basic commands:\n\n2+2\n\n[1] 4\n\n2^3\n\n[1] 8\n\n(2+3)/5\n\n[1] 1\n\n\nNow your turn - create a script (File -&gt; New File -&gt; R Script) and perform some simple math operations. Also explore how R handles order of operations.\nGet at it!\nVariables\nIn most cases, we don’t want to just type things into the console and then get an answer - we’d be just as well served with a calculator. Our next step is to understand that R can store the output of a command for later use. The most basic way to do this is to assign our output to an object. we can do this using the &lt;- assignment operator:\n\nx &lt;- 2+2\n\nLet’s learn how to speak this out. We just told R, into an object we have (arbitrarily) named “x”, store the output of 2 + 2. Because this is now stored, we can retrieve it and use it later. If we simply ask for “x” R will share with us the previously assigned output - 2+2\nOption + - (Mac) or Alt + - (PC) is the shortcut for inserting the assignment operator.\n\nx\n\n[1] 4\n\n\nThis means that we could also use this output in other formulas. Let’s see what happens if we square X:\n\nx^2\n\n[1] 16\n\n\nSince X is 4, we get the output that is the equivalent of typing 4^2.\nIt is important to note here that we can provide any type of label we’d like for an R variable. Instead of using “x” as a variable name, we could use anything else.\nAssign the sum of 4+6 into a variable named “cat”.\n\ncat&lt;-4+6\n\nWe just assigned to a variable called “cat” the product of 4+6. To retrieve the value of your assigned variable, you can just call it by name:\n\ncat\n\n[1] 10\n\n\nR allows you to name variables as you wish. Note that variables need to start with a character, cannot start with a number (e.g. 1_Numbers would not work), and cannot include spaces (e.g. “variables squared” would not work but you could use an underscore - “variables_squared” which would work). Also note that you will want to avoid variable names that are the same as R functions (so naming a variable “mean” for instance, would not be a good idea, as this would cause confusion with the function mean() which calculates the average of a vector).\nWe can of course work with multiple variables at once:\n\ncat+x\n\n[1] 14\n\n\nIn this case, cat is 10 and x is 4 (you can see the values stored in objects in the environment pane). Let’s divide cat+x by x:\n\n(cat+x)/x\n\n[1] 3.5\n\n\nThis is great (i guess…) - we have a calculator that can store and make use of values as objects. Not so exciting for neighborhood analysis just yet, though…\nLists\nThe next thing to note is that objects don’t have to be single values. We could also assign lists of values to an object:\n\ncol1 &lt;- c(2, 3, 4, 5, 6)\n\nNote here that c() (formally the concatenate function) is used to denote that we have a list. Each list item it separated by commas. If we call up this object, we can have a look at our list:\n\ncol1\n\n[1] 2 3 4 5 6\n\n\nWorking with a single object, we could do things by using the object in a formula. We can do the same with a list:\n\ncol1+2\n\n[1] 4 5 6 7 8\n\n\nTo each list item, we added 2. We could even store this as a new object if we wanted 2\n\ncol2 &lt;- col1+2\n\nWriting this out, we told r “Place into an object called”col2” the product of adding 2 to each item in the list contained in “col1”.\n\ncol2\n\n[1] 4 5 6 7 8\n\n\nCool! We can manipulate our list items all at once.\nSequences\nWe can also have R automatically create sequences of numbers for us, if they follow a regular pattern using the seq() command:\n\nseq(0,100, 5)\n\n [1]   0   5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90\n[20]  95 100\n\n\nThis says create a list containing the sequence of numbers from 0 to 100 counting by fives.\nTry creating your own sequence - count up from 4 to 24 by 4\n\n# Your Work Here\nseq(4, 24, 4)\n\n[1]  4  8 12 16 20 24\n\n\nTry creating your another sequence - count down from 50 to 2 by 4. How would you do this?\n\n# Your Work Here\nseq(50, 2, -4)\n\n [1] 50 46 42 38 34 30 26 22 18 14 10  6  2\n\n\nList Interaction\nBut we digress - back to our existing lists. What would happen if we decided to multiply col1 by col2?\n\ncol1*col2\n\n[1]  8 15 24 35 48\n\n\nCan you see what happens here? Since our lists are the same size, R multiples the first item in col1 by the first item in col2, the second item in col1 by the second item in col2, and so on - e.g. (24, 35 …)\nVector Types\nLists, however, don’t have to be just numeric - they can be other types of things as well:\n\n\nNumeric: Values containing integers (positive or negative whole numbers such as 1, 10, 25840) or double values (any real number such as 1, 2.14, 3.254, -12). Double values may also include some special classes such as Inf, -Inf, and NaN - “positive infinity”, “negative infinity”, and “not a number”.\n\nLogical: Logical values including TRUE, FALSE, and NA. TRUE and FALSE are self-explanatory. NA stands for “not available”, which should not be confused with NULL, which is no value.\n\nCharacter: Also known as strings, these consist of text or text-like information. In R, we tend to surround strings by quotation marks to denote them. For instance, c(\"Black Cat\", \"Brown Dog\", \"Dappled Donkey\", \"Red Rooster\") is a character vector containing four items.\nList Manipulation\nWe’ll talk about some other types of vectors later, but these are sufficient to get you started. In addition to numeric vectors, probably the most common other type of vectors we will encounter are character vectors. Let’s make a list of the items we need to make a single serving of oatmeal (your professor is hungry as he writes this tutorial):\n\nc(\"Oatmeal\", \"Water\", \"Salt\", \"Sugar\")\n\n[1] \"Oatmeal\" \"Water\"   \"Salt\"    \"Sugar\"  \n\n\nIn a separate list, let’s place the quantity of ingredients:\n\nc(1/2, 1, .25, 1)\n\n[1] 0.50 1.00 0.25 1.00\n\n\nPerhaps it would be useful to make a third list with the unit of measure for the quantity of ingredients:\n\nc(\"Cup\", \"Cup\", \"Teaspoon\", \"Tablespoon\")\n\n[1] \"Cup\"        \"Cup\"        \"Teaspoon\"   \"Tablespoon\"\n\n\nOkay, we have three lists, that we might be able to use for different things. In your script, write code that assigns the list of ingredients to a new object called “ingredients”, write the quantities to a new object called “quantity”, and write the units to a new object called “units”.\n\n\nYour Work\nSolution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\ningredients &lt;- c(\"Oatmeal\", \"Water\", \"Salt\", \"Sugar\")\nquantity &lt;- c(1/2, 1, .25, 1)\nunits &lt;- c(\"Cup\", \"Cup\", \"Teaspoon\", \"Tablespoon\")\n\n\n\n\nWe could do some interesting things here, like paste together the different list items into something approximating a recipe:\n\npaste(quantity, units, ingredients, sep=\" \")\n\n[1] \"0.5 Cup Oatmeal\"    \"1 Cup Water\"        \"0.25 Teaspoon Salt\"\n[4] \"1 Tablespoon Sugar\"\n\n\nWhat is sep = \" \" doing here? What would happen if you changed sep to a comma? Try it.\nYou can take a look at the documentation for the paste() command by typing ?paste.\nVerbalizing what we just asked R to do (a valuable habit for problem solving more complex functions and data manipulation later on), we said “paste together the list items contained in the variables quantity, units, and ingredients, placing a space between each of the list items.”\nWe can also manipulate list objects - let’s say you volunteer to host a community meeting and need to make 45 portions of your oatmeal recipe - how would you go about constructing your grocery list? Below, write out the operations that you would need to do to modify your existing list of quantities to account for 45 portions (let’s assume that ingredient quantities remain the same when we scale up our recipe):\nSince you’re new at this, here are few ways to do this (I hope you’ve tried on your own to figure it out on your own before reading on) - you could either modify the quantities in the quantity vector by multiplying them directly and creating a new vector:\n\nquantity45 &lt;- quantity*45\npaste(quantity45, units, ingredients, sep=\" \")\n\n[1] \"22.5 Cup Oatmeal\"    \"45 Cup Water\"        \"11.25 Teaspoon Salt\"\n[4] \"45 Tablespoon Sugar\"\n\n\nAlternately, you could modify the list directly in your paste command:\n\npaste(quantity*45, units, ingredients, sep=\" \")\n\n[1] \"22.5 Cup Oatmeal\"    \"45 Cup Water\"        \"11.25 Teaspoon Salt\"\n[4] \"45 Tablespoon Sugar\"\n\n\nThe outputs are exactly the same.\nSelecting List Items\nR can also help us to pick out list items. The brackets [] allow us to return list items by position (left to right).\n\nex_list&lt;-c(\"Jane\", \"Jacobs\", \"beats\", \"Robert\", \"Moses\", \"in\", \"a\", \"fight\", \"for\", \"New York\")\n\nex_list[4]\n\n[1] \"Robert\"\n\n\nWhat would happen if we put [-4] instead of 4?\n\n# Your Work Here\nex_list[-4]\n\n[1] \"Jane\"     \"Jacobs\"   \"beats\"    \"Moses\"    \"in\"       \"a\"        \"fight\"   \n[8] \"for\"      \"New York\"\n\n\nNow you try selecting the tenth element from the list.\n\n\nYour Work\nSolution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\nex_list[10]\n\n[1] \"New York\"\n\n\n\n\n\nWe can also select multiple elements at the same time:\n\nex_list[c(4, 5, 3, 1, 2)]\n\n[1] \"Robert\" \"Moses\"  \"beats\"  \"Jane\"   \"Jacobs\"\n\n\nWe created a list c() and placed it in brackets which told R that we wanted to return the values of ex_list that corresponded to the positions in our other list c(4, 5, 3, 1, 2).\nNow you try creating and manipulating a list.\n\n\nYour Work\nExample Solution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\nex_list_2&lt;-c(\"I have eaten\", \"the plums\", \"that were in\", \"the icebox\", \"and which\", \"you were probably\", \"saving\", \"for breakfast\", \"Forgive me\", \"they were delicious\", \"so sweet\", \"and so cold\")\n\nex_list_2[c(1, 4, 8, 9)]\n\n[1] \"I have eaten\"  \"the icebox\"    \"for breakfast\" \"Forgive me\"   \n\n\n\n\n\nNow re-create the sequence you crafted earlier (count up from 4 to 24 by 4) and subset out the fifth element from that numeric sequence:\n\n\nYour Work\nExample Solution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\nseq(4, 24, 4)[5]\n\n[1] 20\n\n\n\n\n\nFun (maybe), but not yet particularly useful. You didn’t take this class because you wanted to scale oatmeal recipes or identify numbers in a sequence. The more powerful stuff is coming up! - these are building blocks to teach some of the logic of the language.\nData Frames\nThe next thing for us to think about is how we might combine lists together. Thinking back to our oatmeal recipe, right now we have three separate lists each (respectively) with our quantity, units, and ingredients. We’ve figured out that we can paste together items from different lists, but it might be nice to be able to store them in one object rather than three. This is a good time to introduce the R data frame, which is the object you’ll be dealing with the most.\nStart by looking at R’s internal documentation on data frames (?data.frame):\n\n# Your Work Here\n?data.frame\n\nNow lets coerce our three lists into a single data frame called “oatmeal”:\n\noatmeal &lt;- data.frame(quantity, units, ingredients, stringsAsFactors = FALSE)\noatmeal\n\n  quantity      units ingredients\n1     0.50        Cup     Oatmeal\n2     1.00        Cup       Water\n3     0.25   Teaspoon        Salt\n4     1.00 Tablespoon       Sugar\n\n\nWe created a new object called “oatmeal” that has bound our three lists together into a data frame. We need to specify stringsAsFactors = FALSE to keep R from turning our strings (characters) into a special data type called factors (more on these later). R assumes that we want our columns labeled with the original list object names.\nWe can now look at our list as a series of columns that have been given the name of the variable they were stored in as a list, and each row represents one of the list items. An important concept to keep in mind is that a data frame is a series of lists that are in essence glued together.\nWe can refer to and access rows and columns in our data frame in several ways. If we want to return those items in a specific column if the list, we can use the $ operator to refer to that item:\n\noatmeal$ingredients\n\n[1] \"Oatmeal\" \"Water\"   \"Salt\"    \"Sugar\"  \n\n\nWe just returned the list items that were in the column named ingredients. We could also use our subset notation to retrieve the same things. This subset notation builds upon what we learned when we accessed list items by position (e.g. units[2]):\n\n#knitr::include_graphics(\"Images/04_guru99_dataframe.png\")\n\nWhile subsets of lists require one number corresponding to the index position, data frames have two dimensions - rows and columns, so we need to be able to differentiate between each. R does this using a comma in the subset notation [row, column]. If we want all rows or columns, we can just leave that portion of the bracket empty. For instance, the code below is the equivalent of typing oatmeal$ingredients since ingredients are the third column in the oatmeal data frame:\n\noatmeal[,3]\n\n[1] \"Oatmeal\" \"Water\"   \"Salt\"    \"Sugar\"  \n\n\nThis is because ingredients is the third column in the oatmeal data frame.\nNow you try: query the second row of the oatmeal data frame:\n\n\nYour Work\nExample Solution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\noatmeal[2,]\n\n  quantity units ingredients\n2        1   Cup       Water\n\n\n\n\n\nHow would you query the third row of the second column?\n\n\nYour Work\nExample Solution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\noatmeal[3,2]\n\n[1] \"Teaspoon\"\n\n\n\n\n\nOk, so we see that we can create subsets fairly easily, either using column names or index positions in our dataset.\nWe can add new columns to our data frame. Oftentimes when we are working with data, we’ll need to calculate a new column based upon the values in other columns. We have our data frame with a recipe for 1 serving of oatmeal. Let’s say we frequently need to make 45 servings of oatmeal (its famous, and the reason why people show up to your 7am neighborhood meetings…), so you want to include that quantity alongside the single serving quantity.\nLet’s create a new column called “quantity45” and add to it the quantity of ingredients for a 45 serving batch of oatmeal:\n\noatmeal$quantity45&lt;-oatmeal$quantity*45\n\noatmeal\n\n  quantity      units ingredients quantity45\n1     0.50        Cup     Oatmeal      22.50\n2     1.00        Cup       Water      45.00\n3     0.25   Teaspoon        Salt      11.25\n4     1.00 Tablespoon       Sugar      45.00\n\n\nNote that we need to refer to the original quantity by pointing to the oatmeal data frame as well. Let’s verbalize this to think about what we’re doing. “Into a new column in the oatmeal data frame called quantity45 (oatmeal$quantity45 &lt;-), write the value contained in the oatmeal data frame called quantity multiplied by 45 (oatmeal$quantity*45).”\nNow you try - create a new column called “instructions” in the oatmeal data table that contains our recipe quantity for 45 portions of oatmeal, units, and ingredients pasted together (this will require you reference data using concepts we learned earlier):\n\n\nYour Work\nExample Solution\n\n\n\n\n# Try in your script or notebook.\n\n\n\n\noatmeal$instructions&lt;-paste(quantity*45, units, ingredients, sep=\" \")\n\n\n\n\nNote again that we need to refer to each of the specific columns in the oatmeal data frame using their appropriate vector (e.g. oatmeal$ingredients. Note that in this case if we omitted the pointer to oatmeal, R would assume we wanted to do something with the list called ingredients. In this case, that would actually work, but in most cases, we will just have a data frame and won’t have a separate list stored as an R object - we’d get an error.\nWe can also pull out all items meeting a specific criteria in our data frame - let’s say we want to look at those ingredients that are measured in cups:\n\noatmeal[oatmeal$units == \"Cup\",]\n\n  quantity units ingredients quantity45     instructions\n1      0.5   Cup     Oatmeal       22.5 22.5 Cup Oatmeal\n2      1.0   Cup       Water       45.0     45 Cup Water\n\n\nThis looks weird - we’ve introduced some new notation here. Let’s first speak this out and then we can learn more about the notation. “From the oatmeal data frame, return those rows from within the oatmeal data frame for which the value of the units column is equal to”Cup” (the equal sign in R is == two equal signs together). Notice also that the word “Cup” has parentheses surrounding it, denoting that it is a character string. The square brackets [] denote that we’re looking for something (or some things) within the oatmeal data frame. We continue to follow the [Row, Column] logic within the brackets.\nWhile in this case, we’re looking for rows that meet a specific criteria based upon the word “Cup” (searching for a character string), we could return subsets of numeric records in other ways:\n\noatmeal[oatmeal$quantity &gt; .5,]\n\n  quantity      units ingredients quantity45        instructions\n2        1        Cup       Water         45        45 Cup Water\n4        1 Tablespoon       Sugar         45 45 Tablespoon Sugar\n\n\nReturns those records from the oatmeal data frame for which the quantity value is greater than .5. If we wanted to include our .5 cups of water, we could specify &gt;= (greater than or equal to).\n\noatmeal[oatmeal$quantity &gt;= .5,]\n\n  quantity      units ingredients quantity45        instructions\n1      0.5        Cup     Oatmeal       22.5    22.5 Cup Oatmeal\n2      1.0        Cup       Water       45.0        45 Cup Water\n4      1.0 Tablespoon       Sugar       45.0 45 Tablespoon Sugar"
  },
  {
    "objectID": "howto/rbasics_01.html#lesson-1-summary-and-debrief",
    "href": "howto/rbasics_01.html#lesson-1-summary-and-debrief",
    "title": "Lesson 1: Basic Principles",
    "section": "Lesson 1 Summary and Debrief",
    "text": "Lesson 1 Summary and Debrief\nIn this lesson, you became more familiar with the R console and RStudio interface, learned about scrips and notebooks, and started to explore some of the basic functionality for how to store and retrieve variables, construct lists, and perform calculations. This may seem like a lot of details to internalize at this point (and it is), but these very basic building blocks will prove useful as you start to understand some of the more advanced functions for data manipulation.\nMoving forward, we’ll start to build on these basic building blocks by looking more at how to manipulate tabular data.\nCore Concepts and Terminology\n\nR Script\nNotebook\nCode Chunk\nVariables\nLists\nVectors\nData Frame\nHelpful Practices\n\nTake your time with mastering and feeling comfortable with basic concepts. While you may be eager to move ahead to move advanced (and interesting) things, if you don’t have a good hold of the underlying logic behind the basics, you’ll struggle to identify and solve problems in the future.\nBegin to internalize the practice of speaking or writing out what the code is doing. R is a language and you are learning to “speak” R. Being able to speak out in plain language what you think your code is doing is the first step to becoming “fluent” as a coder. This will also help you greatly when it comes time to debug code in the future.\nBegin the practice of developing good habits about object names in R - you now know the basic rules about names. Start to think about schemes or personal conventions that you might use to help you stay organized, partiuclarly when you have a lot of named objects in your environment."
  },
  {
    "objectID": "howto/rbasics_03.html",
    "href": "howto/rbasics_03.html",
    "title": "Lesson 3: Tidy Data",
    "section": "",
    "text": "In our previous two lessons, we’ve been working with Base R to do basic manipulation of data. These strategies are powerful and can do a lot, however, they are a bit clunky (something you may have been thinking to yourself) - there are tools and strategies that are tailored to the types of data forms and structures we tend to use to measure characteristics and dynamics of neighborhoods.\nIn this lesson, we’ll introduce principles of tidy data as well as a frequently used R package designed to help us manipulate and work more efficiently."
  },
  {
    "objectID": "howto/rbasics_03.html#lesson-overview",
    "href": "howto/rbasics_03.html#lesson-overview",
    "title": "Lesson 3: Tidy Data",
    "section": "",
    "text": "In our previous two lessons, we’ve been working with Base R to do basic manipulation of data. These strategies are powerful and can do a lot, however, they are a bit clunky (something you may have been thinking to yourself) - there are tools and strategies that are tailored to the types of data forms and structures we tend to use to measure characteristics and dynamics of neighborhoods.\nIn this lesson, we’ll introduce principles of tidy data as well as a frequently used R package designed to help us manipulate and work more efficiently."
  },
  {
    "objectID": "howto/rbasics_03.html#lesson-goals",
    "href": "howto/rbasics_03.html#lesson-goals",
    "title": "Lesson 3: Tidy Data",
    "section": "Lesson Goals",
    "text": "Lesson Goals\nBy the end of this lesson, you should be familiar with:\n\nPrinciples of tidy data\nHigh-level tools for selecting and subsetting data using dplyr syntax\nMore advanced strategies for grouping and summarizing data using dplyr syntax"
  },
  {
    "objectID": "howto/rbasics_03.html#getting-set-up",
    "href": "howto/rbasics_03.html#getting-set-up",
    "title": "Lesson 3: Tidy Data",
    "section": "Getting Set Up",
    "text": "Getting Set Up\n\nLoading Required Packages\nWe’re been working primarily in “base” R as we are getting familiar with the R language and RStudio interface. In Lesson 2, we introduced packages and made use of the readxl package to load data from an Excel file into R.\nTo review, we used install.packages() and library() to (respectively) install and load packages that extend R and RStudio’s functionality. If you remember from our last lesson, you will only need to install a package once, but you will need to load it every time you start your R session and want to use it.\nLet’s start by loading the following packages:\n\nreadxl contains tools which will help us to read Excel files into R\ntidyverse contains tools which we’ll use to subset, filter, group, and summarize our data\n\nIf you completed the last lesson, you will already have installed readxl. Let’s install the tidyverse package and then load both the readxl and tidyverse packages for use:\n\nYour TurnSolution\n\n\nTry installing the tidyverse package (if it is not already installed on your machine), and then load readxl and tidyverse for use in your R session.\n\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(readxl)\n\n\n\n\n\n\nLoading Data\nIf it’s not already loaded, load the OZ dataset we worked with in Lesson 2. You can assign the data whatever name you’d like, but we will stick with the name we used previously, ‘ozs’. A copy of the Urban Institute’s dataset is available here for download.\n\n\nYou’ll need to authenticate and log in to UIUC Box to access this file. You can also download the data directly from Urban Institute’s Opportunity Zone landing page.\n\nozs &lt;- read_excel(\"data/urbaninstitute_tractlevelozanalysis_update01142021.xlsx\")"
  },
  {
    "objectID": "howto/rbasics_03.html#an-easier-and-more-efficient-way",
    "href": "howto/rbasics_03.html#an-easier-and-more-efficient-way",
    "title": "Lesson 3: Tidy Data",
    "section": "An easier and more efficient way?",
    "text": "An easier and more efficient way?\nWe could keep building queries in base R to describe or summarize other variables in our data. Looking at the code you’ve created in Lesson 2 in particular, you’re probably thinking that it looks fairly illegible. Part of the challenge of code like this is that you have to read from the inside out.\nLet’s learn a whole different way of constructing this same thing.\n\nPrinciples of Tidy Data\nThis lesson focuses on introducing the tidyverse, a series of packages designed specifically to make data science easier in R and RStudio. The functionality of the tidyverse is largely described in the accompanying book R for Data Science.\nData are structured for tidy analysis when columns each contain one individual variable, each row represents a unique observation, and there is only one value for each variable and observation:\n\nThe majority of the data which we will encounter in this class, and the majority of data we work with as planners already conforms to these principles.\nIn the case of the Opportunity Zone data we first looked at in Lesson 2, here’s what that looked like:\n\nEach column represented a different variable, for instance, whether an observation was designated an Opportunity Zone, the poverty rate, or the median household income.\nEach row represented a unique observation, in this case a unique census tract.\nEach value was unique and there was only one value for every variable and observation.\n\n\n\nIf you want to understand some of the rationale behind tidy data, Hadley Wickham’s article is a good resource."
  },
  {
    "objectID": "howto/rbasics_03.html#your-first-tidy-coding",
    "href": "howto/rbasics_03.html#your-first-tidy-coding",
    "title": "Lesson 3: Tidy Data",
    "section": "Your First Tidy Coding",
    "text": "Your First Tidy Coding\nAt this point, you should have your data loaded and available and you should also have the tidyverse and readxl packages loaded.\nIn Lesson 2, you worked on solving the following two data manipulation and description problems:\n\nReport average poverty rates for designated opportunity zones in metropolitan, micropolitan, and non-CBSA areas.\nFor Illinois, how different are the average vacancy rates for designated and undesignated census tracts?\n\nLet’s compare how to do that using base R and using commands from the tidyverse suite.\n\nPoverty Rates\n\nThe ProblemBase RTidy\n\n\nReport average poverty rates for designated opportunity zones in metropolitan, micropolitan, and non-CBSA areas.\n\n\n\nmean(ozs$PovertyRate[ozs$DesignatedOZ == 1 & ozs$Metro == 1], na.rm=TRUE)\n\n[1] 0.3335197\n\nmean(ozs$PovertyRate[ozs$DesignatedOZ == 1 & ozs$Micro == 1], na.rm=TRUE)\n\n[1] 0.2803457\n\nmean(ozs$PovertyRate[ozs$DesignatedOZ == 1 & ozs$NoCBSAType == 1], na.rm=TRUE)\n\n[1] 0.2357986\n\n\n\n\n\nozs |&gt; \n  filter(DesignatedOZ ==1, Metro == 1) |&gt;\n  summarise(mean(PovertyRate, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(PovertyRate, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             0.334\n\nozs |&gt; \n  filter(DesignatedOZ ==1, Micro == 1) |&gt;\n  summarise(mean(PovertyRate, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(PovertyRate, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             0.280\n\nozs |&gt; \n  filter(DesignatedOZ ==1, NoCBSAType == 1) |&gt;\n  summarise(mean(PovertyRate, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(PovertyRate, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             0.236\n\n\n\n\n\nWe get the same values out, but note the code we input in order to get these outputs is very different!\nLet’s break this down further.\nIn Base R…\n\nWe first specified the statistic we wanted mean().\nWe then specified the dataset and columns we wanted that mean for ozs$PovertyRate.\nWe then specified we only wanted a subset of the poverty rate variable where observations were designated opportunity zones and then based upon a metropolitan criterion. [ozs$DesignatedOZ == 1 & ozs$Metro == 1]\nWe also specified that we wanted to remove NA values from our calculation of the average mean(na.rm=TRUE).\n\nAs a reminder, when put together, these things looked like this:\n\nozs |&gt; \n  filter(DesignatedOZ ==1, Metro == 1) |&gt;\n  summarise(mean(PovertyRate, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(PovertyRate, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             0.334\n\n\nNext, let’s look at the structure of the tidy command to do the same thing:\n\n1ozs |&gt;\n2  filter(DesignatedOZ ==1, Metro == 1) |&gt;\n3  summarise(mean(PovertyRate, na.rm=TRUE))\n\n\n1\n\nFrom the ‘ozs’ dataset;\n\n2\n\nFilter (select rows from) the dataset where the DesignatedOZ column is equal to 1 (designated) AND the Metropolitan area flag is equal to 1 (a metropolitan area);\n\n3\n\nFor the filtered data from ‘ozs’, summarize (report back) the mean value for the PovertyRate column, removing NA values.\n\n\n\n\n# A tibble: 1 × 1\n  `mean(PovertyRate, na.rm = TRUE)`\n                              &lt;dbl&gt;\n1                             0.334\n\n\nThis is still complex, but we gain a major benefit - where in our Base R strategy the code is nested and hard to read, the Tidy syntax offers a more logical workflow. We used something called a pipe |&gt; to pass results of previous commands along a data analysis pipeline. This allows us to code steps in a logical order and makes it much easier to read and interpret what we’re doing step-by-step.\n\n\nVacancy Rates\nLet’s now compare code for our second challenge - examining vacancy rates.\n\nThe ProblemBase RTidy\n\n\nFor Illinois, how different are the average vacancy rates for designated and undesignated census tracts?\n\n\n\nozs$DesignatedOZ[is.na(ozs$DesignatedOZ)]&lt;-0\n\nmean(ozs$vacancyrate[ozs$DesignatedOZ == 1], na.rm=TRUE)\n\n[1] 0.1583661\n\nmean(ozs$vacancyrate[ozs$DesignatedOZ == 0], na.rm=TRUE)\n\n[1] 0.1367463\n\n\n\n\n\n111ozs |&gt;\n222  replace_na(list(DesignatedOZ = 0)) |&gt;\n333  group_by(DesignatedOZ) |&gt;\n4  summarise(mean(vacancyrate, na.rm=TRUE))\n\n\n1\n\nFrom the ‘ozs’ dataset,\n\n2\n\nReplace any values that at NA in the DesignatedOZ column with the value 0,\n\n3\n\nTreat our data as being grouped by the unique values of DesignatedOZ,\n\n4\n\nSummarize for us the mean value for vacancy rate, removing any NA values from our calculation.\n\n\n\n\n# A tibble: 2 × 2\n  DesignatedOZ `mean(vacancyrate, na.rm = TRUE)`\n         &lt;dbl&gt;                             &lt;dbl&gt;\n1            0                             0.137\n2            1                             0.158\n\n\n\n\n\nLots going on, but let’s pay attention to some cool things we just saw.\n\nAs we had with the poverty rate we started with our ‘ozs’ dataset and then sequentially modified the dataset to get to our final output - a summary output with values for the average vacancy rate for designated and eligible but not designated tracts.\nWe were able to substitute NA values with 0 using a special command in line with our data modification workflow.\nWe used something we haven’t seen before - group_by() to tell R to treat our data as grouped by the values of the DesignatedOZ variable.\nWe used summarise() to create an output table containing the average values for the vacancy rate grouped by the values in DesignatedOZ.\n\nThis quick illustration helps you understand some of the basics of how dplyr works. Two major improvements, in addition to specific commands for filtering rows and selecting columns are the use of pipes |&gt; and the ability to summarize data. You’ll also notice that the output is rendered in a minimally formatted table."
  },
  {
    "objectID": "howto/rbasics_03.html#basic-dplyr-verbs",
    "href": "howto/rbasics_03.html#basic-dplyr-verbs",
    "title": "Lesson 3: Tidy Data",
    "section": "Basic dplyr verbs",
    "text": "Basic dplyr verbs\n\nFiltering Data\nWe can use dplyr to filter out rows that meet certain criteria.\nFor instance, here’s how we’re filter out all records for tracts in Illinois:\n\n1ozs |&gt;\n2  filter(state == \"Illinois\")\n\n\n1\n\nFrom the ozs data object;\n\n2\n\nFilter out those rows in the column “state” for which state is equal to “Illinois”\n\n\n\n\n# A tibble: 1,659 × 27\n   geoid       state    DesignatedOZ county   Type  dec_score SE_Flag Population\n   &lt;chr&gt;       &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 17001000201 Illinois            0 Adams C… Non-…         7      NA       1937\n 2 17001000202 Illinois            0 Adams C… Low-…         1      NA       2563\n 3 17001000400 Illinois            0 Adams C… Low-…         1      NA       3403\n 4 17001000500 Illinois            0 Adams C… Low-…         1      NA       2298\n 5 17001000700 Illinois            0 Adams C… Low-…         1      NA       1259\n 6 17001000800 Illinois            1 Adams C… Low-…         1      NA       2700\n 7 17001000900 Illinois            0 Adams C… Low-…         5      NA       2671\n 8 17001010100 Illinois            0 Adams C… Non-…         2      NA       4323\n 9 17001010200 Illinois            0 Adams C… Low-…         2      NA       3436\n10 17001010300 Illinois            0 Adams C… Non-…         8      NA       6038\n# ℹ 1,649 more rows\n# ℹ 19 more variables: medhhincome &lt;dbl&gt;, PovertyRate &lt;dbl&gt;, unemprate &lt;dbl&gt;,\n#   medvalue &lt;dbl&gt;, medrent &lt;dbl&gt;, pctown &lt;dbl&gt;, severerentburden &lt;dbl&gt;,\n#   vacancyrate &lt;dbl&gt;, pctwhite &lt;dbl&gt;, pctBlack &lt;dbl&gt;, pctHispanic &lt;dbl&gt;,\n#   pctAAPIalone &lt;dbl&gt;, pctunder18 &lt;dbl&gt;, pctover64 &lt;dbl&gt;, HSorlower &lt;dbl&gt;,\n#   BAorhigher &lt;dbl&gt;, Metro &lt;dbl&gt;, Micro &lt;dbl&gt;, NoCBSAType &lt;dbl&gt;\n\n\n\n\nSelecting Columns\nSimilar to filter, we can use select() to select specific columns in our data frame:\n\n1ozs |&gt;\n2  select(state, DesignatedOZ)\n\n\n1\n\nFrom the ozs dataset;\n\n2\n\nSelect the columns named “state” and “DesignatedOZ”.\n\n\n\n\n# A tibble: 42,178 × 2\n   state   DesignatedOZ\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 Alabama            0\n 2 Alabama            0\n 3 Alabama            1\n 4 Alabama            0\n 5 Alabama            0\n 6 Alabama            0\n 7 Alabama            0\n 8 Alabama            1\n 9 Alabama            0\n10 Alabama            1\n# ℹ 42,168 more rows\n\n\n\n\nCombining filter() and select()\nYour turn - create a table containing the variables state, Designated, and Metro, for Illinois:\n\nYour TurnSolution\n\n\nFor Illinois, create a table containing the variables state, Designated, and Metro.\n\n\n\nozs |&gt;\n  select(state, DesignatedOZ, Metro) |&gt;\n  filter(state == \"Illinois\")\n\n\n1\n\nFrom the ‘ozs’ dataset;\n\n2\n\nSelect the columns “state”, “Designated OZ”, and “Metro”;\n\n3\n\nFrom the state column, select the subset of values where state is equal to “Illinois”.\n\n\n\n\n# A tibble: 1,659 × 3\n   state    DesignatedOZ Metro\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1 Illinois            0    NA\n 2 Illinois            0    NA\n 3 Illinois            0    NA\n 4 Illinois            0    NA\n 5 Illinois            0    NA\n 6 Illinois            1    NA\n 7 Illinois            0    NA\n 8 Illinois            0    NA\n 9 Illinois            0    NA\n10 Illinois            0    NA\n# ℹ 1,649 more rows\n\n\n\n\n\nYou should return a data frame with three columns and 1,659 rows.\n\nYour TurnSolution\n\n\nHow would you modify your code to limit this to tracts that were Metropolitan (Metro equal to 1)?\n\n\n\nozs |&gt;\n  select(state, DesignatedOZ, Metro) |&gt;\n  filter(state == \"Illinois\", Metro == 1)\n\n\n1\n\nFrom the ‘ozs’ dataset;\n\n2\n\nSelect the columns “state”, “Designated OZ”, and “Metro”;\n\n3\n\nFrom the state column, select the subset of values where state is equal to “Illinois” AND where the Metro column is equal to 1.\n\n\n\n\n# A tibble: 1,344 × 3\n   state    DesignatedOZ Metro\n   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n 1 Illinois            0     1\n 2 Illinois            0     1\n 3 Illinois            0     1\n 4 Illinois            1     1\n 5 Illinois            1     1\n 6 Illinois            0     1\n 7 Illinois            1     1\n 8 Illinois            0     1\n 9 Illinois            0     1\n10 Illinois            0     1\n# ℹ 1,334 more rows\n\n\n\n\n\nIf you do this successfully, you should end up with 1,344 observations.\n\nozs |&gt;\n  select(state, DesignatedOZ, Metro) |&gt;\n  filter(state == \"Illinois\", Metro == 1) |&gt; \n  nrow()\n\n[1] 1344\n\n\n\n\nGroup By and Summarise\nIn the vacancy rate illustration that we saw above, we were able to group our data by a particular categorical variable and then summarize based upon another variable, in that case then average vacancy rate.\nLet’s see what that looks like again, this time, finding the average median household income for designated and not designated but eligible opportunity zone tracts:\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt;  summarise(mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 2 × 2\n  DesignatedOZ `mean(medhhincome, na.rm = TRUE)`\n         &lt;dbl&gt;                             &lt;dbl&gt;\n1            0                            44446.\n2            1                            33346.\n\n\nA little tip here - we can easily change the name of the column label for our summarized values as follows:\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt;  summarise(income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 2 × 2\n  DesignatedOZ income\n         &lt;dbl&gt;  &lt;dbl&gt;\n1            0 44446.\n2            1 33346.\n\n\nWithin our summarise() code, we can create multiple columns with each separated by a comma.\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt;  summarise(\n    tracts = n(),\n    income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 2 × 3\n  DesignatedOZ tracts income\n         &lt;dbl&gt;  &lt;int&gt;  &lt;dbl&gt;\n1            0  33414 44446.\n2            1   8764 33346.\n\n\nn() returns the count of the number of records within each group.\n\nYour TurnSolution\n\n\nYour turn - add to our above summary table the average poverty rate (PovertyRate) and the average proportion of the population facing severe rent burden (severerentburden). You can name them whatever you want\n\n\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt;  summarise(\n    tracts = n(),\n    income = mean(medhhincome, na.rm=TRUE),\n    poverty = mean(PovertyRate, na.rm=TRUE),\n    rent_burden = mean(severerentburden, na.rm=TRUE))\n\n# A tibble: 2 × 5\n  DesignatedOZ tracts income poverty rent_burden\n         &lt;dbl&gt;  &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1            0  33414 44446.   0.211       0.243\n2            1   8764 33346.   0.317       0.265\n\n\n\n\n\nIt looks like designated opportunity zones have lower incomes, higher poverty rates, and higher levels of severe rent burden.\nThis is a big step up from what we were doing earlier. We know how different designated and undesignated tracts are throughout the US, but how different are they for each state in the US?\nHow would we go about modifying our code to create this grouping?\n\nYour TurnSolution\n\n\nModify your above code to group your data by state and designation status in order to be able to examine state-to-state differences.\n\n\n\nozs |&gt; \n  group_by(state, DesignatedOZ) |&gt;  summarise(\n    tracts = n(),\n    income = mean(medhhincome, na.rm=TRUE),\n    poverty = mean(PovertyRate, na.rm=TRUE),\n    rent_burden = mean(severerentburden, na.rm=TRUE))\n\n`summarise()` has grouped output by 'state'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 108 × 6\n# Groups:   state [57]\n   state          DesignatedOZ tracts income poverty rent_burden\n   &lt;chr&gt;                 &lt;dbl&gt;  &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 Alabama                   0    677 36542.   0.239       0.212\n 2 Alabama                   1    158 30044.   0.328       0.246\n 3 Alaska                    0     43 54784.   0.149       0.180\n 4 Alaska                    1     25 49840.   0.167       0.178\n 5 American Samoa            1     16   NaN  NaN         NaN    \n 6 Arizona                   0    702 40961.   0.246       0.236\n 7 Arizona                   1    168 34373.   0.315       0.237\n 8 Arkansas                  0    435 37814.   0.221       0.192\n 9 Arkansas                  1     85 31254.   0.301       0.228\n10 California                0   3464 50858.   0.207       0.298\n# ℹ 98 more rows\n\n\n\n\n\nIf you modified this correctly, you should now have an output table with 108 rows, each reflecting summaries for a state and unique OZ designation status.\nThere are other fairly interesting things that we can do with our grouping and summarizing. We figured out how to use multiple groups to summarize our data in useful ways. What we probably want is to get that all into the same table.\nOne strategy for doing this is to include conditions in our summary statements. The code below summarizes the average median income by state, but then includes conditions on summarizing means income. This allows us to get the incomes of designated and undesignated tracts on the same row.\n\nozs |&gt;  \n  group_by(state) |&gt; \n  summarise(\n    tracts = n(), \n    income = mean(medhhincome, na.rm=TRUE), \n    Des_inc = mean(medhhincome[DesignatedOZ == 1], na.rm=TRUE), \n    Not_Des_Inc = mean(medhhincome[DesignatedOZ == 0], na.rm=TRUE))\n\n# A tibble: 57 × 5\n   state                tracts income Des_inc Not_Des_Inc\n   &lt;chr&gt;                 &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 Alabama                 835 35311.  30044.      36542.\n 2 Alaska                   68 52911.  49840.      54784.\n 3 American Samoa           16   NaN     NaN         NaN \n 4 Arizona                 870 39692.  34373.      40961.\n 5 Arkansas                520 36740.  31254.      37814.\n 6 California             4343 47878.  36134.      50858.\n 7 Colorado                657 47976.  41138.      49601.\n 8 Connecticut             344 48318.  36760.      51389.\n 9 Delaware                118 48200.  40971.      50143.\n10 District of Columbia    116 57672.  38291.      62840.\n# ℹ 47 more rows\n\n\n\nYour TurnSolution\n\n\nHow would you modify the above code to produce the same table for counties in Illinois?\n\n\n\nozs |&gt;  \n  filter(state == \"Illinois\") |&gt; \n  group_by(county) |&gt; \n  summarise(\n    tracts = n(), \n    income = mean(medhhincome, na.rm=TRUE), \n    Des_inc = mean(medhhincome[DesignatedOZ == 1], na.rm=TRUE), \n    Not_Des_Inc = mean(medhhincome[DesignatedOZ == 0], na.rm=TRUE))\n\n# A tibble: 95 × 5\n   county           tracts income Des_inc Not_Des_Inc\n   &lt;chr&gt;             &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1 Adams County         10 38254   26012       39614.\n 2 Alexander County      4 29982.  21500       32809 \n 3 Bond County           2 50950   49590       52310 \n 4 Boone County          3 44028.  40599       45742 \n 5 Bureau County         4 57275.  48083       60339.\n 6 Calhoun County        2 55290     NaN       55290 \n 7 Carroll County        2 47063   35184       58942 \n 8 Cass County           3 43787.  37679       46840.\n 9 Champaign County     30 39063.  13989.      45604.\n10 Christian County      8 44723.  36164       45945.\n# ℹ 85 more rows\n\n\n\n\n\n\n\nMutate\nWe’re getting pretty good at passing data along using pipes (|&gt;). We’ve learned how to use group_by() and summarise() to quickly create summary tables. What if we wanted to modify these tables? One thing that might help us better understand our summary table would be to calculate the difference in the average median income for our designated and not designated tracts.\nmutate() allows us to add new columns to our existing data (this will work on non-summarized data too). The code below adds a column called “Inc_Diff” to our summary table, and places into this column the difference between the income in designated and not designated census tracts:\n\nozs |&gt;  \n  filter(state == \"Illinois\") |&gt;  \n  group_by(county) |&gt;  \n  summarise(\n    tracts = n(), \n    income = mean(medhhincome, na.rm=TRUE), \n    Des_inc = mean(medhhincome[DesignatedOZ == 1], na.rm=TRUE), \n    Not_Des_Inc = mean(medhhincome[DesignatedOZ == 0], na.rm=TRUE)) |&gt; \n  mutate(Inc_Diff = Des_inc - Not_Des_Inc)\n\n# A tibble: 95 × 6\n   county           tracts income Des_inc Not_Des_Inc Inc_Diff\n   &lt;chr&gt;             &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 Adams County         10 38254   26012       39614.  -13602.\n 2 Alexander County      4 29982.  21500       32809   -11309 \n 3 Bond County           2 50950   49590       52310    -2720 \n 4 Boone County          3 44028.  40599       45742    -5143 \n 5 Bureau County         4 57275.  48083       60339.  -12256.\n 6 Calhoun County        2 55290     NaN       55290      NaN \n 7 Carroll County        2 47063   35184       58942   -23758 \n 8 Cass County           3 43787.  37679       46840.   -9162.\n 9 Champaign County     30 39063.  13989.      45604.  -31615.\n10 Christian County      8 44723.  36164       45945.   -9781.\n# ℹ 85 more rows\n\n\nNotice that we needed to add another pipe here so that we were mutating our summary table and not our original data. Notice that most designated tracts have much lower median household incomes when compared to eligible but not designated places - that would suggest that the program is targeting neighborhoods with greater need."
  },
  {
    "objectID": "howto/rbasics_03.html#time-for-practice",
    "href": "howto/rbasics_03.html#time-for-practice",
    "title": "Lesson 3: Tidy Data",
    "section": "Time for Practice!",
    "text": "Time for Practice!\nLet’s spend a little time practicing filtering, grouping, and summarizing data using dplyr commands.\n\nYour TurnSolution\n\n\nCreate a summary table of the racial characteristics of designated and not designated tracts at the nation level.\nRacial characteristics are pctwhitw, pctBlack, pctHispanic, pctAAPIalone.\n\n\n\nozs |&gt; \n  group_by(DesignatedOZ) |&gt; \n  summarise(\n    White = mean(pctwhite, na.rm=TRUE), \n    Black = mean(pctBlack, na.rm=TRUE), \n    Hispanic = mean(pctHispanic, na.rm=TRUE), \n    AAPI = mean(pctAAPIalone, na.rm=TRUE))\n\n# A tibble: 2 × 5\n  DesignatedOZ White Black Hispanic   AAPI\n         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1            0 0.554 0.172    0.200 0.0404\n2            1 0.396 0.240    0.299 0.0292\n\n\n\n\n\n\nYour TurnSolution\n\n\nLooking at the state level (by each state), how different are the poverty rates of designated opportunity zones in metropolitan, micropolitan, and non-CBSA areas?\n\n\n\nozs |&gt; \n  filter(DesignatedOZ == 1) |&gt; \n  group_by(state) |&gt;  \n  summarise(\n    Metro = mean(PovertyRate[Metro == 1], na.rm=TRUE),\n    Micro = mean(PovertyRate[Micro == 1], na.rm=TRUE),\n    Non_CBSA = mean(PovertyRate[NoCBSAType == 1], na.rm=TRUE))\n\n# A tibble: 56 × 4\n   state                  Metro   Micro Non_CBSA\n   &lt;chr&gt;                  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 Alabama                0.347   0.282    0.275\n 2 Alaska                 0.156 NaN        0.178\n 3 American Samoa       NaN     NaN      NaN    \n 4 Arizona                0.319   0.311    0.239\n 5 Arkansas               0.334   0.287    0.262\n 6 California             0.332   0.311    0.209\n 7 Colorado               0.245   0.169    0.201\n 8 Connecticut            0.284   0.319  NaN    \n 9 Delaware               0.262 NaN      NaN    \n10 District of Columbia   0.322 NaN      NaN    \n# ℹ 46 more rows\n\n\n\n\n\n\nYour TurnSolution\n\n\nLooking at the state level (by state), what’s the average age dependence ratio for designated and non-designated tracts?\nTip: The age dependence ratio is the proportion of the population under 18 or over 64 compared to the population between 18 and 64. In our dataset, we have the proportion under 18 (pctunder18) and the proportion over 64 (pctover64)\n\n\n\nozs |&gt;  \n  select(state, DesignatedOZ, pctunder18, pctover64) |&gt;  \n  mutate(\n    adr = (pctunder18+pctover64)/(1-(pctunder18+pctover64))) |&gt; \n  group_by(state) |&gt;   \n  summarise(Designated_ADR = mean(adr[DesignatedOZ == 1], na.rm=TRUE),\n          NotDesignated_ADR = mean(adr[DesignatedOZ == 0], na.rm=TRUE))\n\n# A tibble: 57 × 3\n   state                Designated_ADR NotDesignated_ADR\n   &lt;chr&gt;                         &lt;dbl&gt;             &lt;dbl&gt;\n 1 Alabama                       0.592             0.644\n 2 Alaska                        0.540             0.556\n 3 American Samoa              NaN               NaN    \n 4 Arizona                       0.651             0.771\n 5 Arkansas                      0.643             0.681\n 6 California                    0.606             0.590\n 7 Colorado                      0.590             0.572\n 8 Connecticut                   0.543             0.560\n 9 Delaware                      0.601             0.614\n10 District of Columbia          0.535             0.486\n# ℹ 47 more rows\n\n\n\n\n\n\nYour Turn\n\n\nLooking the state of Illinois, whats the average poverty and income for tracts based upon their level of investment flows (the dec_score variable)? #### Solution\n\nozs |&gt;  \n  filter(state == \"Illinois\") |&gt; \n  group_by(dec_score) |&gt;   \n  summarise(Count = n(),\n            Poverty = mean(PovertyRate, na.rm=TRUE),\n            Income = mean(medhhincome, na.rm=TRUE))\n\n# A tibble: 11 × 4\n   dec_score Count Poverty Income\n       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1         1   165   0.199 41642.\n 2         2   165   0.227 40782.\n 3         3   165   0.278 36666.\n 4         4   164   0.277 37705.\n 5         5   165   0.252 39391.\n 6         6   165   0.224 43224.\n 7         7   164   0.233 42253.\n 8         8   165   0.230 43776.\n 9         9   165   0.215 47064.\n10        10   164   0.209 51795.\n11        NA    12   0.459 32213.\n\n\n\n\n\nCongratulations! You are well on your way to being able to do some very powerful things in R! Take a moment to relish in your accomplishment! ## Lesson 2 Summary and Debrief\nIn this lesson, you …"
  },
  {
    "objectID": "howto/rbasics_03.html#core-concepts-and-terminology",
    "href": "howto/rbasics_03.html#core-concepts-and-terminology",
    "title": "Lesson 3: Tidy Data",
    "section": "Core Concepts and Terminology",
    "text": "Core Concepts and Terminology\n\nR Script\nNotebook\nCode Chunk\nVariables\nLists\nVectors\nData Frame"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neighborhood Analysis Spring 2024",
    "section": "",
    "text": "Neighborhood Analysis Spring 2024\nLearn to tell stories about neighborhoods for decision-making, public deliberation, and accountability using  and principles of reproducible data analysis.\n  \n\n\n\nInstructor\n\n   Dr. Andrew J. Greenlee\n   M210 Temple Buell Hall\n   agreen4@illinois.edu\n   Github\n   urbprof\n   https://fediscience.org/@urbprof\n   Schedule an appointment\n\n\n\nTeaching Assistant\n\n   Ouafa Benkraouda\n   ouafab2@illinois.edu\n   Github\n   Schedule an appointment\n\n\n\nCourse details\n\n   Mondays and Wednesdays\n   11:00 AM - 12:20 PM\n   Temple Buell Hall 223\n   Discussion"
  },
  {
    "objectID": "resources/data.html",
    "href": "resources/data.html",
    "title": "Data Analysis",
    "section": "",
    "text": "covdata R Package\nData Science Central Big data sets available for free\nEconomic Innovation Group Distressed Communities Index\nEviction Lab data on residential evictions summarized to various administrative geographies\nGlobal Human Settlements Dataset\nHUD Location Affordability Index\nHUD Urbanization Perceptions Small Area Index\nHUD USPS Vacancy Data\nLongitudinal Tract Database (LTDB): 1970 - 2010 Census data normalized to 2010 census tracts\nMIT Senseable Cities Lab Treepedia\nNational Housing Preservation Database (NHPD)\nPolicy Surveillance Program Eviction Laws database\nUSDA Rural-Urban Commuting Area Codes\nStanford Open Policing Project\nSURGO Foundation U.S. COVID Community Vulnerability Index (CCVI)"
  },
  {
    "objectID": "resources/inspiration.html",
    "href": "resources/inspiration.html",
    "title": "Inspiration",
    "section": "",
    "text": "BBC: How the BBC Visual and Data Journalism Team Works with Graphics in R\nBloomberg: Visualizing U.S. Immigration History as Tree Rings\nCraig Dalton and Timm Stallmann: Counter-mapping data science\nGapminder (check out in particular their temporal animations)\nGeoDa Center U.S. COVID Atlas\nR at the ACLU: Joining Tables to Reunite Families\nLondon: The Information Capital: Stunning book featuring 100 visualizations about the city of London. Many visualizations were created using R.\nMIT: Atlas of Inequality\nNew York City Neighborhood Planning Playbook\nR Graph Gallery\nWashington Post: America is more diverse than ever - but still segregated\nW.E.B. Du Bois’s Data Portraits: Visualizing Black America and related background from Smithsonian Magazine"
  },
  {
    "objectID": "schedule/01_introduction.html",
    "href": "schedule/01_introduction.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Welcome to UP 570: Neighborhood Analysis! This session serves as our introduction to the class and an opportunity to start exploring how we will work together over the course of the semester. We will introduce ourselves and discuss how we will learn together. We’ll also think through two foundational questions:\n\nWhat is a neighborhood?\nWhy do neighborhoods matter?"
  },
  {
    "objectID": "schedule/01_introduction.html#session-description",
    "href": "schedule/01_introduction.html#session-description",
    "title": "Course Introduction",
    "section": "",
    "text": "Welcome to UP 570: Neighborhood Analysis! This session serves as our introduction to the class and an opportunity to start exploring how we will work together over the course of the semester. We will introduce ourselves and discuss how we will learn together. We’ll also think through two foundational questions:\n\nWhat is a neighborhood?\nWhy do neighborhoods matter?"
  },
  {
    "objectID": "schedule/01_introduction.html#before-class",
    "href": "schedule/01_introduction.html#before-class",
    "title": "Course Introduction",
    "section": "Before Class",
    "text": "Before Class\n\nWatch this video which provides an overview of our course website and resources:\n\n\n\nBookmark this website https://up570s24.netlify.app so you can access it easily.\nIntroduce yourself on our course Github Discussion Forum\nRead the course syllabus and bring questions with you.\nComplete the course introductory survey."
  },
  {
    "objectID": "schedule/01_introduction.html#reflect",
    "href": "schedule/01_introduction.html#reflect",
    "title": "Course Introduction",
    "section": "Reflect",
    "text": "Reflect\n\nWhat are your goals for taking this class? What would you like to learn about neighborhoods?\nWhat matters about neighborhoods? How have neighborhoods shaped your life?\nWhat types of stories do we tend to tell about neighborhoods? How do these stories contextualize how neighborhoods “fit” within cities and their regions?"
  },
  {
    "objectID": "schedule/01_introduction.html#slides",
    "href": "schedule/01_introduction.html#slides",
    "title": "Course Introduction",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/01_introduction.html#resources-for-further-exploration",
    "href": "schedule/01_introduction.html#resources-for-further-exploration",
    "title": "Course Introduction",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\n\nJoin the Data Science in Planning (DSIP) Listserv (dsip-durp-l@lists.illinois.edu) where members of our community can share opportunities things of interest.\n\nChoose the address with which you want to subscribe to the list - you should choose an address you can check frequently.\nSend a message to sympa@lists.illinois.edu from the address you want to subscribe to the list.\nIn the subject line of your message, type in subscribe dsip-durp-l Firstname Name (replace Firstname and Name with your preferred first name and last name).\nLeave the message body blank.\nAfter this you will recieve a message telling you whether your request was accepted or not.\n\nComprehensive Development Plan for Champaign - Urbana (1950)"
  },
  {
    "objectID": "schedule/03_datapipeline.html",
    "href": "schedule/03_datapipeline.html",
    "title": "Building a Data Pipeline",
    "section": "",
    "text": "In this session, we’ll explore some of the basic workflow which we’ll use over the course of the semester to package and share analysis. We’ll develop familiarity with Quarto, and basic operations in Github so that you are able to share code and analysis over the course of the semester.\nLab 1 Link"
  },
  {
    "objectID": "schedule/03_datapipeline.html#session-description",
    "href": "schedule/03_datapipeline.html#session-description",
    "title": "Building a Data Pipeline",
    "section": "",
    "text": "In this session, we’ll explore some of the basic workflow which we’ll use over the course of the semester to package and share analysis. We’ll develop familiarity with Quarto, and basic operations in Github so that you are able to share code and analysis over the course of the semester.\nLab 1 Link"
  },
  {
    "objectID": "schedule/03_datapipeline.html#before-class",
    "href": "schedule/03_datapipeline.html#before-class",
    "title": "Building a Data Pipeline",
    "section": "Before Class",
    "text": "Before Class\nReview today’s lab guide.\nEnsure that your computer has the latest stable versions of R and RStudio installed.\nAccept the GitHub invitation to our Lab 1 repository and download the repository to your local computer (we will set up more advanced tools for interacting with GitHub in our next lab session."
  },
  {
    "objectID": "schedule/03_datapipeline.html#reflect",
    "href": "schedule/03_datapipeline.html#reflect",
    "title": "Building a Data Pipeline",
    "section": "Reflect",
    "text": "Reflect\n\nWorkflows\n\nWhat are the types of common tasks in your workflows that you think would benefit from a data pipeline?\nHow do we hold ourselves accountable for our analysis?\n\n\n\nReadings\n\nWhose interests and goals do you seek to represent through your work?\nWhat missing datasets (akin to the Library of Missing Datasets) have you observed?1"
  },
  {
    "objectID": "schedule/03_datapipeline.html#slides",
    "href": "schedule/03_datapipeline.html#slides",
    "title": "Building a Data Pipeline",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/03_datapipeline.html#resources-for-further-exploration",
    "href": "schedule/03_datapipeline.html#resources-for-further-exploration",
    "title": "Building a Data Pipeline",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/03_datapipeline.html#footnotes",
    "href": "schedule/03_datapipeline.html#footnotes",
    "title": "Building a Data Pipeline",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAt the beginning of our session, we’ll catalog some of these datasets - it may help to write down some of your thoughts to share.↩︎"
  },
  {
    "objectID": "schedule/05_tidydata.html",
    "href": "schedule/05_tidydata.html",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "",
    "text": "This week, we’ll spend time reviewing the basics of what tidy data are, and will review common manipulation strategies. We will continue the work that we began on Tuesday and will take a few moments to reflect upon how these principles help us with some of our foundational approaches to information in this class."
  },
  {
    "objectID": "schedule/05_tidydata.html#session-description",
    "href": "schedule/05_tidydata.html#session-description",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "",
    "text": "This week, we’ll spend time reviewing the basics of what tidy data are, and will review common manipulation strategies. We will continue the work that we began on Tuesday and will take a few moments to reflect upon how these principles help us with some of our foundational approaches to information in this class."
  },
  {
    "objectID": "schedule/05_tidydata.html#before-class",
    "href": "schedule/05_tidydata.html#before-class",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "Before Class",
    "text": "Before Class\nReview the How To Lesson 3."
  },
  {
    "objectID": "schedule/05_tidydata.html#reflect",
    "href": "schedule/05_tidydata.html#reflect",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/05_tidydata.html#slides",
    "href": "schedule/05_tidydata.html#slides",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/05_tidydata.html#resources-for-further-exploration",
    "href": "schedule/05_tidydata.html#resources-for-further-exploration",
    "title": "Introduction to Tidy Data - Session 2",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/07_communicating.html",
    "href": "schedule/07_communicating.html",
    "title": "Communicating Complex Information",
    "section": "",
    "text": "In this lab session, you’ll do some initial work on analyzing and communicating “real world” data drawing from your prior knowledge of R and from our initial work on building data pipelines and communication streams. This session marks out transition from setting up some basic workflows to focusing on data analysis techniques and applications. This lab is designed to help us learn more about your familiarity and proficiency with basic data manipulation using common dplyr functions.\nYour goal is to use our lab session plus an additional 3-4 hours to work through lab prompts. Address the prompts if you can. If you can’t, provide written descriptions about what you’re trying to do, pointers about how you’ve tried to address the problems, and insights into where you’re getting stuck. To repeat, the overall goal isn’t to complete the lab, but rather to share your process and insights. This will help us to refine future labs and instructions based upon the collective knowledge and understanding of the class."
  },
  {
    "objectID": "schedule/07_communicating.html#session-description",
    "href": "schedule/07_communicating.html#session-description",
    "title": "Communicating Complex Information",
    "section": "",
    "text": "In this lab session, you’ll do some initial work on analyzing and communicating “real world” data drawing from your prior knowledge of R and from our initial work on building data pipelines and communication streams. This session marks out transition from setting up some basic workflows to focusing on data analysis techniques and applications. This lab is designed to help us learn more about your familiarity and proficiency with basic data manipulation using common dplyr functions.\nYour goal is to use our lab session plus an additional 3-4 hours to work through lab prompts. Address the prompts if you can. If you can’t, provide written descriptions about what you’re trying to do, pointers about how you’ve tried to address the problems, and insights into where you’re getting stuck. To repeat, the overall goal isn’t to complete the lab, but rather to share your process and insights. This will help us to refine future labs and instructions based upon the collective knowledge and understanding of the class."
  },
  {
    "objectID": "schedule/07_communicating.html#before-class",
    "href": "schedule/07_communicating.html#before-class",
    "title": "Communicating Complex Information",
    "section": "Before Class",
    "text": "Before Class\n\nTake a look at these instructions.\nAccept the lab repository, link to Github, and create a local version of the repository.\nCome to class with any initial questions you have about the Learner’s Permit of what you’re being asked."
  },
  {
    "objectID": "schedule/07_communicating.html#reflect",
    "href": "schedule/07_communicating.html#reflect",
    "title": "Communicating Complex Information",
    "section": "Reflect",
    "text": "Reflect\n\nHow do you typically approach exploring unfamiliar data? What types of questions help you find a direction?\nWhat kinds of stories might data on code violations help us tell?\nWhat types of information are missing from these datasets? What questions come up as you complete your labs?"
  },
  {
    "objectID": "schedule/07_communicating.html#resources-for-further-exploration",
    "href": "schedule/07_communicating.html#resources-for-further-exploration",
    "title": "Communicating Complex Information",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration\nNew York City Code Violations \nNew York City PLUTO Data"
  },
  {
    "objectID": "schedule/09_places.html",
    "href": "schedule/09_places.html",
    "title": "Describing Places",
    "section": "",
    "text": "Today’s lab session focuses on the basic description of places. In your prior labs, you have developed basic workflows for communicating using principles of reproducible data analysis. Today’s lab asks you to apply those skills to basic description of place characteristics.\nTo add depth to our example, we will explore not only basic place descriptions, but will also think about how these descriptions might change in relation to an applied policy problem."
  },
  {
    "objectID": "schedule/09_places.html#session-description",
    "href": "schedule/09_places.html#session-description",
    "title": "Describing Places",
    "section": "",
    "text": "Today’s lab session focuses on the basic description of places. In your prior labs, you have developed basic workflows for communicating using principles of reproducible data analysis. Today’s lab asks you to apply those skills to basic description of place characteristics.\nTo add depth to our example, we will explore not only basic place descriptions, but will also think about how these descriptions might change in relation to an applied policy problem."
  },
  {
    "objectID": "schedule/09_places.html#before-class",
    "href": "schedule/09_places.html#before-class",
    "title": "Describing Places",
    "section": "Before Class",
    "text": "Before Class\nAccept the GitHub Classroom invitation to our lab repository and use RStudio to pull the repository to your local computer.\nRead our lab background so you are prepared to start working through the lab repository notebook."
  },
  {
    "objectID": "schedule/09_places.html#reflect",
    "href": "schedule/09_places.html#reflect",
    "title": "Describing Places",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/09_places.html#resources-for-further-exploration",
    "href": "schedule/09_places.html#resources-for-further-exploration",
    "title": "Describing Places",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/11_census.html",
    "href": "schedule/11_census.html",
    "title": "Population and the Census",
    "section": "",
    "text": "In this session, we will work on our lab focused on exploring population and the census."
  },
  {
    "objectID": "schedule/11_census.html#session-description",
    "href": "schedule/11_census.html#session-description",
    "title": "Population and the Census",
    "section": "",
    "text": "In this session, we will work on our lab focused on exploring population and the census."
  },
  {
    "objectID": "schedule/11_census.html#before-class",
    "href": "schedule/11_census.html#before-class",
    "title": "Population and the Census",
    "section": "Before Class",
    "text": "Before Class\nAccept the GitHub Classroom invitation to our lab repository and use RStudio to pull the repository to your local computer.\nRead our lab background so you are prepared to start working through the lab repository notebook.\nIf you do not already have one, please register for a U.S. Census Bureau API Key which we will need for the tidycensus package."
  },
  {
    "objectID": "schedule/11_census.html#reflect",
    "href": "schedule/11_census.html#reflect",
    "title": "Population and the Census",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/11_census.html#slides",
    "href": "schedule/11_census.html#slides",
    "title": "Population and the Census",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/11_census.html#resources-for-further-exploration",
    "href": "schedule/11_census.html#resources-for-further-exploration",
    "title": "Population and the Census",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/13_segregation.html",
    "href": "schedule/13_segregation.html",
    "title": "Segregation",
    "section": "",
    "text": "In this session, we will begin to work on a lab focused on measures of residential segregation. We will also spend some time workshopping your approach to the Population Memo.\nWe’ll take around 20 minutes at the beginning of class for you to share with each other what you’re thinking and your approach.\nWe’ll then engage any questions you have about this week’s lab or last week’s lab."
  },
  {
    "objectID": "schedule/13_segregation.html#session-description",
    "href": "schedule/13_segregation.html#session-description",
    "title": "Segregation",
    "section": "",
    "text": "In this session, we will begin to work on a lab focused on measures of residential segregation. We will also spend some time workshopping your approach to the Population Memo.\nWe’ll take around 20 minutes at the beginning of class for you to share with each other what you’re thinking and your approach.\nWe’ll then engage any questions you have about this week’s lab or last week’s lab."
  },
  {
    "objectID": "schedule/13_segregation.html#before-class",
    "href": "schedule/13_segregation.html#before-class",
    "title": "Segregation",
    "section": "Before Class",
    "text": "Before Class\nGitHub Classroom Link\nPlease come ready to share your approach to the population memo in small groups. Please bring in examples of analysis or visualization that we can look at, either in small groups or collectively."
  },
  {
    "objectID": "schedule/13_segregation.html#reflect",
    "href": "schedule/13_segregation.html#reflect",
    "title": "Segregation",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/13_segregation.html#code",
    "href": "schedule/13_segregation.html#code",
    "title": "Segregation",
    "section": "Code",
    "text": "Code\nIn class, we spent some time looking at code that would select tracts within census place geographies. Here’s that code for your reference.\nLet’s start by downloading tract and place geometries and selecting Chicago (as an example):\nFirst, download tracts, subset to Cook County, and plot to confirm:\n\nlibrary(sf)\nlibrary(tigris)\nlibrary(tidyverse)\n\n\nil_trt &lt;- tracts(state = \"IL\") |&gt; \n  filter(COUNTYFP == \"031\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nggplot()+\n  geom_sf(data = il_trt) \n\n\n\n\nNext, download census place boundaries, filter to Chicago, and plot to confirm:\n\nil_place &lt;- places(state = \"IL\") |&gt; \n  filter(NAME == \"Chicago\")\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |======================================================================| 100%\n\nggplot()+\n  geom_sf(data = il_place, cex = 1) \n\n\n\n\nPlot together to see overlay:\n\nggplot()+\n  geom_sf(data = il_trt, cex=.1)+\n  geom_sf(data = il_place, color = \"blue\",cex = .5, fill = NA)\n\n\n\n\n\nOption 1: Spatial Join\nPerform a spatial join to identify all tracts that overlap with Chicago’s municipal boundaries, plot to confirm:\n\ntest &lt;- st_join(il_trt, il_place) |&gt; \n  filter(NAME.y == \"Chicago\")\n\nggplot() +\n  geom_sf(data = test) +\n  geom_sf(data = il_place, color = \"blue\", fill = NA)\n\n\n\n\n\n\nOption 2: Intersection\nAnother option is to find the spatial intersection of the two objects:\n\ntest2 &lt;- st_intersection(il_trt, il_place)\n\nggplot() +\n  geom_sf(data = test2)+\n  geom_sf(data = il_place, color = \"blue\", fill = NA)\n\n\n\n\nAs we discussed in class, there may be good reasons for pursuing either method. Intersection may produce visually “cleaner” output, but we may need to spend more time explaining that there are portions of tract geometry that fall outside of the city boundary. We may even want to do some analysis to estimate where uncertainty may come from due to our strategy."
  },
  {
    "objectID": "schedule/13_segregation.html#slides",
    "href": "schedule/13_segregation.html#slides",
    "title": "Segregation",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/13_segregation.html#resources-for-further-exploration",
    "href": "schedule/13_segregation.html#resources-for-further-exploration",
    "title": "Segregation",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/15_neighborhood.html#before-class",
    "href": "schedule/15_neighborhood.html#before-class",
    "title": "Neighborhood Change",
    "section": "Before Class",
    "text": "Before Class\nGithub Classroom Link"
  },
  {
    "objectID": "schedule/15_neighborhood.html#reflect",
    "href": "schedule/15_neighborhood.html#reflect",
    "title": "Neighborhood Change",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/15_neighborhood.html#slides",
    "href": "schedule/15_neighborhood.html#slides",
    "title": "Neighborhood Change",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/15_neighborhood.html#resources-for-further-exploration",
    "href": "schedule/15_neighborhood.html#resources-for-further-exploration",
    "title": "Neighborhood Change",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/17_opportunity.html#before-class",
    "href": "schedule/17_opportunity.html#before-class",
    "title": "Place Opportunity",
    "section": "Before Class",
    "text": "Before Class\nReview the Place Opportunity lab guide.\nGitHub Classroom Link"
  },
  {
    "objectID": "schedule/17_opportunity.html#reflect",
    "href": "schedule/17_opportunity.html#reflect",
    "title": "Place Opportunity",
    "section": "Reflect",
    "text": "Reflect\nWhat types of value judgements inform what makes a place “good” or “bad”?"
  },
  {
    "objectID": "schedule/17_opportunity.html#slides",
    "href": "schedule/17_opportunity.html#slides",
    "title": "Place Opportunity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/17_opportunity.html#resources-for-further-exploration",
    "href": "schedule/17_opportunity.html#resources-for-further-exploration",
    "title": "Place Opportunity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/19_transequity.html#before-class",
    "href": "schedule/19_transequity.html#before-class",
    "title": "Transit Equity",
    "section": "Before Class",
    "text": "Before Class\nGithub Classroom Link"
  },
  {
    "objectID": "schedule/19_transequity.html#reflect",
    "href": "schedule/19_transequity.html#reflect",
    "title": "Transit Equity",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/19_transequity.html#slides",
    "href": "schedule/19_transequity.html#slides",
    "title": "Transit Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/19_transequity.html#resources-for-further-exploration",
    "href": "schedule/19_transequity.html#resources-for-further-exploration",
    "title": "Transit Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/21_healthequity.html",
    "href": "schedule/21_healthequity.html",
    "title": "Health Equity",
    "section": "",
    "text": "We will work in groups today to do analysis and storytelling as part of the health equity lab. As mentioned in class on Monday, this lab will focus less on technical details and more on storytelling and interpretation of indicators."
  },
  {
    "objectID": "schedule/21_healthequity.html#session-description",
    "href": "schedule/21_healthequity.html#session-description",
    "title": "Health Equity",
    "section": "",
    "text": "We will work in groups today to do analysis and storytelling as part of the health equity lab. As mentioned in class on Monday, this lab will focus less on technical details and more on storytelling and interpretation of indicators."
  },
  {
    "objectID": "schedule/21_healthequity.html#group-assignments",
    "href": "schedule/21_healthequity.html#group-assignments",
    "title": "Health Equity",
    "section": "Group Assignments",
    "text": "Group Assignments\nFor this assignment, you will work in three groups:\n\nHousing\nGabriela A. Dakshinya B. Dominic C. Ar’Mand E. Dasom H. Shinmyeong H. Tushar K. Jenifer M. Tillie P. Anna S.\n\n\nPoverty\nSiti A. Bhagyashree Natalie C. Luisa P. Erin H. Joseph J. Trinity L. Anjana N. Matthew R. Alec T.\n\n\nEducation\nHyndavi A. Leela B. Rithvika D. Cole F. Matthew H. Nadia K. Anukriti M. Erin P. Aabha S. Zhenpeng Z.\nYour group will present your narrative at the end of our class session. After class, please complete your lab reflection and push this to Github. There will be no other lab content or work for this week other than completing your own individual reflection on what we’ve done in small groups in class."
  },
  {
    "objectID": "schedule/21_healthequity.html#before-class",
    "href": "schedule/21_healthequity.html#before-class",
    "title": "Health Equity",
    "section": "Before Class",
    "text": "Before Class\nGithub Classroom Link"
  },
  {
    "objectID": "schedule/21_healthequity.html#reflect",
    "href": "schedule/21_healthequity.html#reflect",
    "title": "Health Equity",
    "section": "Reflect",
    "text": "Reflect\n\nWhat are some tropes or conventions we use to talk about individual health? Collective health?\nHow might health disparities (at a neighborhood or population level) be connected to the many themes we’ve talked about in this class?\nIs population health a planning issue? What role do practitioners and researchers in planning have for influencing how we think and act around health at the neighborhood level?"
  },
  {
    "objectID": "schedule/21_healthequity.html#slides",
    "href": "schedule/21_healthequity.html#slides",
    "title": "Health Equity",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/21_healthequity.html#resources-for-further-exploration",
    "href": "schedule/21_healthequity.html#resources-for-further-exploration",
    "title": "Health Equity",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/23_CheckIn.html",
    "href": "schedule/23_CheckIn.html",
    "title": "Final Project Check-In: Wednesday",
    "section": "",
    "text": "This will be an independent work session. Andrew will be available during the class session to consult briefly on final projects. Andrew has also opened up additional slots in his Calendly schedule to meet one-on-one."
  },
  {
    "objectID": "schedule/23_CheckIn.html#session-description",
    "href": "schedule/23_CheckIn.html#session-description",
    "title": "Final Project Check-In: Wednesday",
    "section": "",
    "text": "This will be an independent work session. Andrew will be available during the class session to consult briefly on final projects. Andrew has also opened up additional slots in his Calendly schedule to meet one-on-one."
  },
  {
    "objectID": "schedule/23_CheckIn.html#reflect",
    "href": "schedule/23_CheckIn.html#reflect",
    "title": "Final Project Check-In: Wednesday",
    "section": "Reflect",
    "text": "Reflect\n\nHow does this evidence support the point you’re trying to make?\nHow do you plan to use the evidence in your argument?\nWhat critiques might others have regarding your evidence? How are you planning to preemptively engage these critiques in your analysis?"
  },
  {
    "objectID": "schedule/25_fieldobs.html",
    "href": "schedule/25_fieldobs.html",
    "title": "Field Observations",
    "section": "",
    "text": "During this session, we will focus on translating our group field observations into structured information - a memorandum that blends your direct observation and knowledge of the neighborhood developed through existing indicators."
  },
  {
    "objectID": "schedule/25_fieldobs.html#session-description",
    "href": "schedule/25_fieldobs.html#session-description",
    "title": "Field Observations",
    "section": "",
    "text": "During this session, we will focus on translating our group field observations into structured information - a memorandum that blends your direct observation and knowledge of the neighborhood developed through existing indicators."
  },
  {
    "objectID": "schedule/25_fieldobs.html#session-goals",
    "href": "schedule/25_fieldobs.html#session-goals",
    "title": "Field Observations",
    "section": "Session Goals",
    "text": "Session Goals\n\nCollaboratively build a story about the West Urbana neighborhood that leverages both your knowledge of existing indicators.\nReflect upon the tactical and design choices you have made in telling your story."
  },
  {
    "objectID": "schedule/25_fieldobs.html#before-class",
    "href": "schedule/25_fieldobs.html#before-class",
    "title": "Field Observations",
    "section": "Before Class",
    "text": "Before Class\n\nReview your notes and materials from your visit work on Monday.\n\nGithub Classroom Link"
  },
  {
    "objectID": "schedule/25_fieldobs.html#during-class",
    "href": "schedule/25_fieldobs.html#during-class",
    "title": "Field Observations",
    "section": "During Class",
    "text": "During Class\nWork collaboratively with your group members to develop a short memorandum describing the West Urbana neighborhood. Your memo should include demographic information coming from census data or other sources of secondary data (please start with the same selected census indicators we have worked with for the past few weeks).\nYour memo should outline the following:\n\nNeighborhood character, identity, and assets - drawing from secondary data, describe the character, identity, and assets of the neighborhood:\n\nInfrastructure and Environment\nEconomy and Housing\nHealth and Wellbeing\nSense of Place\n\nInformation Gaps - based upon your group’s description above, what information gaps exist? What types of information do you need to prioritize observation of on the ground in the West Urbana neighborhood?\nProposed Strategy for Systematic Examination - based upon your assessment of information gaps, how do you propose collecting that information, and how would you integrate it into your report?"
  },
  {
    "objectID": "schedule/25_fieldobs.html#groups",
    "href": "schedule/25_fieldobs.html#groups",
    "title": "Field Observations",
    "section": "Groups",
    "text": "Groups\n\nGroup 1\nTBD\n\n\nGroup 2\nTBD\n\n\nGroup 3\nTBD\n\n\nGroup 4\nTBD"
  },
  {
    "objectID": "schedule/25_fieldobs.html#after-class",
    "href": "schedule/25_fieldobs.html#after-class",
    "title": "Field Observations",
    "section": "After Class",
    "text": "After Class\nDraw upon your shared memo and your individual observations to develop a short reflection on the process.\n\nShare your initial reflection on what you knew about the neighborhood before you visited.\nShare a brief summary of your group’s thoughts and information priorities coming from Monday’s pre-field work preparation.\nShare your field observations, focusing on the element which you were responsible for describing.\nShare your reflections on what you know now and what you experienced in attempting to systematically observe the neighborhood and communicate its qualities.\nThink carefully about what your next steps might (hypothetically be) following your initial field observation.\n\nPush your individual reflections to your lab GitHub repository."
  },
  {
    "objectID": "schedule/27_indwork.html",
    "href": "schedule/27_indwork.html",
    "title": "Independent Work and Advising",
    "section": "",
    "text": "This session is devoted to individual advising and independent work. There are no course sessions scheduled for today. You should use this time to continue working, to take stock of your progress towards your course contract, and to solidify plans for your final project."
  },
  {
    "objectID": "schedule/27_indwork.html#session-description",
    "href": "schedule/27_indwork.html#session-description",
    "title": "Independent Work and Advising",
    "section": "",
    "text": "This session is devoted to individual advising and independent work. There are no course sessions scheduled for today. You should use this time to continue working, to take stock of your progress towards your course contract, and to solidify plans for your final project."
  },
  {
    "objectID": "schedule/27_indwork.html#before-class",
    "href": "schedule/27_indwork.html#before-class",
    "title": "Independent Work and Advising",
    "section": "Before Class",
    "text": "Before Class\nProfessor Greenlee has availability for office hours appointments. If you have not already scheduled an appointment and wish to, please sign up."
  },
  {
    "objectID": "schedule/27_indwork.html#reflect",
    "href": "schedule/27_indwork.html#reflect",
    "title": "Independent Work and Advising",
    "section": "Reflect",
    "text": "Reflect"
  },
  {
    "objectID": "schedule/27_indwork.html#slides",
    "href": "schedule/27_indwork.html#slides",
    "title": "Independent Work and Advising",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "schedule/27_indwork.html#resources-for-further-exploration",
    "href": "schedule/27_indwork.html#resources-for-further-exploration",
    "title": "Independent Work and Advising",
    "section": "Resources for Further Exploration",
    "text": "Resources for Further Exploration"
  },
  {
    "objectID": "schedule/29_finalpresent.html",
    "href": "schedule/29_finalpresent.html",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/29_finalpresent.html#session-description",
    "href": "schedule/29_finalpresent.html#session-description",
    "title": "Final Presentations",
    "section": "",
    "text": "We’ve made it to the end of the semester! Now is the time to share and celebrate your hard work. This week’s sessions are devoted to sharing your final stories. You will each have eight very brief minutes to share your policy-informed stories with us. Please plan to use the entire time to present to us - we’ll give written comments and feedback so you have the maximum amount of time for sharing your work."
  },
  {
    "objectID": "schedule/29_finalpresent.html#todays-presenters",
    "href": "schedule/29_finalpresent.html#todays-presenters",
    "title": "Final Presentations",
    "section": "Today’s Presenters",
    "text": "Today’s Presenters\nReminder - each presenter has ten minutes to share."
  },
  {
    "objectID": "schedule/29_finalpresent.html#providing-feedback",
    "href": "schedule/29_finalpresent.html#providing-feedback",
    "title": "Final Presentations",
    "section": "Providing Feedback",
    "text": "Providing Feedback\nPlease provide feedback to each other in the following form - I will consolidate your written feedback on each presentation and will send it to you."
  },
  {
    "objectID": "schedule/index.html",
    "href": "schedule/index.html",
    "title": "Schedule",
    "section": "",
    "text": "Please find descriptions for our course sessions here - you will also find resources that will help you prepare for each session."
  },
  {
    "objectID": "schedule/index.html#course-introduction",
    "href": "schedule/index.html#course-introduction",
    "title": "Schedule",
    "section": "Course Introduction",
    "text": "Course Introduction\n\n\n\nWeek\nSession\nDate\nDescription\n\n\n\n\nWeek 1\nSession 1\nJanuary 17\nCourse Introduction\n\n\nWeek 2\nSession 2\nJanuary 22\nWhat is a Neighborhood?\n\n\nWeek 2\nSession 3\nJanuary 24\nBuilding a Data Pipeline\n\n\nWeek 3\nSession 4\nJanuary 29\nWorking with Tidy Data\n\n\nWeek 3\nSession 5\nJanuary 31\nWorking with Tidy Data\n\n\nWeek 4\n\nFebruary 5\nPlace Selection Memorandum\n\n\nWeek 4\nSession 6\nFebruary 5\nSharing Your Work\n\n\nWeek 4\nSession 7\nFebruary 7\nCommunicating Complex Information"
  },
  {
    "objectID": "schedule/index.html#strategies-for-analysis",
    "href": "schedule/index.html#strategies-for-analysis",
    "title": "Schedule",
    "section": "Strategies for Analysis",
    "text": "Strategies for Analysis\n\n\n\nWeek\nSession\nDate\nDescription\n\n\n\n\nWeek 5\nSession 8\nFebruary 12\nDescribing Places\n\n\nWeek 5\nSession 9\nFebruary 14\nDescribing Places\n\n\nWeek 6\n\nFebruary 19\nPlace Background Memorandum\n\n\nWeek 6\nSession 10\nFebruary 19\nPopulation and the Census\n\n\nWeek 6\nSession 11\nFebruary 21\nPopulation and the Census\n\n\nWeek 7\nSession 12\nFebruary 26\nSegregation\n\n\nWeek 7\nSession 13\nFebruary 28\nSegregation\n\n\nWeek 8\n\nMarch 4\nPopulation Memorandum\n\n\nWeek 8\nSession 14\nMarch 4\nNeighborhood Change\n\n\nWeek 8\nSession 15\nMarch 6\nNeighborhood Change\n\n\nWeek 9\n\nMarch 11\nSpring Break\n\n\nWeek 9\n\nMarch 13\nSpring Break\n\n\nWeek 10\nSession 16\nMarch 18\nPlace Opportunity\n\n\nWeek 10\nSession 17\nMarch 20\nPlace Opportunity\n\n\nWeek 11\n\nMarch 25\nPolicy Memorandum\n\n\nWeek 11\nSession 18\nMarch 25\nTransportation Equity\n\n\nWeek 11\nSession 19\nMarch 27\nTransportation Equity\n\n\nWeek 12\nSession 20\nApril 1\nHealth Equity\n\n\nWeek 12\nSession 21\nApril 3\nHealth Equity\n\n\nWeek 13\nSession 22\nApril 8\nFinal Project Check-In\n\n\nWeek 13\nSession 23\nApril 10\nFinal Project Check-In"
  },
  {
    "objectID": "schedule/index.html#course-wrap-up",
    "href": "schedule/index.html#course-wrap-up",
    "title": "Schedule",
    "section": "Course Wrap-Up",
    "text": "Course Wrap-Up\n\n\n\nWeek\nSession\nDate\nDescription\n\n\n\n\nWeek 14\n\nApril 15\nFinal Assignment First Draft\n\n\nWeek 14\nSession 24\nApril 15\nField Observation\n\n\nWeek 14\nSession 25\nApril 17\nField Observation\n\n\nWeek 15\nSession 26\nApril 22\nFinal Presentations\n\n\nWeek 15\nSession 27\nApril 24\nIndependent Work (No Class)\n\n\nWeek 16\nSession 28\nApril 29\nFinal Presentations\n\n\nWeek 16\nSession 29\nMay 1\nFinal Presentations\n\n\nWeek 17\n\nMay 10\nFinal Assignment Due"
  }
]